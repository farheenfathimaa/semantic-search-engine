[
  {
    "id": "http://arxiv.org/abs/2507.02864v1",
    "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real",
    "authors": [
      "Renhao Wang",
      "Haoran Geng",
      "Tingle Li",
      "Feishi Wang",
      "Gopala Anumanchipalli",
      "Philipp Wu",
      "Trevor Darrell",
      "Boyi Li",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Alexei A. Efros"
    ],
    "abstract": "Robots must integrate multiple sensory modalities to act effectively in the real world. Yet, learning such multimodal policies at scale remains challenging. Simulation offers a viable solution, but while vision has benefited from high-fidelity simulators, other modalities (e.g. sound) can be notoriously difficult to simulate. As a result, sim-to-real transfer has succeeded primarily in vision-based tasks, with multimodal transfer still largely unrealized. In this work, we tackle these challenges by introducing MultiGen, a framework that integrates large-scale generative models into traditional physics simulators, enabling multisensory simulation. We showcase our framework on the dynamic task of robot pouring, which inherently relies on multimodal feedback. By synthesizing realistic audio conditioned on simulation video, our method enables training on rich audiovisual trajectories -- without any real robot data. We demonstrate effective zero-shot transfer to real-world pouring with novel containers and liquids, highlighting the potential of generative modeling to both simulate hard-to-model modalities and close the multimodal sim-to-real gap.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02864v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02863v1",
    "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory",
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code is available at: https://github.com/YkiWu/Point3R.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02863v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02862v1",
    "title": "RefTok: Reference-Based Tokenization for Video Generation",
    "authors": [
      "Xiang Fan",
      "Xiaohang Sun",
      "Kushan Thakkar",
      "Zhu Liu",
      "Vimal Bhat",
      "Ranjay Krishna",
      "Xiang Hao"
    ],
    "abstract": "Effectively handling temporal redundancy remains a key challenge in learning video models. Prevailing approaches often treat each set of frames independently, failing to effectively capture the temporal dependencies and redundancies inherent in videos. To address this limitation, we introduce RefTok, a novel reference-based tokenization method capable of capturing complex temporal dynamics and contextual information. Our method encodes and decodes sets of frames conditioned on an unquantized reference frame. When decoded, RefTok preserves the continuity of motion and the appearance of objects across frames. For example, RefTok retains facial details despite head motion, reconstructs text correctly, preserves small patterns, and maintains the legibility of handwriting from the context. Across 4 video datasets (K600, UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or higher compression ratios. When a video generation model is trained using RefTok's latents on the BAIR Robot Pushing task, the generations not only outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters, across all generation metrics by an average of 27.9%.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02862v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02856v1",
    "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation",
    "authors": [
      "Nikhil Chandak",
      "Shashwat Goel",
      "Ameya Prabhu",
      "Moritz Hardt",
      "Jonas Geiping"
    ],
    "abstract": "Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02856v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02851v1",
    "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
    "authors": [
      "Purbesh Mitra",
      "Sennur Ulukus"
    ],
    "abstract": "Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ],
    "url": "http://arxiv.org/abs/2507.02851v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02850v1",
    "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users",
    "authors": [
      "Almog Hilel",
      "Idan Shenfeld",
      "Leshem Choshen",
      "Jacob Andreas"
    ],
    "abstract": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a \"poisoned\" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02850v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02847v1",
    "title": "MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis",
    "authors": [
      "Kunyu Zhang",
      "Qiang Li",
      "Shujian Yu"
    ],
    "abstract": "Recent evidence suggests that modeling higher-order interactions (HOIs) in functional magnetic resonance imaging (fMRI) data can enhance the diagnostic accuracy of machine learning systems. However, effectively extracting and utilizing HOIs remains a significant challenge. In this work, we propose MvHo-IB, a novel multi-view learning framework that integrates both pairwise interactions and HOIs for diagnostic decision-making, while automatically compressing task-irrelevant redundant information. MvHo-IB introduces several key innovations: (1) a principled method that combines O-information from information theory with a matrix-based Renyi alpha-order entropy estimator to quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to effectively utilize these interactions, and (3) a new multi-view learning information bottleneck objective to enhance representation learning. Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves state-of-the-art performance, significantly outperforming previous methods, including recent hypergraph-based techniques. The implementation of MvHo-IB is available at https://github.com/zky04/MvHo-IB.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02847v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02846v1",
    "title": "Legal Requirements Translation from Law",
    "authors": [
      "Anmol Singhal",
      "Travis Breaux"
    ],
    "abstract": "Software systems must comply with legal regulations, which is a resource-intensive task, particularly for small organizations and startups lacking dedicated legal expertise. Extracting metadata from regulations to elicit legal requirements for software is a critical step to ensure compliance. However, it is a cumbersome task due to the length and complex nature of legal text. Although prior work has pursued automated methods for extracting structural and semantic metadata from legal text, key limitations remain: they do not consider the interplay and interrelationships among attributes associated with these metadata types, and they rely on manual labeling or heuristic-driven machine learning, which does not generalize well to new documents. In this paper, we introduce an approach based on textual entailment and in-context learning for automatically generating a canonical representation of legal text, encodable and executable as Python code. Our representation is instantiated from a manually designed Python class structure that serves as a domain-specific metamodel, capturing both structural and semantic legal metadata and their interrelationships. This design choice reduces the need for large, manually labeled datasets and enhances applicability to unseen legislation. We evaluate our approach on 13 U.S. state data breach notification laws, demonstrating that our generated representations pass approximately 89.4% of test cases and achieve a precision and recall of 82.2 and 88.7, respectively.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE",
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02846v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02843v1",
    "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding",
    "authors": [
      "Yuchen Ma",
      "Dennis Frauen",
      "Jonas Schweisthal",
      "Stefan Feuerriegel"
    ],
    "abstract": "Estimating treatment effects is crucial for personalized decision-making in medicine, but this task faces unique challenges in clinical practice. At training time, models for estimating treatment effects are typically trained on well-structured medical datasets that contain detailed patient information. However, at inference time, predictions are often made using textual descriptions (e.g., descriptions with self-reported symptoms), which are incomplete representations of the original patient information. In this work, we make three contributions. (1) We show that the discrepancy between the data available during training time and inference time can lead to biased estimates of treatment effects. We formalize this issue as an inference time text confounding problem, where confounders are fully observed during training time but only partially available through text at inference time. (2) To address this problem, we propose a novel framework for estimating treatment effects that explicitly accounts for inference time text confounding. Our framework leverages large language models together with a custom doubly robust learner to mitigate biases caused by the inference time text confounding. (3) Through a series of experiments, we demonstrate the effectiveness of our framework in real-world applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02843v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02841v1",
    "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason",
    "authors": [
      "Kaiyi Zhang",
      "Ang Lv",
      "Jinpeng Li",
      "Yongbo Wang",
      "Feng Wang",
      "Haoyuan Hu",
      "Rui Yan"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02841v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02834v1",
    "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning",
    "authors": [
      "Ruiyang Zhou",
      "Shuozhe Li",
      "Amy Zhang",
      "Liu Leqi"
    ],
    "abstract": "Recent advances in large language models have been driven by reinforcement learning (RL)-style post-training, which improves reasoning by optimizing model outputs based on reward or preference signals. GRPO-style approaches implement this by using self-generated samples labeled by an outcome-based verifier. However, these methods depend heavily on the model's initial ability to produce positive samples. They primarily refine what the model already knows (distribution sharpening) rather than enabling the model to solve problems where it initially fails. This limitation is especially problematic in early-stage RL training and on challenging reasoning tasks, where positive samples are unlikely to be generated. To unlock reasoning ability in such settings, the model must explore new reasoning trajectories beyond its current output distribution. Such exploration requires access to sufficiently good positive samples to guide the learning. While expert demonstrations seem like a natural solution, we find that they are often ineffective in RL post-training. Instead, we identify two key properties of effective positive samples: they should (1) be likely under the current policy, and (2) increase the model's likelihood of predicting the correct answer. Based on these insights, we propose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and modular framework that generates such samples by conditioning on the ground-truth answer. ExPO enables efficient exploration and guides the model to produce reasoning trajectories more aligned with its policy than expert-written CoTs, while ensuring higher quality than its own (incorrect) samples. Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5, where the model initially struggles the most.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02834v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02833v1",
    "title": "Generalizing Verifiable Instruction Following",
    "authors": [
      "Valentina Pyatkin",
      "Saumya Malik",
      "Victoria Graf",
      "Hamish Ivison",
      "Shengyi Huang",
      "Pradeep Dasigi",
      "Nathan Lambert",
      "Hannaneh Hajishirzi"
    ],
    "abstract": "A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02833v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02832v1",
    "title": "LCQNN: Linear Combination of Quantum Neural Networks",
    "authors": [
      "Hongshun Yao",
      "Xia Liu",
      "Mingrui Jing",
      "Xin Wang"
    ],
    "abstract": "Quantum neural networks combine quantum computing with advanced data-driven methods, offering promising applications in quantum machine learning. However, the optimal paradigm of quantum neural networks in the context trainability and expressivity remains an open question. To overcome this issue, we first introduce a framework called Linear Combination of Quantum Neural Networks (LCQNN), which leverages the linear combination of unitaries concept to coordinate the composition of QNNs, thereby balancing trainability and expressivity. Secondly, LCQNN provides a tunable design that can reduce vanishing gradients without incurring excessive classical simulability. Specifically, we detail how restricting certain subspaces or adopting $k$-local control unitaries prevents gradients from collapsing while maintaining enough parameter volume for complex tasks. These findings also align with prior discussions on expanding QNN capabilities for multi-qubit systems. Additionally, we extend the LCQNN to the group action scenarios. By exploiting symmetry, the LCQNN model, excluding exponentially large irreducible subspaces, can circumvent barren plateaus. Overall, LCQNN provides a novel framework for focusing quantum resources into architectures that remain practically trainable yet expressive enough to tackle challenging machine-learning applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02832v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02826v1",
    "title": "Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach",
    "authors": [
      "Panpan Ji",
      "Junni Song",
      "Hang Xiao",
      "Hanyu Liu",
      "Chao Li"
    ],
    "abstract": "Sensor-based Human Activity Recognition (HAR) is a core technology that enables intelligent systems to perceive and interact with their environment. However, multimodal HAR systems still encounter key challenges, such as difficulties in cross-modal feature alignment and imbalanced modality contributions. To address these issues, we propose a novel framework called the Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three key components. First, a dual-path feature extraction architecture is employed, where ResNet and DenseNet branches collaboratively process multimodal sensor data. Second, a multi-stage contrastive learning mechanism is introduced to achieve progressive alignment from local perception to semantic abstraction. Third, we present a confidence-driven gradient modulation strategy that dynamically monitors and adjusts the learning intensity of each modality branch during backpropagation, effectively alleviating modality competition. In addition, a momentum-based gradient accumulation strategy is adopted to enhance training stability. We conduct ablation studies to validate the effectiveness of each component and perform extensive comparative experiments on four public benchmark datasets.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02826v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02824v1",
    "title": "DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift",
    "authors": [
      "Po-Heng Chou",
      "Ching-Wen Chen",
      "Wan-Jen Huang",
      "Walid Saad",
      "Yu Tsao",
      "Ronald Y. Chang"
    ],
    "abstract": "In this paper, the precoding design is investigated for maximizing the throughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO) systems with obstructed direct communication paths. In particular, a reconfigurable intelligent surface (RIS) is employed to enhance MIMO transmissions, considering mmWave characteristics related to line-of-sight (LoS) and multipath effects. The traditional exhaustive search (ES) for optimal codewords in the continuous phase shift is computationally intensive and time-consuming. To reduce computational complexity, permuted discrete Fourier transform (DFT) vectors are used for finding codebook design, incorporating amplitude responses for practical or ideal RIS systems. However, even if the discrete phase shift is adopted in the ES, it results in significant computation and is time-consuming. Instead, the trained deep neural network (DNN) is developed to facilitate faster codeword selection. Simulation results show that the DNN maintains sub-optimal spectral efficiency even as the distance between the end-user and the RIS has variations in the testing phase. These results highlight the potential of DNN in advancing RIS-aided systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02824v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02822v1",
    "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model",
    "authors": [
      "Wencheng Zhang",
      "Shiqin Qiao",
      "Lingjie Luo",
      "Yinfeng Li",
      "Chuanyang Zheng",
      "Qian Xu",
      "Meng Li",
      "Yong Gui",
      "Yijun He",
      "Jianing Qiu",
      "Jindong Hong",
      "Jiankai Sun"
    ],
    "abstract": "With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between \"thinking\" (high reasoning) and \"non-thinking\" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02822v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02819v1",
    "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks",
    "authors": [
      "Luke Guerdan",
      "Devansh Saxena",
      "Stevie Chancellor",
      "Zhiwei Steven Wu",
      "Kenneth Holstein"
    ],
    "abstract": "Data scientists often formulate predictive modeling tasks involving fuzzy, hard-to-define concepts, such as the \"authenticity\" of student writing or the \"healthcare need\" of a patient. Yet the process by which data scientists translate fuzzy concepts into a concrete, proxy target variable remains poorly understood. We interview fifteen data scientists in education (N=8) and healthcare (N=7) to understand how they construct target variables for predictive modeling tasks. Our findings suggest that data scientists construct target variables through a bricolage process, involving iterative negotiation between high-level measurement objectives and low-level practical constraints. Data scientists attempt to satisfy five major criteria for a target variable through bricolage: validity, simplicity, predictability, portability, and resource requirements. To achieve this, data scientists adaptively use problem (re)formulation strategies, such as swapping out one candidate target variable for another when the first fails to meet certain criteria (e.g., predictability), or composing multiple outcomes into a single target variable to capture a more holistic set of modeling objectives. Based on our findings, we present opportunities for future HCI, CSCW, and ML research to better support the art and science of target variable construction.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.CY",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02819v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02818v1",
    "title": "Genetic Features for Drug Responses in Cancer -- Investigating an Ensemble-Feature-Selection Approach",
    "authors": [
      "Johannes Schlüter",
      "Alexander Schönhuth"
    ],
    "abstract": "Predicting drug responses using genetic and transcriptomic features is crucial for enhancing personalized medicine. In this study, we implemented an ensemble of machine learning algorithms to analyze the correlation between genetic and transcriptomic features of cancer cell lines and IC50 values, a reliable metric for drug efficacy. Our analysis involved a reduction of the feature set from an original pool of 38,977 features, demonstrating a strong linear relationship between genetic features and drug responses across various algorithms, including SVR, Linear Regression, and Ridge Regression. Notably, copy number variations (CNVs) emerged as more predictive than mutations, suggesting a significant reevaluation of biomarkers for drug response prediction. Through rigorous statistical methods, we identified a highly reduced set of 421 critical features. This set offers a novel perspective that contrasts with traditional cancer driver genes, underscoring the potential for these biomarkers in designing targeted therapies. Furthermore, our findings advocate for IC50 values as a predictable measurement of drug responses and underscore the need for more data that can represent the dimensionality of genomic data in drug response prediction. Future work will aim to expand the dataset and refine feature selection to enhance the generalizability of the predictive model in clinical settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "q-bio.GN"
    ],
    "url": "http://arxiv.org/abs/2507.02818v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02817v1",
    "title": "ML-based muon identification using a FNAL-NICADD scintillator chamber for the MID subsystem of ALICE 3",
    "authors": [
      "Jesus Eduardo Muñoz Mendez",
      "Antonio Ortiz",
      "Alom Antonio Paz Jimenez",
      "Paola Vargas Torres",
      "Ruben Alfaro Molina",
      "Laura Helena González Trueba",
      "Varlen Grabski",
      "Arturo Fernandez Tellez",
      "Hector David Regules Medel",
      "Mario Rodriguez Cahuantzi",
      "Guillermo Tejeda Muñoz",
      "Yael Antonio Vasquez Beltran",
      "Juan Carlos Cabanillas Noris",
      "Solangel Rojas Torres",
      "Gergely Gabor Barnafoldi",
      "Daniel Szaraz",
      "Dezso Varga",
      "Robert Vertesi",
      "Edmundo Garciaa Solis"
    ],
    "abstract": "The ALICE Collaboration is planning to construct a new detector (ALICE 3) aiming at exploiting the potential of the high-luminosity Large Hadron Collider (LHC). The new detector will allow ALICE to participate in LHC Run 5 scheduled from 2036 to 2041. The muon-identifier subsystem (MID) is part of the ALICE 3 reference detector layout. The MID will consist of a standard magnetic iron absorber ($\\approx4$ nuclear interaction lengths) followed by muon chambers. The baseline option for the MID chambers considers plastic scintillation bars equipped with wave-length shifting fibers and readout with silicon photomultipliers. This paper reports on the performance of a MID chamber prototype using 3 GeV/$c$ pion- and muon-enriched beams delivered by the CERN Proton Synchrotron (PS). The prototype was built using extruded plastic scintillator produced by FNAL-NICADD (Fermi National Accelerator Laboratory - Northern Illinois Center for Accelerator and Detector Development). The prototype was experimentally evaluated using varying absorber thicknesses (60, 70, 80, 90, and 100 cm) to assess its performance. The analysis was performed using Machine Learning techniques and the performance was validated with GEANT 4 simulations. Potential improvements in both hardware and data analysis are discussed.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.ins-det",
      "hep-ex"
    ],
    "url": "http://arxiv.org/abs/2507.02817v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02815v1",
    "title": "Towards Perception-Informed Latent HRTF Representations",
    "authors": [
      "You Zhang",
      "Andrew Francl",
      "Ruohan Gao",
      "Paul Calamia",
      "Zhiyao Duan",
      "Ishwarya Ananthabhotla"
    ],
    "abstract": "Personalized head-related transfer functions (HRTFs) are essential for ensuring a realistic auditory experience over headphones, because they take into account individual anatomical differences that affect listening. Most machine learning approaches to HRTF personalization rely on a learned low-dimensional latent space to generate or select custom HRTFs for a listener. However, these latent representations are typically learned in a manner that optimizes for spectral reconstruction but not for perceptual compatibility, meaning they may not necessarily align with perceptual distance. In this work, we first study whether traditionally learned HRTF representations are well correlated with perceptual relations using auditory-based objective perceptual metrics; we then propose a method for explicitly embedding HRTFs into a perception-informed latent space, leveraging a metric-based loss function and supervision via Metric Multidimensional Scaling (MMDS). Finally, we demonstrate the applicability of these learned representations to the task of HRTF personalization. We suggest that our method has the potential to render personalized spatial audio, leading to an improved listening experience.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.AS",
      "cs.SD"
    ],
    "url": "http://arxiv.org/abs/2507.02815v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02814v1",
    "title": "Replicable Distribution Testing",
    "authors": [
      "Ilias Diakonikolas",
      "Jingyi Gao",
      "Daniel Kane",
      "Sihan Liu",
      "Christopher Ye"
    ],
    "abstract": "We initiate a systematic investigation of distribution testing in the framework of algorithmic replicability. Specifically, given independent samples from a collection of probability distributions, the goal is to characterize the sample complexity of replicably testing natural properties of the underlying distributions. On the algorithmic front, we develop new replicable algorithms for testing closeness and independence of discrete distributions. On the lower bound front, we develop a new methodology for proving sample complexity lower bounds for replicable testing that may be of broader interest. As an application of our technique, we establish near-optimal sample complexity lower bounds for replicable uniformity testing -- answering an open question from prior work -- and closeness testing.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "G.3"
    ],
    "url": "http://arxiv.org/abs/2507.02814v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02810v1",
    "title": "Advancements in Computing and Simulation Techniques for the HIBEAM-NNBAR Experiment",
    "authors": [
      "Bernhard Meirose",
      "Jorge Amaral",
      "Alexander Burgman",
      "Matthias Holl",
      "Ernesto Kemp",
      "Adam Kozela",
      "David Milstead",
      "André Nepomuceno",
      "Anders Oskarsson",
      "Krzysztof Pysz",
      "Valentina Santoro",
      "Tiago Quirino",
      "Blahoslav Rataj",
      "Gabriel Silva",
      "Samuel Silverstein",
      "Magnus Wolke",
      "Lucas Åstrand"
    ],
    "abstract": "The HIBEAM-NNBAR program is a proposed two-stage experiment at the European Spallation Source focusing on searches for baryon number violation processes as well as ultralight dark matter. This paper presents recent advancements in computing and simulation, including machine learning for event selection, fast parametric simulations for detector studies, and detailed modeling of the time projection chamber and readout electronics.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.ins-det"
    ],
    "url": "http://arxiv.org/abs/2507.02810v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02808v1",
    "title": "Prediction of synthesis parameters for N, Si, Ge and Sn diamond vacancy centers using machine learning",
    "authors": [
      "Zhi Jiang",
      "Marco Peres",
      "Carlo Bradac",
      "Gil Gonçalves"
    ],
    "abstract": "Diamond and diamond color centers have become prime hardware candidates for solid state-based technologies in quantum information and computing, optics, photonics and (bio)sensing. The synthesis of diamond materials with specific characteristics and the precise control of the hosted color centers is thus essential to meet the demands of advanced applications. Yet, challenges remain in improving the concentration, uniform distribution and quality of these centers. Here we perform a review and meta-analysis of some of the main diamond synthesis methods and their parameters for the synthesis of N-, Si-, Ge- and Sn-vacancy color-centers, including worldwide trends in fabrication techniques and processes. We extract quantitative data from over 60 experimental papers and organize it in a large database (170 data sets and 1692 entries). We then use the database to train two machine learning algorithms to make robust predictions about the fabrication of diamond materials with specific properties from careful combinations of synthesis parameters. We use traditional statistical indicators to benchmark the performance of the algorithms and show that they are powerful and resource-efficient tools for researchers and material scientists working with diamond color centers and their applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mtrl-sci",
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02808v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02807v1",
    "title": "In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization",
    "authors": [
      "Thiti Suttaket",
      "Stanley Kok"
    ],
    "abstract": "Survival analysis is an important problem in healthcare because it models the relationship between an individual's covariates and the onset time of an event of interest (e.g., death). It is important for survival models to be well-calibrated (i.e., for their predicted probabilities to be close to ground-truth probabilities) because badly calibrated systems can result in erroneous clinical decisions. Existing survival models are typically calibrated at the population level only, and thus run the risk of being poorly calibrated for one or more minority subpopulations. We propose a model called GRADUATE that achieves multicalibration by ensuring that all subpopulations are well-calibrated too. GRADUATE frames multicalibration as a constrained optimization problem, and optimizes both calibration and discrimination in-training to achieve a good balance between them. We mathematically prove that the optimization method used yields a solution that is both near-optimal and feasible with high probability. Empirical comparisons against state-of-the-art baselines on real-world clinical datasets demonstrate GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of the baselines vis-a-vis GRADUATE's strengths.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02807v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02804v1",
    "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective",
    "authors": [
      "Wenhao Shi",
      "Zhiqiang Hu",
      "Yi Bin",
      "Yang Yang",
      "See-Kiong Ng",
      "Heng Tao Shen"
    ],
    "abstract": "Recent progress in large-scale reinforcement learning (RL) has notably enhanced the reasoning capabilities of large language models (LLMs), especially in mathematical domains. However, current multimodal LLMs (MLLMs) for mathematical reasoning often rely on one-to-one image-text pairs and single-solution supervision, overlooking the diversity of valid reasoning perspectives and internal reflections. In this work, we introduce MathV-DP, a novel dataset that captures multiple diverse solution trajectories for each image-question pair, fostering richer reasoning supervision. We further propose Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and enhanced via group relative policy optimization (GRPO), a rule-based RL approach that integrates correctness discrimination and diversity-aware reward functions. Our method emphasizes learning from varied reasoning perspectives and distinguishing between correct yet distinct solutions. Extensive experiments on the MathVista's minitest and Math-V benchmarks demonstrate that Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and generative diversity, highlighting the importance of incorporating diverse perspectives and reflective reasoning in multimodal mathematical reasoning.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02804v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02801v1",
    "title": "Learning to Coordinate Bidders in Non-Truthful Auctions",
    "authors": [
      "Hu Fu",
      "Tao Lin"
    ],
    "abstract": "In non-truthful auctions such as first-price and all-pay auctions, the independent strategic behaviors of bidders, with the corresponding equilibrium notion -- Bayes Nash equilibria -- are notoriously difficult to characterize and can cause undesirable outcomes. An alternative approach to designing better auction systems is to coordinate the bidders: let a mediator make incentive-compatible recommendations of correlated bidding strategies to the bidders, namely, implementing a Bayes correlated equilibrium (BCE). The implementation of BCE, however, requires knowledge of the distribution of bidders' private valuations, which is often unavailable. We initiate the study of the sample complexity of learning Bayes correlated equilibria in non-truthful auctions. We prove that the BCEs in a large class of non-truthful auctions, including first-price and all-pay auctions, can be learned with a polynomial number $\\tilde O(\\frac{n}{\\varepsilon^2})$ of samples from the bidders' value distributions. Our technique is a reduction to the problem of estimating bidders' expected utility from samples, combined with an analysis of the pseudo-dimension of the class of all monotone bidding strategies of bidders.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.GT",
      "cs.LG",
      "econ.TH"
    ],
    "url": "http://arxiv.org/abs/2507.02801v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02798v1",
    "title": "No time to train! Training-Free Reference-Based Instance Segmentation",
    "authors": [
      "Miguel Espinosa",
      "Chenhongyi Yang",
      "Linus Ericsson",
      "Steven McDonagh",
      "Elliot J. Crowley"
    ],
    "abstract": "The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02798v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02791v1",
    "title": "Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient Extraction of Moving Speakers under Weak Guidance",
    "authors": [
      "Jakob Kienegger",
      "Alina Mannanova",
      "Huajian Fang",
      "Timo Gerkmann"
    ],
    "abstract": "Recent works on deep non-linear spatially selective filters demonstrate exceptional enhancement performance with computationally lightweight architectures for stationary speakers of known directions. However, to maintain this performance in dynamic scenarios, resource-intensive data-driven tracking algorithms become necessary to provide precise spatial guidance conditioned on the initial direction of a target speaker. As this additional computational overhead hinders application in resource-constrained scenarios such as real-time speech enhancement, we present a novel strategy utilizing a low-complexity tracking algorithm in the form of a particle filter instead. Assuming a causal, sequential processing style, we introduce temporal feedback to leverage the enhanced speech signal of the spatially selective filter to compensate for the limited modeling capabilities of the particle filter. Evaluation on a synthetic dataset illustrates how the autoregressive interplay between both algorithms drastically improves tracking accuracy and leads to strong enhancement performance. A listening test with real-world recordings complements these findings by indicating a clear trend towards our proposed self-steering pipeline as preferred choice over comparable methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "url": "http://arxiv.org/abs/2507.02791v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02782v1",
    "title": "Understanding and Improving Length Generalization in Recurrent Models",
    "authors": [
      "Ricardo Buitrago Ruiz",
      "Albert Gu"
    ],
    "abstract": "Recently, recurrent models such as state space models and linear attention have become popular due to their linear complexity in the sequence length. Thanks to their recurrent nature, in principle they can process arbitrarily long sequences, but their performance sometimes drops considerably beyond their training context lengths-i.e. they fail to length generalize. In this work, we provide comprehensive empirical and theoretical analysis to support the unexplored states hypothesis, which posits that models fail to length generalize when during training they are only exposed to a limited subset of the distribution of all attainable states (i.e. states that would be attained if the recurrence was applied to long sequences). Furthermore, we investigate simple training interventions that aim to increase the coverage of the states that the model is trained on, e.g. by initializing the state with Gaussian noise or with the final state of a different input sequence. With only 500 post-training steps ($\\sim 0.1\\%$ of the pre-training budget), these interventions enable length generalization for sequences that are orders of magnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and show improved performance in long context tasks, thus presenting a simple and efficient way to enable robust length generalization in general recurrent models.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02782v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02778v1",
    "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs",
    "authors": [
      "Ken Tsui"
    ],
    "abstract": "Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02778v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02773v1",
    "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs",
    "authors": [
      "Yuzhang Xie",
      "Hejie Cui",
      "Ziyang Zhang",
      "Jiaying Lu",
      "Kai Shu",
      "Fadi Nahab",
      "Xiao Hu",
      "Carl Yang"
    ],
    "abstract": "Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "url": "http://arxiv.org/abs/2507.02773v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02771v1",
    "title": "Grounding Intelligence in Movement",
    "authors": [
      "Melanie Segado",
      "Felipe Parodi",
      "Jordan K. Matelsky",
      "Michael L. Platt",
      "Eva B. Dyer",
      "Konrad P. Kording"
    ],
    "abstract": "Recent advances in machine learning have dramatically improved our ability to model language, vision, and other high-dimensional data, yet they continue to struggle with one of the most fundamental aspects of biological systems: movement. Across neuroscience, medicine, robotics, and ethology, movement is essential for interpreting behavior, predicting intent, and enabling interaction. Despite its core significance in our intelligence, movement is often treated as an afterthought rather than as a rich and structured modality in its own right. This reflects a deeper fragmentation in how movement data is collected and modeled, often constrained by task-specific goals and domain-specific assumptions. But movement is not domain-bound. It reflects shared physical constraints, conserved morphological structures, and purposeful dynamics that cut across species and settings. We argue that movement should be treated as a primary modeling target for AI. It is inherently structured and grounded in embodiment and physics. This structure, often allowing for compact, lower-dimensional representations (e.g., pose), makes it more interpretable and computationally tractable to model than raw, high-dimensional sensory inputs. Developing models that can learn from and generalize across diverse movement data will not only advance core capabilities in generative modeling and control, but also create a shared foundation for understanding behavior across biological and artificial systems. Movement is not just an outcome, it is a window into how intelligent systems engage with the world.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "url": "http://arxiv.org/abs/2507.02771v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02762v1",
    "title": "Contextual Online Pricing with (Biased) Offline Data",
    "authors": [
      "Yixuan Zhang",
      "Ruihao Zhu",
      "Qiaomin Xie"
    ],
    "abstract": "We study contextual online pricing with biased offline data. For the scalar price elasticity case, we identify the instance-dependent quantity $\\delta^2$ that measures how far the offline data lies from the (unknown) online optimum. We show that the time length $T$, bias bound $V$, size $N$ and dispersion $\\lambda_{\\min}(\\hat{\\Sigma})$ of the offline data, and $\\delta^2$ jointly determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty (OFU) policy achieves a minimax-optimal, instance-dependent regret bound $\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT}{\\lambda_{\\min}(\\hat{\\Sigma}) + (N \\wedge T) \\delta^2})\\big)$. For general price elasticity, we establish a worst-case, minimax-optimal rate $\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT }{\\lambda_{\\min}(\\hat{\\Sigma})})\\big)$ and provide a generalized OFU algorithm that attains it. When the bias bound $V$ is unknown, we design a robust variant that always guarantees sub-linear regret and strictly improves on purely online methods whenever the exact bias is small. These results deliver the first tight regret guarantees for contextual pricing in the presence of biased offline data. Our techniques also transfer verbatim to stochastic linear bandits with biased offline data, yielding analogous bounds.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "stat.ML"
    ],
    "url": "http://arxiv.org/abs/2507.02762v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02760v1",
    "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work",
    "authors": [
      "Guangwei Zhang"
    ],
    "abstract": "The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02760v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02757v1",
    "title": "Discovery and Preliminary Characterization of a Third Interstellar Object: 3I/ATLAS",
    "authors": [
      "Darryl Z. Seligman",
      "Marco Micheli",
      "Davide Farnocchia",
      "Larry Denneau",
      "John W. Noonan",
      "Toni Santana-Ros",
      "Luca Conversi",
      "Maxime Devogèle",
      "Laura Faggioli",
      "Adina D. Feinstein",
      "Marco Fenucci",
      "Tessa Frincke",
      "Olivier R. Hainaut",
      "Willem B. Hoogendam",
      "Henry H. Hsieh",
      "Theodore Kareta",
      "Michael S. P. Kelley",
      "Tim Lister",
      "Dušan Marčeta",
      "Karen J. Meech",
      "Francisco Ocaña",
      "Eloy Peña-Asensio",
      "Benjamin J. Shappee",
      "Aster G. Taylor",
      "Richard Wainscoat",
      "Robert Weryk",
      "James J. Wray",
      "Atsuhiro Yaginuma",
      "Bin Yang",
      "Quanzhi Ye"
    ],
    "abstract": "We report initial observations aimed at the characterization of a third interstellar object candidate. This object, 3I/ATLAS -- also C/2025 N1 (ATLAS) -- , was discovered on 2025 July 1 UT and has an orbital eccentricity of $e\\sim6.2$, perihelion of $q\\sim 1.35$ au, inclination of $\\sim175^\\circ$, and hyperbolic velocity of $V_\\infty\\sim 60$ km s$^{-1}$. 3I/ATLAS has an absolute magnitude of $H_V\\sim12$, which corresponds to a nuclear radius of $\\sim10\\text{ km}$, assuming an asteroid-like albedo of $p\\sim0.05$. The discovery of this object implies a spatial number density of $n_0\\sim10^{-3}$ au$^{-3}$ for objects with radii greater than or equal to that of 3I/ATLAS. We report deep stacked images obtained using the Canada-France-Hawaii Telescope that display faint activity. Using images obtained from the Las Cumbres Observatory 0.36 m telescopes at Haleakala and the 2.0 m Faulkes Telescope North, we find a small light curve variation of less than 0.2 mag for the object over a $\\sim29$ h time span. The visible/near-infrared spectral slope of the object is red, comparable to 1I/`Oumuamua. The object will be observable until September 2025, unobservable near perihelion due to low solar elongation, and observable again in November. This limitation unfortunately prohibits detailed observations at perihelion when the activity of 3I/ATLAS is likely to peak. Based on the experience of 1I/`Oumuamua and 2I/Borisov, we ask the community to maintain a constant observational presence while possible with photometric, spectroscopic, and polarimetric methods. Such observational data would constrain the (i) light curve, (ii) onset and variation of activity, and (iii) nongravitational effects. It is essential that the community collaborate to rapidly and comprehensively characterize these properties of 3I/ATLAS.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.IM"
    ],
    "url": "http://arxiv.org/abs/2507.02757v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02756v1",
    "title": "Generation of Intense Deep-Ultraviolet Pulses at 200 nm",
    "authors": [
      "X. Xie",
      "S. Soultanis",
      "G. Knopp",
      "A. L. Cavalieri",
      "S. L. Johnson"
    ],
    "abstract": "We report the generation of intense deep ultraviolet pulses at 200 nm with a duration of 48 fs and pulse energy of 130 uJ, achieved via cascaded sum frequency generation using 800 nm femtosecond pulses in barium borate crystals. Efficient frequency up-conversion is realized by optimizing phase-matching conditions and implementing dispersion control, while maintaining the ultrashort pulse characteristics. The generated deep ultraviolet pulses are characterized using two-photon absorption frequency-resolved optical gating, providing detailed insight into their temporal profile and phase. This approach addresses key challenges in ultrashort deep ultraviolet pulse generation, delivering a high-energy, ultrashort source suitable for ultrafast spectroscopy, nonlinear optics, and strong-field physics. These results represent a significant advancement in the generation of high-energy, ultrashort deep ultraviolet pulses, opening new possibilities for time-resolved investigations in ultrafast molecular dynamics, as well as emerging applications in semiconductor science, quantum materials, and photochemistry.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.optics"
    ],
    "url": "http://arxiv.org/abs/2507.02756v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02754v1",
    "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "authors": [
      "Aurko Roy",
      "Timothy Chou",
      "Sai Surya Duvvuri",
      "Sijia Chen",
      "Jiecao Yu",
      "Xiaodong Wang",
      "Manzil Zaheer",
      "Rohan Anil"
    ],
    "abstract": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02754v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02752v1",
    "title": "Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular Analog Generation",
    "authors": [
      "Shuan Chen",
      "Gunwook Nam",
      "Yousung Jung"
    ],
    "abstract": "The disconnect between AI-generated molecules with desirable properties and their synthetic feasibility remains a critical bottleneck in computational drug and material discovery. While generative AI has accelerated the proposal of candidate molecules, many of these structures prove challenging or impossible to synthesize using established chemical reactions. Here, we introduce SynTwins, a novel retrosynthesis-guided molecular analog design framework that designs synthetically accessible molecular analogs by emulating expert chemist strategies through a three-step process: retrosynthesis, similar building block searching, and virtual synthesis. In comparative evaluations, SynTwins demonstrates superior performance in generating synthetically accessible analogs compared to state-of-the-art machine learning models while maintaining high structural similarity to original target molecules. Furthermore, when integrated with existing molecule optimization frameworks, our hybrid approach produces synthetically feasible molecules with property profiles comparable to unconstrained molecule generators, yet its synthesizability ensured. Our comprehensive benchmarking across diverse molecular datasets demonstrates that SynTwins effectively bridges the gap between computational design and experimental synthesis, providing a practical solution for accelerating the discovery of synthesizable molecules with desired properties for a wide range of applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.chem-ph",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02752v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02751v1",
    "title": "Partial Weakly-Supervised Oriented Object Detection",
    "authors": [
      "Mingxin Liu",
      "Peiyuan Zhang",
      "Yuan Liu",
      "Wei Zhang",
      "Yue Zhou",
      "Ning Liao",
      "Ziyang Gong",
      "Junwei Luo",
      "Zhirui Wang",
      "Yi Yu",
      "Xue Yang"
    ],
    "abstract": "The growing demand for oriented object detection (OOD) across various domains has driven significant research in this area. However, the high cost of dataset annotation remains a major concern. Current mainstream OOD algorithms can be mainly categorized into three types: (1) fully supervised methods using complete oriented bounding box (OBB) annotations, (2) semi-supervised methods using partial OBB annotations, and (3) weakly supervised methods using weak annotations such as horizontal boxes or points. However, these algorithms inevitably increase the cost of models in terms of annotation speed or annotation cost. To address this issue, we propose:(1) the first Partial Weakly-Supervised Oriented Object Detection (PWOOD) framework based on partially weak annotations (horizontal boxes or single points), which can efficiently leverage large amounts of unlabeled data, significantly outperforming weakly supervised algorithms trained with partially weak annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware Student (OS-Student) model capable of learning orientation and scale information with only a small amount of orientation-agnostic or scale-agnostic weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF) to reduce the model's sensitivity to static filtering thresholds. Comprehensive experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD framework performs comparably to, or even surpasses, traditional semi-supervised algorithms.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02751v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02748v1",
    "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics",
    "authors": [
      "Alex Colagrande",
      "Paul Caillon",
      "Eva Feillet",
      "Alexandre Allauzen"
    ],
    "abstract": "Transformers have become the de facto standard for a wide range of tasks, from image classification to physics simulations. Despite their impressive performance, the quadratic complexity of standard Transformers in both memory and time with respect to the input length makes them impractical for processing high-resolution inputs. Therefore, several variants have been proposed, the most successful relying on patchification, downsampling, or coarsening techniques, often at the cost of losing the finest-scale details. In this work, we take a different approach. Inspired by state-of-the-art techniques in $n$-body numerical simulations, we cast attention as an interaction problem between grid points. We introduce the Multipole Attention Neural Operator (MANO), which computes attention in a distance-based multiscale fashion. MANO maintains, in each attention head, a global receptive field and achieves linear time and memory complexity with respect to the number of grid points. Empirical results on image classification and Darcy flows demonstrate that MANO rivals state-of-the-art models such as ViT and Swin Transformer, while reducing runtime and peak memory usage by orders of magnitude. We open source our code for reproducibility at https://github.com/AlexColagrande/MANO.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02748v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02743v1",
    "title": "Prompt learning with bounding box constraints for medical image segmentation",
    "authors": [
      "Mélanie Gaillochet",
      "Mehrdad Noori",
      "Sahar Dastani",
      "Christian Desrosiers",
      "Hervé Lombaert"
    ],
    "abstract": "Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multimodal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The code is available at https://github.com/Minimel/box-prompt-learning-VFM.git",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02743v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02737v1",
    "title": "Early Signs of Steganographic Capabilities in Frontier LLMs",
    "authors": [
      "Artur Zolkowski",
      "Kei Nishimura-Gasparian",
      "Robert McCarthy",
      "Roland S. Zimmermann",
      "David Lindner"
    ],
    "abstract": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances such as using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02737v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02734v1",
    "title": "Leveraging Transformer Models to Capture Multi-Scale Dynamics in Biomolecules by nano-GPT",
    "authors": [
      "Wenqi Zeng",
      "Lu Zhang",
      "Yuan Yao"
    ],
    "abstract": "Long-term biomolecular dynamics are critical for understanding key evolutionary transformations in molecular systems. However, capturing these processes requires extended simulation timescales that often exceed the practical limits of conventional models. To address this, shorter simulations, initialized with diverse perturbations, are commonly used to sample phase space and explore a wide range of behaviors. Recent advances have leveraged language models to infer long-term behavior from short trajectories, but methods such as long short-term memory (LSTM) networks are constrained to low-dimensional reaction coordinates, limiting their applicability to complex systems. In this work, we present nano-GPT, a novel deep learning model inspired by the GPT architecture, specifically designed to capture long-term dynamics in molecular systems with fine-grained conformational states and complex transitions. The model employs a two-pass training mechanism that incrementally replaces molecular dynamics (MD) tokens with model-generated predictions, effectively mitigating accumulation errors inherent in the training window. We validate nano-GPT on three distinct systems: a four-state model potential, the alanine dipeptide, a well-studied simple molecule, and the Fip35 WW domain, a complex biomolecular system. Our results show that nano-GPT effectively captures long-timescale dynamics by learning high-order dependencies through attention mechanism, offering a novel perspective for interpreting biomolecular processes.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "q-bio.QM"
    ],
    "url": "http://arxiv.org/abs/2507.02734v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02732v1",
    "title": "Classification by Separating Hypersurfaces: An Entropic Approach",
    "authors": [
      "Argimiro Arratia",
      "Mahmoud El Daou",
      "Henryk Gzyl"
    ],
    "abstract": "We consider the following classification problem: Given a population of individuals characterized by a set of attributes represented as a vector in ${\\mathbb R}^N$, the goal is to find a hyperplane in ${\\mathbb R}^N$ that separates two sets of points corresponding to two distinct classes. This problem, with a history dating back to the perceptron model, remains central to machine learning. In this paper we propose a novel approach by searching for a vector of parameters in a bounded $N$-dimensional hypercube centered at the origin and a positive vector in ${\\mathbb R}^M$, obtained through the minimization of an entropy-based function defined over the space of unknown variables. The method extends to polynomial surfaces, allowing the separation of data points by more complex decision boundaries. This provides a robust alternative to traditional linear or quadratic optimization techniques, such as support vector machines and gradient descent. Numerical experiments demonstrate the efficiency and versatility of the method in handling diverse classification tasks, including linear and non-linear separability.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "physics.data-an",
      "stat.ML",
      "90C05, 90C25, 90C47, 90C52, 68T01, 68T05, 68T07, 68T20, 68W01"
    ],
    "url": "http://arxiv.org/abs/2507.02732v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02726v1",
    "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
    "authors": [
      "Matthieu Zimmer",
      "Xiaotong Ji",
      "Rasul Tutunov",
      "Anthony Bordg",
      "Jun Wang",
      "Haitham Bou Ammar"
    ],
    "abstract": "Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02726v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02724v1",
    "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms",
    "authors": [
      "Shiyi Liu",
      "Buwen Liang",
      "Yuetong Fang",
      "Zixuan Jiang",
      "Renjing Xu"
    ],
    "abstract": "Recent advances in AI for science have highlighted the power of contrastive learning in bridging heterogeneous biological data modalities. Building on this paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction across Organisms), a hierarchical contrastive framework for protein-protein interaction(PPI) prediction, where protein sequences and their hierarchical attributes are aligned through multi-tiered biological representation matching. The proposed approach incorporates hierarchical contrastive loss functions that emulate the structured relationship among functional classes of proteins. The framework adaptively incorporates domain and family knowledge through a data-driven penalty mechanism, enforcing consistency between the learned embedding space and the intrinsic hierarchy of protein functions. Experiments on benchmark datasets demonstrate that HIPPO achieves state-of-the-art performance, outperforming existing methods and showing robustness in low-data regimes. Notably, the model demonstrates strong zero-shot transferability to other species without retraining, enabling reliable PPI prediction and functional inference even in less characterized or rare organisms where experimental data are limited. Further analysis reveals that hierarchical feature fusion is critical for capturing conserved interaction determinants, such as binding motifs and functional annotations. This work advances cross-species PPI prediction and provides a unified framework for interaction prediction in scenarios with sparse or imbalanced multi-species data.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "q-bio.BM"
    ],
    "url": "http://arxiv.org/abs/2507.02724v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02715v1",
    "title": "A Comprehensive Machine Learning Framework for Micromobility Demand Prediction",
    "authors": [
      "Omri Porat",
      "Michael Fire",
      "Eran Ben-Elia"
    ],
    "abstract": "Dockless e-scooters, a key micromobility service, have emerged as eco-friendly and flexible urban transport alternatives. These services improve first and last-mile connectivity, reduce congestion and emissions, and complement public transport for short-distance travel. However, effective management of these services depends on accurate demand prediction, which is crucial for optimal fleet distribution and infrastructure planning. While previous studies have focused on analyzing spatial or temporal factors in isolation, this study introduces a framework that integrates spatial, temporal, and network dependencies for improved micromobility demand forecasting. This integration enhances accuracy while providing deeper insights into urban micromobility usage patterns. Our framework improves demand prediction accuracy by 27 to 49% over baseline models, demonstrating its effectiveness in capturing micromobility demand patterns. These findings support data-driven micromobility management, enabling optimized fleet distribution, cost reduction, and sustainable urban planning.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02715v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02714v1",
    "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models",
    "authors": [
      "Yuxuan Wang",
      "Tianwei Cao",
      "Huayu Zhang",
      "Zhongjiang He",
      "Kongming Liang",
      "Zhanyu Ma"
    ],
    "abstract": "Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02714v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02712v1",
    "title": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control",
    "authors": [
      "Zilin Kang",
      "Chenyuan Hu",
      "Yu Luo",
      "Zhecheng Yuan",
      "Ruijie Zheng",
      "Huazhe Xu"
    ],
    "abstract": "Deep reinforcement learning for continuous control has recently achieved impressive progress. However, existing methods often suffer from primacy bias, a tendency to overfit early experiences stored in the replay buffer, which limits an RL agent's sample efficiency and generalizability. In contrast, humans are less susceptible to such bias, partly due to infantile amnesia, where the formation of new neurons disrupts early memory traces, leading to the forgetting of initial experiences. Inspired by this dual processes of forgetting and growing in neuroscience, in this paper, we propose Forget and Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First, Experience Replay Decay (ER Decay) \"forgetting early experience\", which balances memory by gradually reducing the influence of early experiences. Second, Network Expansion, \"growing neural capacity\", which enhances agents' capability to exploit the patterns of existing data by dynamically adding new parameters during training. Empirical results on four major continuous control benchmarks with more than 40 tasks demonstrate the superior performance of FoG against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02712v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02710v1",
    "title": "Fluid Democracy in Federated Data Aggregation",
    "authors": [
      "Aditya Vema Reddy Kesari",
      "Krishna Reddy Kesari"
    ],
    "abstract": "Federated learning (FL) mechanisms typically require each client to transfer their weights to a central server, irrespective of how useful they are. In order to avoid wasteful data transfer costs from clients to the central server, we propose the use of consensus based protocols to identify a subset of clients with most useful model weights at each data transfer step. First, we explore the application of existing fluid democracy protocols to FL from a performance standpoint, comparing them with traditional one-person-one-vote (also known as 1p1v or FedAvg). We propose a new fluid democracy protocol named viscous-retained democracy that always does better than 1p1v under the same assumptions as existing fluid democracy protocols while also not allowing for influence accumulation. Secondly, we identify weaknesses of fluid democracy protocols from an adversarial lens in terms of their dependence on topology and/ or number of adversaries required to negatively impact the global model weights. To this effect, we propose an algorithm (FedVRD) that dynamically limits the effect of adversaries while minimizing cost by leveraging the delegation topology.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02710v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02698v1",
    "title": "Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions",
    "authors": [
      "Thomas Hazenberg",
      "Yao Ma",
      "Seyed Sahand Mohammadi Ziabari",
      "Marijn van Rijswijk"
    ],
    "abstract": "This study investigates how Multi-Agent Reinforcement Learning (MARL) can improve dynamic pricing strategies in supply chains, particularly in contexts where traditional ERP systems rely on static, rule-based approaches that overlook strategic interactions among market actors. While recent research has applied reinforcement learning to pricing, most implementations remain single-agent and fail to model the interdependent nature of real-world supply chains. This study addresses that gap by evaluating the performance of three MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines, within a simulated environment informed by real e-commerce transaction data and a LightGBM demand prediction model. Results show that rule-based agents achieve near-perfect fairness (Jain's Index: 0.9896) and the highest price stability (volatility: 0.024), but they fully lack competitive dynamics. Among MARL agents, MADQN exhibits the most aggressive pricing behaviour, with the highest volatility and the lowest fairness (0.5844). MADDPG provides a more balanced approach, supporting market competition (share volatility: 9.5 pp) while maintaining relatively high fairness (0.8819) and stable pricing. These findings suggest that MARL introduces emergent strategic behaviour not captured by static pricing rules and may inform future developments in dynamic pricing.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "econ.EM"
    ],
    "url": "http://arxiv.org/abs/2507.02698v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02690v1",
    "title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes",
    "authors": [
      "Jiaxing Wang",
      "Yifeng Yu",
      "Jiahan Song",
      "Bin Cao",
      "Jing Fan",
      "Ji Zhang"
    ],
    "abstract": "Next activity prediction represents a fundamental challenge for optimizing business processes in service-oriented architectures such as microservices environments, distributed enterprise systems, and cloud-native platforms, which enables proactive resource allocation and dynamic service composition. Despite the prevalence of sequence-based methods, these approaches fail to capture non-sequential relationships that arise from parallel executions and conditional dependencies. Even though graph-based approaches address structural preservation, they suffer from homogeneous representations and static structures that apply uniform modeling strategies regardless of individual process complexity characteristics. To address these limitations, we introduce RLHGNN, a novel framework that transforms event logs into heterogeneous process graphs with three distinct edge types grounded in established process mining theory. Our approach creates four flexible graph structures by selectively combining these edges to accommodate different process complexities, and employs reinforcement learning formulated as a Markov Decision Process to automatically determine the optimal graph structure for each specific process instance. RLHGNN then applies heterogeneous graph convolution with relation-specific aggregation strategies to effectively predict the next activity. This adaptive methodology enables precise modeling of both sequential and non-sequential relationships in service interactions. Comprehensive evaluation on six real-world datasets demonstrates that RLHGNN consistently outperforms state-of-the-art approaches. Furthermore, it maintains an inference latency of approximately 1 ms per prediction, representing a highly practical solution suitable for real-time business process monitoring applications. The source code is available at https://github.com/Joker3993/RLHGNN.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02690v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02686v1",
    "title": "Learning few-step posterior samplers by unfolding and distillation of diffusion models",
    "authors": [
      "Charlesquin Kemajou Mbakam",
      "Jonathan Spence",
      "Marcelo Pereyra"
    ],
    "abstract": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02686v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02681v1",
    "title": "Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education",
    "authors": [
      "Behnam Parsaeifard",
      "Christof Imhof",
      "Tansu Pancar",
      "Ioan-Sorin Comsa",
      "Martin Hlosta",
      "Nicole Bergamin",
      "Per Bergamin"
    ],
    "abstract": "Students disengaging from their tasks can have serious long-term consequences, including academic drop-out. This is particularly relevant for students in distance education. One way to measure the level of disengagement in distance education is to observe participation in non-mandatory exercises in different online courses. In this paper, we detect student disengagement in the non-mandatory quizzes of 42 courses in four semesters from a distance-based university. We carefully identified the most informative student log data that could be extracted and processed from Moodle. Then, eight machine learning algorithms were trained and compared to obtain the highest possible prediction accuracy. Using the SHAP method, we developed an explainable machine learning framework that allows practitioners to better understand the decisions of the trained algorithm. The experimental results show a balanced accuracy of 91\\%, where about 85\\% of disengaged students were correctly detected. On top of the highly predictive performance and explainable framework, we provide a discussion on how to design a timely intervention to minimise disengagement from voluntary tasks in online learning.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02681v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02675v1",
    "title": "TUC-PPO: Team Utility-Constrained Proximal Policy Optimization for Spatial Public Goods Games",
    "authors": [
      "Zhaoqilin Yang",
      "Xin Wang",
      "Ruichen Zhang",
      "Chanchan Li",
      "Youliang Tian"
    ],
    "abstract": "We introduce Team Utility-Constrained Proximal Policy Optimization (TUC-PPO), a new deep reinforcement learning framework. It extends Proximal Policy Optimization (PPO) by integrating team welfare objectives specifically for spatial public goods games. Unlike conventional approaches where cooperation emerges indirectly from individual rewards, TUC-PPO instead optimizes a bi-level objective integrating policy gradients and team utility constraints. Consequently, all policy updates explicitly incorporate collective payoff thresholds. The framework preserves PPO's policy gradient core while incorporating constrained optimization through adaptive Lagrangian multipliers. Therefore, decentralized agents dynamically balance selfish and cooperative incentives. The comparative analysis demonstrates superior performance of this constrained deep reinforcement learning approach compared to unmodified PPO and evolutionary game theory baselines. It achieves faster convergence to cooperative equilibria and greater stability against invasion by defectors. The framework formally integrates team objectives into policy updates. This work advances multi-agent deep reinforcement learning for social dilemmas while providing new computational tools for evolutionary game theory research.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.GT"
    ],
    "url": "http://arxiv.org/abs/2507.02675v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02672v1",
    "title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping",
    "authors": [
      "Qingyu Fan",
      "Yinghao Cai",
      "Chao Li",
      "Chunting Jiao",
      "Xudong Zheng",
      "Tao Lu",
      "Bin Liang",
      "Shuo Wang"
    ],
    "abstract": "Robotic grasping faces challenges in adapting to objects with varying shapes and sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method that integrates multi-scale feature extraction with contrastive feature enhancement for self-adaptive grasping. We propose a query-based interaction between high-level and low-level features through the Insight Transformer, while the Empower Transformer selectively attends to the highest-level features, which synergistically strikes a balance between focusing on fine geometric details and overall geometric structures. Furthermore, MISCGrasp utilizes multi-scale contrastive learning to exploit similarities among positive grasp samples, ensuring consistency across multi-scale features. Extensive experiments in both simulated and real-world environments demonstrate that MISCGrasp outperforms baseline and variant methods in tabletop decluttering tasks. More details are available at https://miscgrasp.github.io/.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02672v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02671v1",
    "title": "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs",
    "authors": [
      "Francesco Di Salvo",
      "Hanh Huyen My Nguyen",
      "Christian Ledig"
    ],
    "abstract": "Deep Learning (DL) has revolutionized medical imaging, yet its adoption is constrained by data scarcity and privacy regulations, limiting access to diverse datasets. Federated Learning (FL) enables decentralized training but suffers from high communication costs and is often restricted to a single downstream task, reducing flexibility. We propose a data-sharing method via Differentially Private (DP) generative models. By adopting foundation models, we extract compact, informative embeddings, reducing redundancy and lowering computational overhead. Clients collaboratively train a Differentially Private Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware data distribution, supporting diverse downstream tasks. Our approach, validated across multiple feature extractors, enhances privacy, scalability, and efficiency, outperforming traditional FL classifiers while ensuring differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings than DP-CGAN while requiring $5{\\times}$ fewer parameters.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "url": "http://arxiv.org/abs/2507.02671v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02670v1",
    "title": "Guided Generation for Developable Antibodies",
    "authors": [
      "Siqi Zhao",
      "Joshua Moller",
      "Porfi Quintero-Cadena",
      "Lood van Niekerk"
    ],
    "abstract": "Therapeutic antibodies require not only high-affinity target engagement, but also favorable manufacturability, stability, and safety profiles for clinical effectiveness. These properties are collectively called `developability'. To enable a computational framework for optimizing antibody sequences for favorable developability, we introduce a guided discrete diffusion model trained on natural paired heavy- and light-chain sequences from the Observed Antibody Space (OAS) and quantitative developability measurements for 246 clinical-stage antibodies. To steer generation toward biophysically viable candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module that biases sampling without compromising naturalness. In unconstrained sampling, our model reproduces global features of both the natural repertoire and approved therapeutics, and under SVDD guidance we achieve significant enrichment in predicted developability scores over unguided baselines. When combined with high-throughput developability assays, this framework enables an iterative, ML-driven pipeline for designing antibodies that satisfy binding and biophysical criteria in tandem.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "q-bio.BM"
    ],
    "url": "http://arxiv.org/abs/2507.02670v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02668v1",
    "title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection",
    "authors": [
      "Zhe Yee Tan"
    ],
    "abstract": "Colorectal polyp segmentation is critical for early detection of colorectal cancer, yet weak and low contrast boundaries significantly limit automated accuracy. Existing deep models either blur fine edge details or rely on handcrafted filters that perform poorly under variable imaging conditions. We propose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects directional, parameter free Haar wavelet edge maps into each decoder stage to recalibrate semantic features. Our two main contributions are: (1) a two-level Haar wavelet head for multi orientation edge extraction; and (2) Wavelet Edge Guided Attention (WEGA) modules that fuse wavelet cues with reverse and input branches. On five public polyp datasets, MEGANetW consistently outperforms existing methods, improving mIoU by up to 2.3% and mDice by 1.2%, while introducing no additional learnable parameters.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02668v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02666v1",
    "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning",
    "authors": [
      "Junyu Wang",
      "Tianrui Wang",
      "Meng Ge",
      "Longbiao Wang",
      "Jianwu Dang"
    ],
    "abstract": "In recent advancements in audio self-supervised representation learning, the standard Transformer architecture has emerged as the predominant approach, yet its attention mechanism often allocates a portion of attention weights to irrelevant information, potentially impairing the model's discriminative ability. To address this, we introduce a differential attention mechanism, which effectively mitigates ineffective attention allocation through the integration of dual-softmax operations and appropriately tuned differential coefficients. Experimental results demonstrate that our ASDA model achieves state-of-the-art (SOTA) performance across multiple benchmarks, including audio classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting (98.3% accuracy on SPC-2), and environmental sound classification (96.1% accuracy on ESC-50). These results highlight ASDA's effectiveness in audio tasks, paving the way for broader applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "url": "http://arxiv.org/abs/2507.02666v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02665v1",
    "title": "Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?",
    "authors": [
      "Timo Kehrer",
      "Robert Haines",
      "Guido Juckeland",
      "Shurui Zhou",
      "David E. Bernholdt"
    ],
    "abstract": "Anecdotal evidence suggests that Research Software Engineers (RSEs) and Software Engineering Researchers (SERs) often use different terminologies for similar concepts, creating communication challenges. To better understand these divergences, we have started investigating how SE fundamentals from the SER community are interpreted within the RSE community, identifying aligned concepts, knowledge gaps, and areas for potential adaptation. Our preliminary findings reveal opportunities for mutual learning and collaboration, and our systematic methodology for terminology mapping provides a foundation for a crowd-sourced extension and validation in the future.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE"
    ],
    "url": "http://arxiv.org/abs/2507.02665v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02659v1",
    "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding",
    "authors": [
      "Ramchalam Kinattinkara Ramakrishnan",
      "Zhaocong Yuan",
      "Shaojie Zhuo",
      "Chen Feng",
      "Yicheng Lin",
      "Chenzheng Su",
      "Xiaopeng Zhang"
    ],
    "abstract": "Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \\textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02659v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02652v1",
    "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search",
    "authors": [
      "Jiajie Jin",
      "Xiaoxi Li",
      "Guanting Dong",
      "Yuyao Zhang",
      "Yutao Zhu",
      "Yang Zhao",
      "Hongjin Qian",
      "Zhicheng Dou"
    ],
    "abstract": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "url": "http://arxiv.org/abs/2507.02652v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02645v1",
    "title": "Fair Deepfake Detectors Can Generalize",
    "authors": [
      "Harry Cheng",
      "Ming-Hui Liu",
      "Yangyang Guo",
      "Tianyi Wang",
      "Liqiang Nie",
      "Mohan Kankanhalli"
    ],
    "abstract": "Deepfake detection models face two critical challenges: generalization to unseen manipulations and demographic fairness among population groups. However, existing approaches often demonstrate that these two objectives are inherently conflicting, revealing a trade-off between them. In this paper, we, for the first time, uncover and formally define a causal relationship between fairness and generalization. Building on the back-door adjustment, we show that controlling for confounders (data distribution and model capacity) enables improved generalization via fairness interventions. Motivated by this insight, we propose Demographic Attribute-insensitive Intervention Detection (DAID), a plug-and-play framework composed of: i) Demographic-aware data rebalancing, which employs inverse-propensity weighting and subgroup-wise feature normalization to neutralize distributional biases; and ii) Demographic-agnostic feature aggregation, which uses a novel alignment loss to suppress sensitive-attribute signals. Across three cross-domain benchmarks, DAID consistently achieves superior performance in both fairness and generalization compared to several state-of-the-art detectors, validating both its theoretical foundation and practical effectiveness.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02645v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02639v1",
    "title": "On Efficient Bayesian Exploration in Model-Based Reinforcement Learning",
    "authors": [
      "Alberto Caron",
      "Chris Hicks",
      "Vasilios Mavroudis"
    ],
    "abstract": "In this work, we address the challenge of data-efficient exploration in reinforcement learning by examining existing principled, information-theoretic approaches to intrinsic motivation. Specifically, we focus on a class of exploration bonuses that targets epistemic uncertainty rather than the aleatoric noise inherent in the environment. We prove that these bonuses naturally signal epistemic information gains and converge to zero once the agent becomes sufficiently certain about the environment's dynamics and rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis provides formal guarantees for IG-based approaches, which previously lacked theoretical grounding. To enable practical use, we also discuss tractable approximations via sparse variational Gaussian Processes, Deep Kernels and Deep Ensemble models. We then outline a general framework - Predictive Trajectory Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based planning with information-theoretic bonuses to achieve sample-efficient deep exploration. We empirically demonstrate that PTS-BE substantially outperforms other baselines across a variety of environments characterized by sparse rewards and/or purely exploratory tasks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02639v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02635v1",
    "title": "SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection",
    "authors": [
      "Mao Luo",
      "Zhi Wang",
      "Yiwen Huang",
      "Qingyun Zhang",
      "Zhouxing Su",
      "Zhipeng Lv",
      "Wen Hu",
      "Jianguo Li"
    ],
    "abstract": "Electronic payment platforms are estimated to process billions oftransactions daily, with the cumulative value of these transactionspotentially reaching into the trillions. Even a minor error within thishigh-volume environment could precipitate substantial financiallosses. To mitigate this risk, manually constructed verification rules,developed by domain experts, are typically employed to identifyand scrutinize transactions in production environments. However,due to the absence of a systematic approach to ensure the robust-ness of these verification rules against vulnerabilities, they remainsusceptible to exploitation.To mitigate this risk, manually constructed verification rules, de-veloped by domain experts, are typically employed to identify andscrutinize transactions in production environments. However, dueto the absence of a systematic approach to ensure the robustness ofthese verification rules against vulnerabilities, they remain suscep-tible to exploitation. To ensure data security, database maintainersusually compose complex verification rules to check whether aquery/update request is valid. However, the rules written by ex-perts are usually imperfect, and malicious requests may bypassthese rules. As a result, the demand for identifying the defects ofthe rules systematically emerges.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR",
      "cs.DB"
    ],
    "url": "http://arxiv.org/abs/2507.02635v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02634v1",
    "title": "High-Order Deep Meta-Learning with Category-Theoretic Interpretation",
    "authors": [
      "David H. Mguni"
    ],
    "abstract": "We introduce a new hierarchical deep learning framework for recursive higher-order meta-learning that enables neural networks (NNs) to construct, solve, and generalise across hierarchies of tasks. Central to this approach is a generative mechanism that creates \\emph{virtual tasks} -- synthetic problem instances designed to enable the meta-learner to learn \\emph{soft constraints} and unknown generalisable rules across related tasks. Crucially, this enables the framework to generate its own informative, task-grounded datasets thereby freeing machine learning (ML) training from the limitations of relying entirely on human-generated data. By actively exploring the virtual point landscape and seeking out tasks lower-level learners find difficult, the meta-learner iteratively refines constraint regions. This enhances inductive biases, regularises the adaptation process, and produces novel, unanticipated tasks and constraints required for generalisation. Each meta-level of the hierarchy corresponds to a progressively abstracted generalisation of problems solved at lower levels, enabling a structured and interpretable learning progression. By interpreting meta-learners as category-theoretic \\emph{functors} that generate and condition a hierarchy of subordinate learners, we establish a compositional structure that supports abstraction and knowledge transfer across progressively generalised tasks. The category-theoretic perspective unifies existing meta-learning models and reveals how learning processes can be transformed and compared through functorial relationships, while offering practical design principles for structuring meta-learning. We speculate this architecture may underpin the next generation of NNs capable of autonomously generating novel, instructive tasks and their solutions, thereby advancing ML towards general artificial intelligence.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02634v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02628v1",
    "title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data",
    "authors": [
      "Irena Girshovitz",
      "Atai Ambus",
      "Moni Shahar",
      "Ran Gilad-Bachrach"
    ],
    "abstract": "Background: The use of Electronic Health Records (EHRs) for epidemiological studies and artificial intelligence (AI) training is increasing rapidly. The reliability of the results depends on the accuracy and completeness of EHR data. However, EHR data often contain significant quality issues, including misrepresentations of subpopulations, biases, and systematic errors, as they are primarily collected for clinical and billing purposes. Existing quality assessment methods remain insufficient, lacking systematic procedures to assess data fitness for research.   Methods: We present the Medical Data Pecking approach, which adapts unit testing and coverage concepts from software engineering to identify data quality concerns. We demonstrate our approach using the Medical Data Pecking Tool (MDPT), which consists of two main components: (1) an automated test generator that uses large language models and grounding techniques to create a test suite from data and study descriptions, and (2) a data testing framework that executes these tests, reporting potential errors and coverage.   Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and SyntheticMass, generating 55-73 tests per cohort across four conditions. These tests correctly identified 20-43 non-aligned or non-conforming data issues. We present a detailed analysis of the LLM-generated test suites in terms of reference grounding and value accuracy.   Conclusion: Our approach incorporates external medical knowledge to enable context-sensitive data quality testing as part of the data analysis workflow to improve the validity of its outcomes. Our approach tackles these challenges from a quality assurance perspective, laying the foundation for further development such as additional data modalities and improved grounding methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02628v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02626v1",
    "title": "VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning",
    "authors": [
      "Siran Chen",
      "Boyu Chen",
      "Chenyun Yu",
      "Yuxiao Luo",
      "Ouyang Yi",
      "Lei Cheng",
      "Chengxiang Zhuo",
      "Zang Li",
      "Yali Wang"
    ],
    "abstract": "Owing to powerful natural language processing and generative capabilities, large language model (LLM) agents have emerged as a promising solution for enhancing recommendation systems via user simulation. However, in the realm of video recommendation, existing studies predominantly resort to prompt-based simulation using frozen LLMs and encounter the intricate challenge of multimodal content understanding. This frequently results in suboptimal item modeling and user preference learning, thereby ultimately constraining recommendation performance. To address these challenges, we introduce VRAgent-R1, a novel agent-based paradigm that incorporates human-like intelligence in user simulation. Specifically, VRAgent-R1 comprises two distinct agents: the Item Perception (IP) Agent and the User Simulation (US) Agent, designed for interactive user-item modeling. Firstly, the IP Agent emulates human-like progressive thinking based on MLLMs, effectively capturing hidden recommendation semantics in videos. With a more comprehensive multimodal content understanding provided by the IP Agent, the video recommendation system is equipped to provide higher-quality candidate items. Subsequently, the US Agent refines the recommended video sets based on in-depth chain-of-thought (CoT) reasoning and achieves better alignment with real user preferences through reinforcement learning. Experimental results on a large-scale video recommendation benchmark have demonstrated the effectiveness of our proposed VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\\% improvement in NDCG@10 on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\\% higher accuracy in user decision simulation compared to state-of-the-art baselines.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.MM"
    ],
    "url": "http://arxiv.org/abs/2507.02626v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02625v1",
    "title": "The covariance matrix spectrum of correlated charge insulators reveals hidden connections to Coupled Cluster, Matrix Product, and Rokhsar-Kivelson states",
    "authors": [
      "Izak Snyman",
      "Serge Florens"
    ],
    "abstract": "Charge ordering induced by strong short-range repulsion in itinerant fermion systems typically follows a two-sites alternation pattern. However, the covariance matrix spectrum of the one-dimensional, half-filled, spinless $t$-$V$ model reveals a post-Hartree-Fock picture at strong repulsion, with emergent four-site disruptions of the underlying staggered mean-field state. These disruptions are captured in a thermodynamically extensive manner by a compact four-fermion Coupled Cluster (doubles) state (CCS). Remarkably, all properties of this state may be computed analytically by combinatorial means, and also derived from an exactly solvable correlated hopping Hamiltonian. Furthermore, this Coupled Cluster state can be re-expressed as a low-rank Matrix Product State (MPS) with bond dimension exactly four. In addition, we unveil a hidden connection between this Coupled Cluster ansatz and a Rokhsar-Kivelson state (RKS), which is the ground state of a solvable parent quantum tetramer model. The broad picture that we uncover here thus provides deep connections between several core concepts of correlated fermions and quantum chemistry that have previously enjoyed limited synergy. In contrast to a recent perturbative treatment on top of Hartree-Fock theory, our approach asymptotically captures the correct correlations in the $t$-$V$ model at small $t/V$, and remains a qualitatively accurate approximation even outside the perturbative regime. Our results make the case for further studies of the covariance matrix for correlated electron systems in which ground states have non-trivial unit-cell structure.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.str-el",
      "cond-mat.stat-mech"
    ],
    "url": "http://arxiv.org/abs/2507.02625v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02624v1",
    "title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes",
    "authors": [
      "Antoine Honoré",
      "Borja Rodríguez Gálvez",
      "Yoomi Park",
      "Yitian Zhou",
      "Volker M. Lauschke",
      "Ming Xiao"
    ],
    "abstract": "Variant effect predictors (VEPs) aim to assess the functional impact of protein variants, traditionally relying on multiple sequence alignments (MSAs). This approach assumes that naturally occurring variants are fit, an assumption challenged by pharmacogenomics, where some pharmacogenes experience low evolutionary pressure. Deep mutational scanning (DMS) datasets provide an alternative by offering quantitative fitness scores for variants. In this work, we propose a transformer-based matrix variational auto-encoder (matVAE) with a structured prior and evaluate its performance on 33 DMS datasets corresponding to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence model in zero-shot prediction on DMS datasets, despite using an order of magnitude fewer parameters and requiring less computation at inference time. We also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on DMS data, and find that the latter performs better on supervised prediction tasks. Additionally, incorporating AlphaFold-generated structures into our transformer model further improves performance, achieving results comparable to DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the potential of DMS datasets to replace MSAs without significant loss in predictive performance, motivating further development of DMS datasets and exploration of their relationships to enhance variant effect prediction.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02624v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02619v1",
    "title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation",
    "authors": [
      "Hazal Mogultay Ozcan",
      "Sinan Kalkan",
      "Fatos T. Yarman-Vural"
    ],
    "abstract": "In this paper, we propose a novel model called Learnable VAE (L-VAE), which learns a disentangled representation together with the hyperparameters of the cost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein the hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the limitations of \\b{eta}-VAE by learning the relative weights of the terms in the loss function to control the dynamic trade-off between disentanglement and reconstruction losses. In the proposed model, the weight of the loss terms and the parameters of the model architecture are learned concurrently. An additional regularization term is added to the loss function to prevent bias towards either reconstruction or disentanglement losses. Experimental analyses show that the proposed L-VAE finds an effective balance between reconstruction fidelity and disentangling the latent dimensions. Comparisons of the proposed L-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that L-VAE consistently provides the best or the second best performances measured by a set of disentanglement metrics. Moreover, qualitative experiments on CelebA dataset, confirm the success of the L-VAE model for disentangling the facial attributes.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02619v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02616v1",
    "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making",
    "authors": [
      "Tianqi Shang",
      "Weiqing He",
      "Charles Zheng",
      "Lingyao Li",
      "Li Shen",
      "Bingxin Zhao"
    ],
    "abstract": "The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02616v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02614v1",
    "title": "A Dopamine-Serotonin Theory of Consciousness",
    "authors": [
      "Diogo Sousa"
    ],
    "abstract": "This work presents a comprehensive theory of consciousness grounded in mathematical formalism and supported by clinical data analysis. The framework developed herein demonstrates that consciousness exists as a continuous, non-monotonic function across a high-dimensional neurochemical space, with dopamine serving as the primary intensity regulator and serotonin (5-HT2A) as the complexity modulator. This work offers mechanistic explanations for the full spectrum of conscious states, from deep sleep and psychosis to the ultimate collapse in neural death. The theory explains paradoxical phenomena such as prefrontal cortex hypoactivity during seizures, the evolutionary persistence of psychosis-prone individuals, and why controlled administration of classical 5-HT2A agonists shows a comparatively low incidence of serious medical events (< 0.01 % in modern clinical trials), while dopaminergic excess proves rapidly lethal. The framework is tested using 70,290 sleep nights from 242 Parkinson's disease patients, using disease severity (UPDRS) as a proxy for system integrity and medication (LEDD) as a proxy for dopaminergic input. The analysis reveals a significant LEDD x UPDRS interaction (beta=-1.7, p<.0001), confirming the model's prediction of state-dependent, non-linear dynamics.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "q-bio.NC",
      "q-bio.OT"
    ],
    "url": "http://arxiv.org/abs/2507.02614v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02608v1",
    "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation",
    "authors": [
      "François Rozet",
      "Ruben Ohana",
      "Michael McCabe",
      "Gilles Louppe",
      "François Lanusse",
      "Shirley Ho"
    ],
    "abstract": "The steep computational cost of diffusion models at inference hinders their use as fast physics emulators. In the context of image and video generation, this computational drawback has been addressed by generating in the latent space of an autoencoder instead of the pixel space. In this work, we investigate whether a similar strategy can be effectively applied to the emulation of dynamical systems and at what cost. We find that the accuracy of latent-space emulation is surprisingly robust to a wide range of compression rates (up to 1000x). We also show that diffusion-based emulators are consistently more accurate than non-generative counterparts and compensate for uncertainty in their predictions with greater diversity. Finally, we cover practical design choices, spanning from architectures to optimizers, that we found critical to train latent-space emulators.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "url": "http://arxiv.org/abs/2507.02608v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02607v1",
    "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures",
    "authors": [
      "Frida Sundfeldt",
      "Bianca Widstam",
      "Mahshid Helali Moghadam",
      "Kuo-Yun Liang",
      "Anders Vesterberg"
    ],
    "abstract": "The digital evolution of connected vehicles and the subsequent security risks emphasize the critical need for implementing in-vehicle cyber security measures such as intrusion detection and response systems. The continuous advancement of attack scenarios further highlights the need for adaptive detection mechanisms that can detect evolving, unknown, and complex threats. The effective use of ML-driven techniques can help address this challenge. However, constraints on implementing diverse attack scenarios on test vehicles due to safety, cost, and ethical considerations result in a scarcity of data representing attack scenarios. This limitation necessitates alternative efficient and effective methods for generating high-quality attack-representing data. This paper presents a context-aware attack data generator that generates attack inputs and corresponding in-vehicle network log, i.e., controller area network (CAN) log, representing various types of attack including denial of service (DoS), fuzzy, spoofing, suspension, and replay attacks. It utilizes parameterized attack models augmented with CAN message decoding and attack intensity adjustments to configure the attack scenarios with high similarity to real-world scenarios and promote variability. We evaluate the practicality of the generated attack-representing data within an intrusion detection system (IDS) case study, in which we develop and perform an empirical evaluation of two deep neural network IDS models using the generated data. In addition to the efficiency and scalability of the approach, the performance results of IDS models, high detection and classification capabilities, validate the consistency and effectiveness of the generated data as well. In this experience study, we also elaborate on the aspects influencing the fidelity of the data to real-world scenarios and provide insights into its application.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "url": "http://arxiv.org/abs/2507.02607v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02606v1",
    "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks",
    "authors": [
      "Wei Fan",
      "Kejiang Chen",
      "Chang Liu",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "abstract": "The rapid advancement of speech generation models has heightened privacy and security concerns related to voice cloning (VC). Recent studies have investigated disrupting unauthorized voice cloning by introducing adversarial perturbations. However, determined attackers can mitigate these protective perturbations and successfully execute VC. In this study, we conduct the first systematic evaluation of these protective perturbations against VC under realistic threat models that include perturbation purification. Our findings reveal that while existing purification methods can neutralize a considerable portion of the protective perturbations, they still lead to distortions in the feature space of VC models, which degrades the performance of VC. From this perspective, we propose a novel two-stage purification method: (1) Purify the perturbed speech; (2) Refine it using phoneme guidance to align it with the clean speech distribution. Experimental results demonstrate that our method outperforms state-of-the-art purification methods in disrupting VC defenses. Our study reveals the limitations of adversarial perturbation-based VC defenses and underscores the urgent need for more robust solutions to mitigate the security and privacy risks posed by VC. The code and audio samples are available at https://de-antifake.github.io.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "url": "http://arxiv.org/abs/2507.02606v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02599v1",
    "title": "Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data",
    "authors": [
      "Sertac Kilickaya",
      "Levent Eren"
    ],
    "abstract": "Purpose: The primary aim of this study is to enhance fault diagnosis in induction machines by leveraging the Pad\\'e Approximant Neuron (PAON) model. While accelerometers and microphones are standard in motor condition monitoring, deep learning models with nonlinear neuron architectures offer promising improvements in diagnostic performance. This research addresses the question: Can Pad\\'e Approximant Neural Networks (Pad\\'eNets) outperform conventional Convolutional Neural Networks (CNNs) and Self-Organized Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical faults using vibration and acoustic data?   Methods: We evaluate and compare the diagnostic capabilities of three deep learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\\'eNets. These models are tested on the University of Ottawa's publicly available constant-speed induction motor datasets, which include both vibration and acoustic sensor data. The Pad\\'eNet model is designed to introduce enhanced nonlinearity and is compatible with unbounded activation functions such as Leaky ReLU.   Results and Conclusion: Pad\\'eNets consistently outperformed the baseline models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33% for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced nonlinearity of Pad\\'eNets, together with their compatibility with unbounded activation functions, significantly improves fault diagnosis performance in induction motor condition monitoring.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.SD",
      "cs.SY",
      "eess.SY"
    ],
    "url": "http://arxiv.org/abs/2507.02599v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02598v1",
    "title": "AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models",
    "authors": [
      "Chenhao Xue",
      "Kezhi Li",
      "Jiaxing Zhang",
      "Yi Ren",
      "Zhengyuan Shi",
      "Chen Zhang",
      "Yibo Lin",
      "Lining Zhang",
      "Qiang Xu",
      "Guangyu Sun"
    ],
    "abstract": "Arithmetic circuits, such as adders and multipliers, are fundamental components of digital systems, directly impacting the performance, power efficiency, and area footprint. However, optimizing these circuits remains challenging due to the vast design space and complex physical constraints. While recent deep learning-based approaches have shown promise, they struggle to consistently explore high-potential design variants, limiting their optimization efficiency. To address this challenge, we propose AC-Refiner, a novel arithmetic circuit optimization framework leveraging conditional diffusion models. Our key insight is to reframe arithmetic circuit synthesis as a conditional image generation task. By carefully conditioning the denoising diffusion process on target quality-of-results (QoRs), AC-Refiner consistently produces high-quality circuit designs. Furthermore, the explored designs are used to fine-tune the diffusion model, which focuses the exploration near the Pareto frontier. Experimental results demonstrate that AC-Refiner generates designs with superior Pareto optimality, outperforming state-of-the-art baselines. The performance gain is further validated by integrating AC-Refiner into practical applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AR",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02598v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02593v1",
    "title": "Revisiting Active Learning under (Human) Label Variation",
    "authors": [
      "Cornelia Gruber",
      "Helen Alber",
      "Bernd Bischl",
      "Göran Kauermann",
      "Barbara Plank",
      "Matthias Aßenmacher"
    ],
    "abstract": "Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ],
    "url": "http://arxiv.org/abs/2507.02593v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02585v1",
    "title": "Scalable Interconnect Learning in Boolean Networks",
    "authors": [
      "Fabian Kresse",
      "Emily Yu",
      "Christoph H. Lampert"
    ],
    "abstract": "Learned Differentiable Boolean Logic Networks (DBNs) already deliver efficient inference on resource-constrained hardware. We extend them with a trainable, differentiable interconnect whose parameter count remains constant as input width grows, allowing DBNs to scale to far wider layers than earlier learnable-interconnect designs while preserving their advantageous accuracy. To further reduce model size, we propose two complementary pruning stages: an SAT-based logic equivalence pass that removes redundant gates without affecting performance, and a similarity-based, data-driven pass that outperforms a magnitude-style greedy baseline and offers a superior compression-accuracy trade-off.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.LO"
    ],
    "url": "http://arxiv.org/abs/2507.02585v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02581v1",
    "title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning",
    "authors": [
      "Tan Pan",
      "Zhaorui Tan",
      "Kaiyu Guo",
      "Dongli Xu",
      "Weidi Xu",
      "Chen Jiang",
      "Xin Guo",
      "Yuan Qi",
      "Yuan Cheng"
    ],
    "abstract": "3D medical image self-supervised learning (mSSL) holds great promise for medical analysis. Effectively supporting broader applications requires considering anatomical structure variations in location, scale, and morphology, which are crucial for capturing meaningful distinctions. However, previous mSSL methods partition images with fixed-size patches, often ignoring the structure variations. In this work, we introduce a novel perspective on 3D medical images with the goal of learning structure-aware representations. We assume that patches within the same structure share the same semantics (semantic consistency) while those from different structures exhibit distinct semantics (semantic discrepancy). Based on this assumption, we propose an mSSL framework named $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency in two steps. First, $S^2DC$ enforces distinct representations for different patches to increase semantic discrepancy by leveraging an optimal transport strategy. Second, $S^2DC$ advances semantic consistency at the structural level based on neighborhood similarity distribution. By bridging patch-level and structure-level representations, $S^2DC$ achieves structure-aware representations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3 modalities, our proposed method consistently outperforms the state-of-the-art methods in mSSL.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02581v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02576v1",
    "title": "Parametric shape models for vessels learned from segmentations via differentiable voxelization",
    "authors": [
      "Alina F. Dima",
      "Suprosanna Shit",
      "Huaqi Qiu",
      "Robbie Holland",
      "Tamara T. Mueller",
      "Fabio Antonio Musio",
      "Kaiyuan Yang",
      "Bjoern Menze",
      "Rickmer Braren",
      "Marcus Makowski",
      "Daniel Rueckert"
    ],
    "abstract": "Vessels are complex structures in the body that have been studied extensively in multiple representations. While voxelization is the most common of them, meshes and parametric models are critical in various applications due to their desirable properties. However, these representations are typically extracted through segmentations and used disjointly from each other. We propose a framework that joins the three representations under differentiable transformations. By leveraging differentiable voxelization, we automatically extract a parametric shape model of the vessels through shape-to-segmentation fitting, where we learn shape parameters from segmentations without the explicit need for ground-truth shape parameters. The vessel is parametrized as centerlines and radii using cubic B-splines, ensuring smoothness and continuity by construction. Meshes are differentiably extracted from the learned shape parameters, resulting in high-fidelity meshes that can be manipulated post-fit. Our method can accurately capture the geometry of complex vessels, as demonstrated by the volumetric fits in experiments on aortas, aneurysms, and brain vessels.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02576v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02574v1",
    "title": "Learning and Testing Inverse Statistical Problems For Interacting Systems Undergoing Phase Transition",
    "authors": [
      "Stefano Bae",
      "Dario Bocchi",
      "Luca Maria Del Bono",
      "Luca Leuzzi"
    ],
    "abstract": "Inverse problems arise in situations where data is available, but the underlying model is not. It can therefore be necessary to infer the parameters of the latter starting from the former. Statistical mechanics offers a toolbox of techniques to address this challenge. In this work, we illustrate three of the main methods: the Maximum Likelihood, Maximum Pseudo-Likelihood, and Mean-Field approaches. We begin with a thorough theoretical introduction to these methods, followed by their application to inference in several well-known statistical physics systems undergoing phase transitions. Namely, we consider the ordered and disordered Ising models, the vector Potts model, and the Blume-Capel model on both regular lattices and random graphs. This discussion is accompanied by a GitHub repository that allows users to both reproduce the results and experiment with new systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.stat-mech",
      "cond-mat.dis-nn",
      "physics.data-an"
    ],
    "url": "http://arxiv.org/abs/2507.02574v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02565v1",
    "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning",
    "authors": [
      "Buzhen Huang",
      "Chen Li",
      "Chongyang Xu",
      "Dongyue Lu",
      "Jinnan Chen",
      "Yangang Wang",
      "Gim Hee Lee"
    ],
    "abstract": "Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data are available at https://www.buzhenhuang.com/works/CloseApp.html.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02565v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02563v1",
    "title": "Dipolar dark matter theory based on a non-Abelian Yang-Mills field",
    "authors": [
      "Luc Blanchet",
      "Emeric Seraille"
    ],
    "abstract": "Most theories that attempt to reproduce the Modified Newtonian Dynamics (MOND) phenomenology for dark matter at galactic scales rely on ad hoc free functions, preventing them from being regarded as fundamental. In this work, we present a new theory that reproduces MOND, built on a supposed to be fundamental Yang-Mills gauge field based on SU(2), with a gravitational coupling constant, and emerging in a low-acceleration regime, below the MOND acceleration scale. The gauge field plays the role of the internal force in the dipolar dark matter (DDM) model. We discuss how certain solutions of this theory recover the deep MOND regime without introducing arbitrary functions in the action. Within this framework, the MOND phenomenology appears to be due to the existence of a new sector of particle physics.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "gr-qc",
      "astro-ph.CO",
      "hep-th"
    ],
    "url": "http://arxiv.org/abs/2507.02563v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02562v1",
    "title": "Multi-Utterance Speech Separation and Association Trained on Short Segments",
    "authors": [
      "Yuzhu Wang",
      "Archontis Politis",
      "Konstantinos Drossos",
      "Tuomas Virtanen"
    ],
    "abstract": "Current deep neural network (DNN) based speech separation faces a fundamental challenge -- while the models need to be trained on short segments due to computational constraints, real-world applications typically require processing significantly longer recordings with multiple utterances per speaker than seen during training. In this paper, we investigate how existing approaches perform in this challenging scenario and propose a frequency-temporal recurrent neural network (FTRNN) that effectively bridges this gap. Our FTRNN employs a full-band module to model frequency dependencies within each time frame and a sub-band module that models temporal patterns in each frequency band. Despite being trained on short fixed-length segments of 10 s, our model demonstrates robust separation when processing signals significantly longer than training segments (21-121 s) and preserves speaker association across utterance gaps exceeding those seen during training. Unlike the conventional segment-separation-stitch paradigm, our lightweight approach (0.9 M parameters) performs inference on long audio without segmentation, eliminating segment boundary distortions while simplifying deployment. Experimental results demonstrate the generalization ability of FTRNN for multi-utterance speech separation and speaker association.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.AS",
      "cs.SD"
    ],
    "url": "http://arxiv.org/abs/2507.02562v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02559v1",
    "title": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability",
    "authors": [
      "Luca Baroni",
      "Galvin Khara",
      "Joachim Schaeffer",
      "Marat Subkhankulov",
      "Stefan Heimersheim"
    ],
    "abstract": "Layer-wise normalization (LN) is an essential component of virtually all transformer-based large language models. While its effects on training stability are well documented, its role at inference time is poorly understood. Additionally, LN layers hinder mechanistic interpretability by introducing additional nonlinearities and increasing the interconnectedness of individual model components. Here, we show that all LN layers can be removed from every GPT-2 model with only a small increase in validation loss (e.g. +0.03 cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in language modeling. We find that the amount of fine-tuning data needed for LN removal grows sublinearly with model parameters, suggesting scaling to larger models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face. Furthermore, we test interpretability techniques on LN-free models. Direct logit attribution now gives the exact direct effect of individual components, while the accuracy of attribution patching does not significantly improve. We also confirm that GPT-2's \"confidence neurons\" are inactive in the LN-free models. Our work clarifies the role of LN layers in language modeling, showing that GPT-2-class models can function without LN layers. We hope that our LN-free analogs of the GPT-2 family of models will enable more precise interpretability research and improve our understanding of language models.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02559v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02554v1",
    "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
    "authors": [
      "Edan Toledo",
      "Karen Hambardzumyan",
      "Martin Josifoski",
      "Rishi Hazra",
      "Nicolas Baldwin",
      "Alexis Audran-Reiss",
      "Michael Kuchnik",
      "Despoina Magka",
      "Minqi Jiang",
      "Alisia Maria Lupidi",
      "Andrei Lupu",
      "Roberta Raileanu",
      "Kelvin Niu",
      "Tatiana Shavrina",
      "Jean-Christophe Gagnon-Audet",
      "Michael Shvartsman",
      "Shagun Sodhani",
      "Alexander H. Miller",
      "Abhishek Charnalia",
      "Derek Dunfield",
      "Carole-Jean Wu",
      "Pontus Stenetorp",
      "Nicola Cancedda",
      "Jakob Nicolaus Foerster",
      "Yoram Bachrach"
    ],
    "abstract": "AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02554v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02550v1",
    "title": "Position: A Theory of Deep Learning Must Include Compositional Sparsity",
    "authors": [
      "David A. Danhofer",
      "Davide D'Ascenzo",
      "Rafael Dubach",
      "Tomaso Poggio"
    ],
    "abstract": "Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable success in a wide variety of domains too high-dimensional for classical shallow networks subject to the curse of dimensionality. However, open questions about fundamental principles, that govern the learning dynamics of DNNs, remain. In this position paper we argue that it is the ability of DNNs to exploit the compositionally sparse structure of the target function driving their success. As such, DNNs can leverage the property that most practically relevant functions can be composed from a small set of constituent functions, each of which relies only on a low-dimensional subset of all inputs. We show that this property is shared by all efficiently Turing-computable functions and is therefore highly likely present in all current learning problems. While some promising theoretical insights on questions concerned with approximation and generalization exist in the setting of compositionally sparse functions, several important questions on the learnability and optimization of DNNs remain. Completing the picture of the role of compositional sparsity in deep learning is essential to a comprehensive theory of artificial, and even general, intelligence.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02550v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02537v1",
    "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue",
    "authors": [
      "Paulo Ricardo Knob",
      "Leonardo Scholler",
      "Juliano Rigatti",
      "Soraia Raupp Musse"
    ],
    "abstract": "Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02537v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02529v1",
    "title": "RetrySQL: text-to-SQL training with retry data for self-correcting query generation",
    "authors": [
      "Alicja Rączkowska",
      "Riccardo Belluzzo",
      "Piotr Zieliński",
      "Joanna Baran",
      "Paweł Olszewski"
    ],
    "abstract": "The text-to-SQL task is an active challenge in Natural Language Processing. Many existing solutions focus on using black-box language models extended with specialized components within customized end-to-end text-to-SQL pipelines. While these solutions use both closed-source proprietary language models and coding-oriented open-source models, there is a lack of research regarding SQL-specific generative models. At the same time, recent advancements in self-correcting generation strategies show promise for improving the capabilities of existing architectures. The application of these concepts to the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL, a new approach to training text-to-SQL generation models. We prepare reasoning steps for reference SQL queries and then corrupt them to create retry data that contains both incorrect and corrected steps, divided with a special token. We continuously pre-train an open-source coding model with this data and demonstrate that retry steps yield an improvement of up to 4 percentage points in both overall and challenging execution accuracy metrics, compared to pre-training without retry data. Additionally, we confirm that supervised fine-tuning with LoRA is ineffective for learning from retry data and that full-parameter pre-training is a necessary requirement for that task. We showcase that the self-correcting behavior is learned by the model and the increase in downstream accuracy metrics is a result of this additional skill. Finally, we incorporate RetrySQL-trained models into the full text-to-SQL pipeline and showcase that they are competitive in terms of execution accuracy with proprietary models that contain orders of magnitude more parameters. RetrySQL demonstrates that self-correction can be learned in the text-to-SQL task and provides a novel way of improving generation accuracy for SQL-oriented language models.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02529v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02527v1",
    "title": "A Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE). XIX. The discovery of a spectacular 230 kpc Halpha tail following NGC 4569 in the Virgo cluster",
    "authors": [
      "M. Sun",
      "H. Le",
      "B. Epinat",
      "A. Boselli",
      "R. Luo",
      "K. Hosogi",
      "N. Pichette",
      "W. Forman",
      "C. Sarazin",
      "M. Fossati",
      "H. Chen",
      "E. Sarpa",
      "J. Braine",
      "J. C. Cuillandre",
      "S. Gwyn",
      "G. Hensler",
      "S. Martocchia",
      "B. Vollmer"
    ],
    "abstract": "Context. Galaxies fly inside galaxy clusters and ram pressure by the ICM can remove a large amount of the ISM from the galaxy, and deposit the gas in the ICM. The ISM decoupled from the host galaxy leaves a long trail following the moving galaxy. Such long trails track the galaxy motion and can be detected with sensitive data in Halpha.   Aims. We study the Halpha tail trailing NGC 4569 in the Virgo cluster.   Methods. The initial discovery was made with the deep Halpha imaging data with CFHT, from the VESTIGE project. The follow-up spectroscopic observations were made with APO/DIS, MMT/Binospec and CFHT/SITELLE.   Results. Besides the known 80 kpc Halpha tail downstream of NGC 4569, the deep Halpha imaging data allow the Halpha tail detected to at least 230 kpc from the galaxy. More importantly, the Halpha clumps implied from the imaging data are confirmed with the spectroscopic data. The Halpha clumps show a smooth radial velocity gradient across about 1300 km/s, eventually reaching the velocity of the cluster. We build a simple model to explain the deceleration of stripped clumps and constrain the age to about 0.9 Gyr.   Conclusions. This discovery, for the first time, demonstrates the full deceleration process of the stripped ISM. This discovery also showcases the potential with wide-field Halpha survey on galaxy clusters to discover intracluster optical emission-line clouds originated from cluster galaxies. These clouds provide kinematic tracers to the infall history of cluster galaxies and the turbulence in the ICM. They are also excellent multi-phase objects to study the relevant important physical processes.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.GA"
    ],
    "url": "http://arxiv.org/abs/2507.02527v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02524v1",
    "title": "Time Resolution Independent Operator Learning",
    "authors": [
      "Diab W. Abueidda",
      "Mbebo Nonna",
      "Panos Pantidis",
      "Mostafa E. Mobasher"
    ],
    "abstract": "Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data remains a challenging task. Recurrent DeepONet extensions inherit the discrete-time limitations of sequence-to-sequence (seq2seq) RNN architectures, while neural-ODE surrogates cannot incorporate new inputs after initialization. We introduce NCDE-DeepONet, a continuous-time operator network that embeds a Neural Controlled Differential Equation (NCDE) in the branch and augments the trunk with explicit space-time coordinates. The NCDE encodes an entire load history as the solution of a controlled ODE driven by a spline-interpolated input path, making the representation input-resolution-independent: it encodes different input signal discretizations of the observed samples. The trunk then probes this latent path at arbitrary spatial locations and times, rendering the overall map output-resolution independent: predictions can be queried on meshes and time steps unseen during training without retraining or interpolation. Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems confirm the robustness and accuracy of the framework, achieving almost instant solution prediction. These findings suggest that controlled dynamics provide a principled and efficient foundation for high-fidelity operator learning in transient mechanics.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CE",
      "cs.NA",
      "math.NA"
    ],
    "url": "http://arxiv.org/abs/2507.02524v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02521v1",
    "title": "Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings",
    "authors": [
      "Ayodeji O. Abioye",
      "Jayati Deshmukh",
      "Athina Georgara",
      "Dominic Price",
      "Tuyen Nguyen",
      "Aleksandra Landowska",
      "Amel Bennaceur",
      "Joel E. Fischer",
      "Sarvapali D. Ramchurn"
    ],
    "abstract": "This research investigates strategies for multi-robot coordination in multi-human environments. It proposes a multi-objective learning-based coordination approach to addressing the problem of path planning, navigation, task scheduling, task allocation, and human-robot interaction in multi-human multi-robot (MHMR) settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO"
    ],
    "url": "http://arxiv.org/abs/2507.02521v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02520v1",
    "title": "Exploring black hole shadows in axisymmetric spacetimes with coordinate-independent methods and neural networks",
    "authors": [
      "Temurbek Mirzaev",
      "Bobomurat Ahmedov",
      "Cosimo Bambi"
    ],
    "abstract": "The study of black hole shadows provides a powerful tool for testing the predictions of general relativity and exploring deviations from the standard Kerr metric in the strong gravitational field regime. Here, we investigate the shadow properties of axisymmetric gravitational compact objects using a coordinate-independent formalism. We analyze black hole shadows in various spacetime geometries, including the Kerr, Taub-NUT, $\\gamma$, and Kaluza-Klein metrics, to identify distinctive features that can be used to constrain black hole parameters. To achieve a more robust characterization, we employ both Legendre and Fourier expansions, demonstrating that the Fourier approach may offer better coordinate independence and facilitate cross-model comparisons. Finally, we develop a machine learning framework based on neural networks trained on synthetic shadow data, enabling precise parameter estimation from observational results. Using data from observational astronomical facilities such as the Event Horizon Telescope (EHT), Keck, and the Very Large Telescope Interferometer (VLTI), we provide constraints on black hole parameters derived from shadow observations. Our findings highlight the potential of coordinate-independent techniques and machine learning for advancing black hole astrophysics and testing fundamental physics beyond general relativity.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "gr-qc"
    ],
    "url": "http://arxiv.org/abs/2507.02520v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02519v1",
    "title": "IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning",
    "authors": [
      "Abiam Remache González",
      "Meriem Chagour",
      "Timon Bijan Rüth",
      "Raúl Trapiella Cañedo",
      "Marina Martínez Soler",
      "Álvaro Lorenzo Felipe",
      "Hyun-Suk Shin",
      "María-Jesús Zamorano Serrano",
      "Ricardo Torres",
      "Juan-Antonio Castillo Parra",
      "Eduardo Reyes Abad",
      "Miguel-Ángel Ferrer Ballester",
      "Juan-Manuel Afonso López",
      "Francisco-Mario Hernández Tejera",
      "Adrian Penate-Sanchez"
    ],
    "abstract": "This paper introduces IMASHRIMP, an adapted system for the automated morphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing genetic selection tasks in aquaculture. Existing deep learning and computer vision techniques were modified to address the specific challenges of shrimp morphology analysis from RGBD images. IMASHRIMP incorporates two discrimination modules, based on a modified ResNet-50 architecture, to classify images by the point of view and determine rostrum integrity. It is proposed a \"two-factor authentication (human and IA)\" system, it reduces human error in view classification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%. Additionally, a pose estimation module was adapted from VitPose to predict 23 key points on the shrimp's skeleton, with separate networks for lateral and dorsal views. A morphological regression module, using a Support Vector Machine (SVM) model, was integrated to convert pixel measurements to centimeter units. Experimental results show that the system effectively reduces human error, achieving a mean average precision (mAP) of 97.94% for pose estimation and a pixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP demonstrates the potential to automate and accelerate shrimp morphological analysis, enhancing the efficiency of genetic selection and contributing to more sustainable aquaculture practices.The code are available at https://github.com/AbiamRemacheGonzalez/ImaShrimp-public",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "I.2.10; I.4.8"
    ],
    "url": "http://arxiv.org/abs/2507.02519v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02517v1",
    "title": "Detecting Multiple Diseases in Multiple Crops Using Deep Learning",
    "authors": [
      "Vivek Yadav",
      "Anugrah Jain"
    ],
    "abstract": "India, as a predominantly agrarian economy, faces significant challenges in agriculture, including substantial crop losses caused by diseases, pests, and environmental stress. Early detection and accurate identification of diseases across different crops are critical for improving yield and ensuring food security. This paper proposes a deep learning based solution for detecting multiple diseases in multiple crops, aimed to cover India's diverse agricultural landscape. We first create a unified dataset encompassing images of 17 different crops and 34 different diseases from various available repositories. Proposed deep learning model is trained on this dataset and outperforms the state-of-the-art in terms of accuracy and the number of crops, diseases covered. We achieve a significant detection accuracy, i.e., 99 percent for our unified dataset which is 7 percent more when compared to state-of-the-art handling 14 crops and 26 different diseases only. By improving the number of crops and types of diseases that can be detected, proposed solution aims to provide a better product for Indian farmers.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "url": "http://arxiv.org/abs/2507.02517v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02510v1",
    "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification",
    "authors": [
      "Ahmed G. Habashi",
      "Ahmed M. Azab",
      "Seif Eldawlatly",
      "Gamal M. Aly"
    ],
    "abstract": "Cross-subject motor imagery (CS-MI) classification in brain-computer interfaces (BCIs) is a challenging task due to the significant variability in Electroencephalography (EEG) patterns across different individuals. This variability often results in lower classification accuracy compared to subject-specific models, presenting a major barrier to developing calibration-free BCIs suitable for real-world applications. In this paper, we introduce a novel approach that significantly enhances cross-subject MI classification performance through optimized preprocessing and deep learning techniques. Our approach involves direct classification of Short-Time Fourier Transform (STFT)-transformed EEG data, optimized STFT parameters, and a balanced batching strategy during training of a Convolutional Neural Network (CNN). This approach is uniquely validated across four different datasets, including three widely-used benchmark datasets leading to substantial improvements in cross-subject classification, achieving 67.60% on the BCI Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we systematically investigate the classification performance using MI windows ranging from the full 4-second window to 1-second windows. These results establish a new benchmark for generalizable, calibration-free MI classification in addition to contributing a robust open-access dataset to advance research in this domain.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.HC",
      "cs.NE"
    ],
    "url": "http://arxiv.org/abs/2507.02510v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02506v1",
    "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders",
    "authors": [
      "Sneha Deshmukh",
      "Prathmesh Kamble"
    ],
    "abstract": "Legal NLP remains underdeveloped in regions like India due to the scarcity of structured datasets. We introduce IndianBailJudgments-1200, a new benchmark dataset comprising 1200 Indian court judgments on bail decisions, annotated across 20+ attributes including bail outcome, IPC sections, crime type, and legal reasoning. Annotations were generated using a prompt-engineered GPT-4o pipeline and verified for consistency. This resource supports a wide range of legal NLP tasks such as outcome prediction, summarization, and fairness analysis, and is the first publicly available dataset focused specifically on Indian bail jurisprudence.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "91B14, 68T50",
      "I.2.7; K.4.1; K.5.2"
    ],
    "url": "http://arxiv.org/abs/2507.02506v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02660v1",
    "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification",
    "authors": [
      "Deepak Narayan Gadde",
      "Keerthan Kopparam Radhakrishna",
      "Vaisakh Naduvodi Viswambharan",
      "Aman Kumar",
      "Djones Lettnin",
      "Wolfgang Kunz",
      "Sebastian Simon"
    ],
    "abstract": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.AR"
    ],
    "url": "http://arxiv.org/abs/2507.02660v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02618v1",
    "title": "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory",
    "authors": [
      "Kenneth Payne",
      "Baptiste Alloui-Cros"
    ],
    "abstract": "Are Large Language Models (LLMs) a new form of strategic intelligence, able to reason about goals in competitive settings? We present compelling supporting evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for studying decision-making. We conduct the first ever series of evolutionary IPD tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger) against agents from the leading frontier AI companies OpenAI, Google, and Anthropic. By varying the termination probability in each tournament (the \"shadow of the future\"), we introduce complexity and chance, confounding memorisation.   Our results show that LLMs are highly competitive, consistently surviving and sometimes even proliferating in these complex ecosystems. Furthermore, they exhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini models proved strategically ruthless, exploiting cooperative opponents and retaliating against defectors, while OpenAI's models remained highly cooperative, a trait that proved catastrophic in hostile environments. Anthropic's Claude emerged as the most forgiving reciprocator, showing remarkable willingness to restore cooperation even after being exploited or successfully defecting. Analysis of nearly 32,000 prose rationales provided by the models reveals that they actively reason about both the time horizon and their opponent's likely strategy, and we demonstrate that this reasoning is instrumental to their decisions. This work connects classic game theory with machine psychology, offering a rich and granular view of algorithmic decision-making under uncertainty.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "url": "http://arxiv.org/abs/2507.02618v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02578v1",
    "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems",
    "authors": [
      "Zoe Pfister"
    ],
    "abstract": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both physical and computational capabilities, which can adjust in response to changing parameters. Furthermore, they increasingly incorporate human-machine collaboration, allowing them to benefit from the individual strengths of humans and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm of human-machine collaboration, envisioning seamless teamwork between humans and machines. However, achieving effective and seamless HMT in adaptive CPS is challenging. While adaptive CPS already benefit from feedback loops such as MAPE-K, there is still a gap in integrating humans into these feedback loops due to different operational cadences of humans and machines. Further, HMT requires constant monitoring of human operators, collecting potentially sensitive information about their actions and behavior. Respecting the privacy and human values of the actors of the CPS is crucial for the success of human-machine teams. This research addresses these challenges by: (1) developing novel methods and processes for integrating HMT into adaptive CPS, focusing on human-machine interaction principles and their incorporation into adaptive feedback loops found in CPS, and (2) creating frameworks for integrating, verifying, and validating ethics and human values throughout the system lifecycle, starting from requirements engineering.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE",
      "cs.HC",
      "D.2.1"
    ],
    "url": "http://arxiv.org/abs/2507.02578v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02503v1",
    "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
    "authors": [
      "Chenxu Wang",
      "Yilin Lyu",
      "Zicheng Sun",
      "Liping Jing"
    ],
    "abstract": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02503v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02496v1",
    "title": "Online Conformal Prediction with Efficiency Guarantees",
    "authors": [
      "Vaidehi Srinivas"
    ],
    "abstract": "We study the problem of conformal prediction in a novel online framework that directly optimizes efficiency. In our problem, we are given a target miscoverage rate $\\alpha > 0$, and a time horizon $T$. On each day $t \\le T$ an algorithm must output an interval $I_t \\subseteq [0, 1]$, then a point $y_t \\in [0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is, $y_t \\in I_t$ on (close to) a $(1 - \\alpha)$-fraction of days, while maintaining efficiency, that is, minimizing the average volume (length) of the intervals played. This problem is an online analogue to the problem of constructing efficient confidence intervals.   We study this problem over arbitrary and exchangeable (random order) input sequences. For exchangeable sequences, we show that it is possible to construct intervals that achieve coverage $(1 - \\alpha) - o(1)$, while having length upper bounded by the best fixed interval that achieves coverage in hindsight. For arbitrary sequences however, we show that any algorithm that achieves a $\\mu$-approximation in average length compared to the best fixed interval achieving coverage in hindsight, must make a multiplicative factor more mistakes than $\\alpha T$, where the multiplicative factor depends on $\\mu$ and the aspect ratio of the problem. Our main algorithmic result is a matching algorithm that can recover all Pareto-optimal settings of $\\mu$ and number of mistakes. Furthermore, our algorithm is deterministic and therefore robust to an adaptive adversary.   This gap between the exchangeable and arbitrary settings is in contrast to the classical online learning problem. In fact, we show that no single algorithm can simultaneously be Pareto-optimal for arbitrary sequences and optimal for exchangeable sequences. On the algorithmic side, we give an algorithm that achieves the near-optimal tradeoff between the two cases.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.DS",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "url": "http://arxiv.org/abs/2507.02496v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02494v1",
    "title": "MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations",
    "authors": [
      "Hyunsoo Son",
      "Jeonghyun Noh",
      "Suemin Jeon",
      "Chaoli Wang",
      "Won-Ki Jeong"
    ],
    "abstract": "Implicit Neural Representations (INRs) are widely used to encode data as continuous functions, enabling the visualization of large-scale multivariate scientific simulation data with reduced memory usage. However, existing INR-based methods face three main limitations: (1) inflexible representation of complex structures, (2) primarily focusing on single-variable data, and (3) dependence on structured grids. Thus, their performance degrades when applied to complex real-world datasets. To address these limitations, we propose a novel neural network-based framework, MC-INR, which handles multivariate data on unstructured grids. It combines meta-learning and clustering to enable flexible encoding of complex structures. To further improve performance, we introduce a residual-based dynamic re-clustering mechanism that adaptively partitions clusters based on local error. We also propose a branched layer to leverage multivariate data through independent branches simultaneously. Experimental results demonstrate that MC-INR outperforms existing methods on scientific data encoding tasks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02494v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02493v1",
    "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy",
    "authors": [
      "Luca Parolari",
      "Andrea Cherubini",
      "Lamberto Ballan",
      "Carlo Biffi"
    ],
    "abstract": "Automated polyp counting in colonoscopy is a crucial step toward automated procedure reporting and quality control, aiming to enhance the cost-effectiveness of colonoscopy screening. Counting polyps in a procedure involves detecting and tracking polyps, and then clustering tracklets that belong to the same polyp entity. Existing methods for polyp counting rely on self-supervised learning and primarily leverage visual appearance, neglecting temporal relationships in both tracklet feature learning and clustering stages. In this work, we introduce a paradigm shift by proposing a supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intra-polyp variability while preserving inter-polyp discriminability, leading to more robust clustering. Additionally, we improve tracklet clustering by integrating a temporal adjacency constraint, reducing false positive re-associations between visually similar but temporally distant tracklets. We train and validate our method on publicly available datasets and evaluate its performance with a leave-one-out cross-validation strategy. Results demonstrate a 2.2x reduction in fragmentation rate compared to prior approaches. Our results highlight the importance of temporal awareness in polyp counting, establishing a new state-of-the-art. Code is available at https://github.com/lparolari/temporally-aware-polyp-counting.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02493v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02480v1",
    "title": "Intrinsic Dimensionality of Molecular Properties",
    "authors": [
      "Ali Banjafar",
      "Guido Falk von Rudorff"
    ],
    "abstract": "Chemical space which encompasses all stable compounds is unfathomably large and its dimension scales linearly with the number of atoms considered. The success of machine learning methods suggests that many physical quantities exhibit substantial redundancy in that space, lowering their effective dimensionality. A low dimensionality is favorable for machine learning applications, as it reduces the required number of data points. It is unknown however, how far the dimensionality of physical properties can be reduced, how this depends on the exact physical property considered, and how accepting a model error can help further reducing the dimensionality. We show that accepting a modest, nearly negligible error leads to a drastic reduction in independent degrees of freedom. This applies to several properties such as the total energy and frontier orbital energies for a wide range of neutral molecules with up to 20 atoms. We provide a method to quantify an upper bound for the intrinsic dimensionality given a desired accuracy threshold by inclusion of all continuous variables in the molecular Hamiltonian including the nuclear charges. We find the intrinsic dimensionality to be remarkably stable across molecules, i.e. it is a property of the underlying physical quantity and the number of atoms rather than a property of an individual molecular configuration and therefore highly transferable between molecules. The results suggest that the feature space of state-of-the-art molecular representations can be compressed further, leaving room for more data efficient and transferable models.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.chem-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02480v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02479v1",
    "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios",
    "authors": [
      "Teng Fu",
      "Yuwen Chen",
      "Zhuofan Chen",
      "Mengyang Zhao",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "abstract": "Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02479v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02580v1",
    "title": "On the analysis of sequential designs without a specified number of observations",
    "authors": [
      "Anna Klimova",
      "Tamás Rudas"
    ],
    "abstract": "The paper focuses on sequential experiments for categorical responses in which whether or not a further observation is made depends on the outcome of a previous experiment. Examples include subsequent medical interventions being performed or not depending on the result of a previous intervention, data about offsprings, life tables, and repeated educational retraininig until a certain proficiency level is achieved. Such experiments do not lead to data with a full Cartesian product structure and, despite a prespecified initial sample size, the total number of observations, or interventions, made cannot be determined in advance. The paper investigates the distributional assumptions behind such data and describes a parameterization of the distribution that arises and the respective model class to analyze it. Both the data structure resulting from such an experiment and the model class are special examples of staged trees in algebraic statistics. The properties of the resulting parameter estimates and test statistics are obtained and illustrated using hypothetical and real data.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.ME"
    ],
    "url": "http://arxiv.org/abs/2507.02580v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02488v1",
    "title": "MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention",
    "authors": [
      "Zunhui Xia",
      "Hongxing Li",
      "Libin Lan"
    ],
    "abstract": "Medical image recognition serves as a key way to aid in clinical diagnosis, enabling more accurate and timely identification of diseases and abnormalities. Vision transformer-based approaches have proven effective in handling various medical recognition tasks. However, these methods encounter two primary challenges. First, they are often task-specific and architecture-tailored, limiting their general applicability. Second, they usually either adopt full attention to model long-range dependencies, resulting in high computational costs, or rely on handcrafted sparse attention, potentially leading to suboptimal performance. To tackle these issues, we present MedFormer, an efficient medical vision transformer with two key ideas. First, it employs a pyramid scaling structure as a versatile backbone for various medical image recognition tasks, including image classification and dense prediction tasks such as semantic segmentation and lesion detection. This structure facilitates hierarchical feature representation while reducing the computation load of feature maps, highly beneficial for boosting performance. Second, it introduces a novel Dual Sparse Selection Attention (DSSA) with content awareness to improve computational efficiency and robustness against noise while maintaining high performance. As the core building technique of MedFormer, DSSA is explicitly designed to attend to the most relevant content. In addition, a detailed theoretical analysis has been conducted, demonstrating that MedFormer has superior generality and efficiency in comparison to existing medical vision transformers. Extensive experiments on a variety of imaging modality datasets consistently show that MedFormer is highly effective in enhancing performance across all three above-mentioned medical image recognition tasks. The code is available at https://github.com/XiaZunhui/MedFormer.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02488v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02439v1",
    "title": "Introducing a New Brexit-Related Uncertainty Index: Its Evolution and Economic Consequences",
    "authors": [
      "Ismet Gocer",
      "Julia Darby",
      "Serdar Ongan"
    ],
    "abstract": "Important game-changer economic events and transformations cause uncertainties that may affect investment decisions, capital flows, international trade, and macroeconomic variables. One such major transformation is Brexit, which refers to the multiyear process through which the UK withdrew from the EU. This study develops and uses a new Brexit-Related Uncertainty Index (BRUI). In creating this index, we apply Text Mining, Context Window, Natural Language Processing (NLP), and Large Language Models (LLMs) from Deep Learning techniques to analyse the monthly country reports of the Economist Intelligence Unit from May 2012 to January 2025. Additionally, we employ a standard vector autoregression (VAR) analysis to examine the model-implied responses of various macroeconomic variables to BRUI shocks. While developing the BRUI, we also create a complementary COVID-19 Related Uncertainty Index (CRUI) to distinguish the uncertainties stemming from these distinct events. Empirical findings and comparisons of BRUI with other earlier-developed uncertainty indexes demonstrate the robustness of the new index. This new index can assist British policymakers in measuring and understanding the impacts of Brexit-related uncertainties, enabling more effective policy formulation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "econ.GN",
      "q-fin.EC"
    ],
    "url": "http://arxiv.org/abs/2507.02439v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02437v1",
    "title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning",
    "authors": [
      "Wei Li",
      "Jingyang Zhang",
      "Lihao Liu",
      "Guoan Wang",
      "Junjun He",
      "Yang Chen",
      "Lixu Gu"
    ],
    "abstract": "Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a source model to unseen medical sites using unlabeled test data, due to the high cost of data annotation. Existing TTA methods consider scenarios where data from one or multiple domains arrives in complete domain units. However, in clinical practice, data usually arrives in domain fragments of arbitrary lengths and in random arrival orders, due to resource constraints and patient variability. This paper investigates a practical Free-Form Test-Time Adaptation (F$^{2}$TTA) task, where a source model is adapted to such free-form domain fragments, with shifts occurring between fragments unpredictably. In this setting, these shifts could distort the adaptation process. To address this problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT) framework. I-DiPT employs an image-invariant prompt to explore domain-invariant representations for mitigating the unpredictable shifts, and an image-specific prompt to adapt the source model to each test image from the incoming fragments. The prompts may suffer from insufficient knowledge representation since only one image is available for training. To overcome this limitation, we first introduce Uncertainty-oriented Masking (UoM), which encourages the prompts to extract sufficient information from the incoming image via masked consistency learning driven by the uncertainty of the source model representations. Then, we further propose a Parallel Graph Distillation (PGD) method that reuses knowledge from historical image-specific and image-invariant prompts through parallel graph networks. Experiments on breast cancer and glaucoma classification demonstrate the superiority of our method over existing TTA approaches in F$^{2}$TTA. Code is available at https://github.com/mar-cry/F2TTA.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "eess.IV"
    ],
    "url": "http://arxiv.org/abs/2507.02437v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02399v1",
    "title": "TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation",
    "authors": [
      "Peilin Zhang",
      "Shaouxan Wua",
      "Jun Feng",
      "Zhuo Jin",
      "Zhizezhang Gao",
      "Jingkun Chen",
      "Yaqiong Xing",
      "Xiao Zhang"
    ],
    "abstract": "Background and objective: Medical image segmentation is a core task in various clinical applications. However, acquiring large-scale, fully annotated medical image datasets is both time-consuming and costly. Scribble annotations, as a form of sparse labeling, provide an efficient and cost-effective alternative for medical image segmentation. However, the sparsity of scribble annotations limits the feature learning of the target region and lacks sufficient boundary supervision, which poses significant challenges for training segmentation networks. Methods: We propose TAB Net, a novel weakly-supervised medical image segmentation framework, consisting of two key components: the triplet augmentation self-recovery (TAS) module and the boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances feature learning through three complementary augmentation strategies: intensity transformation improves the model's sensitivity to texture and contrast variations, cutout forces the network to capture local anatomical structures by masking key regions, and jigsaw augmentation strengthens the modeling of global anatomical layout by disrupting spatial continuity. By guiding the network to recover complete masks from diverse augmented inputs, TAS promotes a deeper semantic understanding of medical images under sparse supervision. The BAP module enhances pseudo-supervision accuracy and boundary modeling by fusing dual-branch predictions into a loss-weighted pseudo-label and introducing a boundary-aware loss for fine-grained contour refinement. Results: Experimental evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB Net significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation. Moreover, it achieves performance comparable to that of fully supervised methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02399v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02367v1",
    "title": "A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\\left[^{18}\\text{F}\\right]$FDG PET imaging",
    "authors": [
      "Christian Salomonsen",
      "Luigi Tommaso Luppino",
      "Fredrik Aspheim",
      "Kristoffer Wickstrøm",
      "Elisabeth Wetzer",
      "Michael Kampffmeyer",
      "Rodrigo Berzaghi",
      "Rune Sundset",
      "Robert Jenssen",
      "Samuel Kuttner"
    ],
    "abstract": "Dynamic positron emission tomography (PET) and kinetic modeling are pivotal in advancing tracer development research in small animal studies. Accurate kinetic modeling requires precise input function estimation, traditionally achieved via arterial blood sampling. However, arterial cannulation in small animals like mice, involves intricate, time-consuming, and terminal procedures, precluding longitudinal studies. This work proposes a non-invasive, fully convolutional deep learning-based approach (FC-DLIF) to predict input functions directly from PET imaging, potentially eliminating the need for blood sampling in dynamic small-animal PET. The proposed FC-DLIF model includes a spatial feature extractor acting on the volumetric time frames of the PET sequence, extracting spatial features. These are subsequently further processed in a temporal feature extractor that predicts the arterial input function. The proposed approach is trained and evaluated using images and arterial blood curves from [$^{18}$F]FDG data using cross validation. Further, the model applicability is evaluated on imaging data and arterial blood curves collected using two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The model was further evaluated on data truncated and shifted in time, to simulate shorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts the arterial input function with respect to mean squared error and correlation. Furthermore, the FC-DLIF model is able to predict the arterial input function even from truncated and shifted samples. The model fails to predict the AIF from samples collected using different radiotracers, as these are not represented in the training data. Our deep learning-based input function offers a non-invasive and reliable alternative to arterial blood sampling, proving robust and flexible to temporal shifts and different scan durations.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph",
      "q-bio.QM"
    ],
    "url": "http://arxiv.org/abs/2507.02367v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02122v1",
    "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training",
    "authors": [
      "Neil K. R. Sehgal",
      "Hita Kambhamettu",
      "Allen Chang",
      "Andrew Zhu",
      "Lyle Ungar",
      "Sharath Chandra Guntuku"
    ],
    "abstract": "Effective communication in serious illness and palliative care is essential but often under-taught due to limited access to training resources like standardized patients. We present PAL (Palliative Assisted Learning-bot), a conversational system that simulates emotionally nuanced patient interactions and delivers structured feedback grounded in an existing empathy-based framework. PAL supports text and voice modalities and is designed to scaffold clinical skill-building through repeated, low-cost practice. Through a mixed-methods study with 17 U.S. medical trainees and clinicians, we explore user engagement with PAL, evaluate usability, and examine design tensions around modalities, emotional realism, and feedback delivery. Participants found PAL helpful for reflection and skill refinement, though some noted limitations in emotional authenticity and the adaptability of feedback. We contribute: (1) empirical evidence that large language models can support palliative communication training; (2) design insights for modality-aware, emotionally sensitive simulation tools; and (3) implications for systems that support emotional labor, cooperative learning, and AI-augmented training in high-stakes care settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.CY"
    ],
    "url": "http://arxiv.org/abs/2507.02122v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02088v1",
    "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models",
    "authors": [
      "Tian Lan",
      "Xiangdong Su",
      "Xu Liu",
      "Ruirui Wang",
      "Ke Chang",
      "Jiang Li",
      "Guanglai Gao"
    ],
    "abstract": "As large language models (LLMs) are increasingly applied to various NLP tasks, their inherent biases are gradually disclosed. Therefore, measuring biases in LLMs is crucial to mitigate its ethical risks. However, most existing bias evaluation datasets focus on English and North American culture, and their bias categories are not fully applicable to other cultures. The datasets grounded in the Chinese language and culture are scarce. More importantly, these datasets usually only support single evaluation tasks and cannot evaluate the bias from multiple aspects in LLMs. To address these issues, we present a Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias evaluation instances, covering 12 single bias categories, 82 subcategories and introducing 5 evaluation tasks, providing extensive category coverage, content diversity, and measuring comprehensiveness. Additionally, we evaluate several popular LLMs from different series and with parameter sizes. In general, all these LLMs demonstrated varying degrees of bias. We conduct an in-depth analysis of results, offering novel insights into bias in LLMs.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02088v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01881v1",
    "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs",
    "authors": [
      "Niccolò McConnell",
      "Pardeep Vasudev",
      "Daisuke Yamada",
      "Daryl Cheng",
      "Mehran Azimbagirad",
      "John McCabe",
      "Shahab Aslani",
      "Ahmed H. Shahin",
      "Yukun Zhou",
      "The SUMMIT Consortium",
      "Andre Altmann",
      "Yipeng Hu",
      "Paul Taylor",
      "Sam M. Janes",
      "Daniel C. Alexander",
      "Joseph Jacob"
    ],
    "abstract": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening (LCS) programs is increasing in uptake worldwide. LCS programs herald a generational opportunity to simultaneously detect cancer and non-cancer-related early-stage lung disease. Yet these efforts are hampered by a shortage of radiologists to interpret scans at scale. Here, we present TANGERINE, a computationally frugal, open-source vision foundation model for volumetric LDCT analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can be fine-tuned off the shelf for a wide range of disease-specific tasks with limited computational resources and training data. Relative to models trained from scratch, TANGERINE demonstrates fast convergence during fine-tuning, thereby requiring significantly fewer GPU hours, and displays strong label efficiency, achieving comparable or superior performance with a fraction of fine-tuning data. Pretrained using self-supervised learning on over 98,000 thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public datasets, TANGERINE achieves state-of-the-art performance across 14 disease classification tasks, including lung cancer and multiple respiratory diseases, while generalising robustly across diverse clinical centres. By extending a masked autoencoder framework to 3D imaging, TANGERINE offers a scalable solution for LDCT analysis, departing from recent closed, resource-intensive models by combining architectural simplicity, public availability, and modest computational requirements. Its accessible, open-source lightweight design lays the foundation for rapid integration into next-generation medical imaging tools that could transform LCS initiatives, allowing them to pivot from a singular focus on lung cancer detection to comprehensive respiratory disease management in high-risk populations.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.01881v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01828v1",
    "title": "Autoadaptive Medical Segment Anything Model",
    "authors": [
      "Tyler Ward",
      "Meredith K. Owen",
      "O'Kira Coleman",
      "Brian Noehren",
      "Abdullah-Al-Zubaer Imran"
    ],
    "abstract": "Medical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose ADA-SAM (automated, domain-specific, and adaptive segment anything model), a novel multitask learning framework for medical image segmentation that leverages class activation maps from an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the Segment Anything (SAM) framework. Additionally, our ADA-SAM model employs a novel gradient feedback mechanism to create a learnable connection between the segmentation and classification branches by using the segmentation gradients to guide and improve the classification predictions. We validate ADA-SAM on real-world clinical data collected during rehabilitation trials, and demonstrate that our proposed method outperforms both fully-supervised and semi-supervised baselines by double digits in limited label settings. Our code is available at: https://github.com/tbwa233/ADA-SAM.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.01828v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01811v1",
    "title": "Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures",
    "authors": [
      "Yash Kulkarni",
      "Susheela Sharma",
      "Sarah Go",
      "Jordan P. Amadio",
      "Mohsen Khadem",
      "Farshid Alambeigi"
    ],
    "abstract": "Current pelvic fixation techniques rely on rigid drilling tools, which inherently constrain the placement of rigid medical screws in the complex anatomy of pelvis. These constraints prevent medical screws from following anatomically optimal pathways and force clinicians to fixate screws in linear trajectories. This suboptimal approach, combined with the unnatural placement of the excessively long screws, lead to complications such as screw misplacement, extended surgery times, and increased radiation exposure due to repeated X-ray images taken ensure to safety of procedure. To address these challenges, in this paper, we present the design and development of a unique 4 degree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic CT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling trajectories that follow the natural curvatures of the pelvic anatomy. The performance of the pelvic CT-SDR was thoroughly evaluated through several S-shape drilling experiments in simulated bone phantoms.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO"
    ],
    "url": "http://arxiv.org/abs/2507.01811v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01802v1",
    "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding",
    "authors": [
      "Katharina Beckh",
      "Elisa Studeny",
      "Sujan Sai Gannamaneni",
      "Dario Antweiler",
      "Stefan Rüping"
    ],
    "abstract": "Automatic medical coding has the potential to ease documentation and billing processes. For this task, transparency plays an important role for medical coders and regulatory bodies, which can be achieved using explainability methods. However, the evaluation of these approaches has been mostly limited to short text and binary settings due to a scarcity of annotated data. Recent efforts by Cheng et al. (2023) have introduced the MDACE dataset, which provides a valuable resource containing code evidence in clinical records. In this work, we conduct an in-depth analysis of the MDACE dataset and perform plausibility evaluation of current explainable medical coding systems from an applied perspective. With this, we contribute to a deeper understanding of automatic medical coding and evidence extraction. Our findings reveal that ground truth evidence aligns with code descriptions to a certain degree. An investigation into state-of-the-art approaches shows a high overlap with ground truth evidence. We propose match measures and highlight success and failure cases. Based on our findings, we provide recommendations for developing and evaluating explainable medical coding systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.01802v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01788v1",
    "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging",
    "authors": [
      "Montasir Shams",
      "Chashi Mahiul Islam",
      "Shaeke Salman",
      "Phat Tran",
      "Xiuwen Liu"
    ],
    "abstract": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60\\%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.01788v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01763v2",
    "title": "Probabilistic Proton Treatment Planning: a novel approach for optimizing underdosage and overdosage probabilities of target and organ structures",
    "authors": [
      "Jelte R. de Jong",
      "Sebastiaan Breedveld",
      "Steven J. M. Habraken",
      "Mischa S. Hoogeman",
      "Danny Lathouwers",
      "Zoltán Perkó"
    ],
    "abstract": "Treatment planning uncertainties are typically managed using margin-based or robust optimization. Margin-based methods expand the clinical target volume (CTV) to a planning target volume, generally unsuited for proton therapy. Robust optimization considers worst-case scenarios, but its quality depends on the uncertainty scenario set: excluding extremes reduces robustness, while too many make plans overly conservative. Probabilistic optimization overcomes these limits by modeling a continuous scenario distribution. We propose a novel probabilistic optimization approach that steers plans toward individualized probability levels to control CTV and organs-at-risk (OARs) under- and overdosage. Voxel-wise dose percentiles ($d$) are estimated by expected value ($E$) and standard deviation (SD) as $E[d] \\pm \\delta \\cdot SD[d]$, where $\\delta$ is iteratively tuned to match the target percentile given Gaussian-distributed setup (3 mm) and range (3%) uncertainties. The method involves an inner optimization of $E[d] \\pm \\delta \\cdot SD[d]$ for fixed $\\delta$, and an outer loop updating $\\delta$. Polynomial Chaos Expansion (PCE) provides accurate and efficient dose estimates during optimization. We validated the method on a spherical CTV abutted by an OAR in different directions and a horseshoe-shaped CTV surrounding a cylindrical spine. For spherical cases with similar CTV coverage, $P(D_{2\\%} > 30 Gy)$ dropped by 10-15%; for matched OAR dose, $P(D_{98\\%} > 57 Gy)$ increased by 67.5-71%. In spinal plans, $P(D_{98\\%} > 57 Gy)$ increased by 10-15% while $P(D_{2\\%} > 30 Gy)$ dropped 24-28%. Probabilistic and robust optimization times were comparable for spherical (hours) but longer for spinal cases (7.5 - 11.5 h vs. 9 - 20 min). Compared to discrete scenario-based optimization, the probabilistic method offered better OAR sparing or target coverage depending on the set priorities.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.med-ph"
    ],
    "url": "http://arxiv.org/abs/2507.01763v2"
  },
  {
    "id": "http://arxiv.org/abs/2507.01744v1",
    "title": "Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans",
    "authors": [
      "Benjamin Jin",
      "Grant Mair",
      "Joanna M. Wardlaw",
      "Maria del C. Valdés Hernández"
    ],
    "abstract": "Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.01744v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01734v1",
    "title": "LLMs for Legal Subsumption in German Employment Contracts",
    "authors": [
      "Oliver Wardas",
      "Florian Matthes"
    ],
    "abstract": "Legal work, characterized by its text-heavy and resource-intensive nature, presents unique challenges and opportunities for NLP research. While data-driven approaches have advanced the field, their lack of interpretability and trustworthiness limits their applicability in dynamic legal environments. To address these issues, we collaborated with legal experts to extend an existing dataset and explored the use of Large Language Models (LLMs) and in-context learning to evaluate the legality of clauses in German employment contracts. Our work evaluates the ability of different LLMs to classify clauses as \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal context, full-text sources of laws and court rulings, and distilled versions of these (referred to as examination guidelines). Results show that full-text sources moderately improve performance, while examination guidelines significantly enhance recall for void clauses and weighted F1-Score, reaching 80\\%. Despite these advancements, LLMs' performance when using full-text sources remains substantially below that of human lawyers. We contribute an extended dataset, including examination guidelines, referenced legal sources, and corresponding annotations, alongside our code and all log files. Our findings highlight the potential of LLMs to assist lawyers in contract legality review while also underscoring the limitations of the methods presented.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "url": "http://arxiv.org/abs/2507.01734v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01633v1",
    "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation",
    "authors": [
      "Georgii Levtsov",
      "Dmitry Ustalov"
    ],
    "abstract": "With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley-Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at https://github.com/HSPyroblast/srw-ranking under a permissive license.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.IR",
      "62-04",
      "D.2.3"
    ],
    "url": "http://arxiv.org/abs/2507.01633v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01564v1",
    "title": "Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling",
    "authors": [
      "Chia-Ming Lee",
      "Bo-Cheng Qiu",
      "Ting-Yao Chen",
      "Ming-Han Sun",
      "Fang-Ying Lin",
      "Jung-Tse Tsai",
      "I-An Tsai",
      "Yu-Fan Lin",
      "Chih-Chung Hsu"
    ],
    "abstract": "We present our solution for the Multi-Source COVID-19 Detection Challenge, which classifies chest CT scans from four distinct medical centers. To address multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL) framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing pipeline combines lung region extraction, quality control, and adaptive slice sampling to select eight representative slices per scan. We compare EfficientNet and Swin Transformer architectures on the validation set. The EfficientNet model achieves an F1-score of 94.68%, compared to the Swin Transformer's 93.34%. The results demonstrate the effectiveness of our KDS-based pipeline on multi-source data and highlight the importance of dataset balance in multi-institutional medical imaging evaluation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.01564v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01456v1",
    "title": "QC-OT: Optimal Transport with Quasiconformal Mapping",
    "authors": [
      "Yuping Lv",
      "Qi Zhao",
      "Xuebin Chang",
      "Wei Zeng"
    ],
    "abstract": "The optimal transport (OT) map offers the most economical way to transfer one probability measure distribution to another. Classical OT theory does not involve a discussion of preserving topological connections and orientations in transmission results and processes. Existing numerical and geometric methods for computing OT seldom pays specific attention on this aspect. Especially, when dealing with the triangular mesh data, the known semi-discrete geometric OT (sd-OT) method employs critical operation of Delaunay triangulation (DT) to adapt topology to ensure the convexity of the energy function and the existence of the solution. This change in topology hampers the applicability of OT in modeling non-flip physical deformations in real-world tasks such as shape registration and editing problems in computer vision and medical imaging fields. This work introduces the topology structure-preserving optimal transport (QC-OT) map for the triangular mesh input. The computational strategy focuses on the two components: relaxing DT and convexity check in sd-OT and integrating quasiconformal (QC) correction. Here, quasiconformal mapping is employed to correct the regions unexpected distortions, and guarantee the topological preserving property of the transport. Furthermore, the spatial-temporal topology-preserving OT map is presented based t-OT to study the dynamics of the transportation. Multiple experiments have validated the efficiency and effectiveness of the proposed method and demonstrated its potential in the applications of mesh parameterization and image editing.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.GN",
      "51H20",
      "I.3.5"
    ],
    "url": "http://arxiv.org/abs/2507.01456v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01437v1",
    "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction",
    "authors": [
      "Ting Xu",
      "Xiaoxiao Deng",
      "Xiandong Meng",
      "Haifeng Yang",
      "Yan Wu"
    ],
    "abstract": "This paper addresses the challenges posed by the unstructured nature and high-dimensional semantic complexity of electronic health record texts. A deep learning method based on attention mechanisms is proposed to achieve unified modeling for information extraction and multi-label disease prediction. The study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is used to perform representation learning over clinical text. Multi-layer self-attention mechanisms are employed to capture key medical entities and their contextual relationships. A Sigmoid-based multi-label classifier is then applied to predict multiple disease labels. The model incorporates a context-aware semantic alignment mechanism, enhancing its representational capacity in typical medical scenarios such as label co-occurrence and sparse information. To comprehensively evaluate model performance, a series of experiments were conducted, including baseline comparisons, hyperparameter sensitivity analysis, data perturbation studies, and noise injection tests. Results demonstrate that the proposed method consistently outperforms representative existing approaches across multiple performance metrics. The model maintains strong generalization under varying data scales, interference levels, and model depth configurations. The framework developed in this study offers an efficient algorithmic foundation for processing real-world clinical texts and presents practical significance for multi-label medical text modeling tasks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.01437v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01410v1",
    "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models",
    "authors": [
      "Abeer Dyoub",
      "Francesca A. Lisi"
    ],
    "abstract": "The ontological and epistemic complexities inherent in the moral domain make it challenging to establish clear standards for evaluating the performance of a moral machine. In this paper, we present a formal method to describe Ethical Decision Making models based on ethical risk assessment. Then, we show how these models that are specified as fuzzy rules can be verified and validated using fuzzy Petri nets. A case study from the medical field is considered to illustrate the proposed approach.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.01410v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01401v1",
    "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound",
    "authors": [
      "Huanwen Liang",
      "Jingxian Xu",
      "Yuanji Zhang",
      "Yuhao Huang",
      "Yuhan Zhang",
      "Xin Yang",
      "Ran Li",
      "Xuedong Deng",
      "Yanjun Liu",
      "Guowei Tao",
      "Yun Wu",
      "Sheng Zhao",
      "Xinru Gao",
      "Dong Ni"
    ],
    "abstract": "Fetal abdominal malformations are serious congenital anomalies that require accurate diagnosis to guide pregnancy management and reduce mortality. Although AI has demonstrated significant potential in medical diagnosis, its application to prenatal abdominal anomalies remains limited. Most existing studies focus on image-level classification and rely on standard plane localization, placing less emphasis on case-level diagnosis. In this paper, we develop a case-level multiple instance learning (MIL)-based method, free of standard plane localization, for classifying fetal abdominal anomalies in prenatal ultrasound. Our contribution is three-fold. First, we adopt a mixture-of-attention-experts module (MoAE) to weight different attention heads for various planes. Secondly, we propose a medical-knowledge-driven feature selection module (MFS) to align image features with medical knowledge, performing self-supervised image token selection at the case-level. Finally, we propose a prompt-based prototype learning (PPL) to enhance the MFS. Extensively validated on a large prenatal abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748 images and 6 categories, our proposed method outperforms the state-of-the-art competitors. Codes are available at:https://github.com/LL-AC/AAcls.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.01401v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01399v1",
    "title": "Reconstruction of the observable universe from the integrated Sachs-Wolfe effect",
    "authors": [
      "Julianne Chung",
      "Yiran Wang"
    ],
    "abstract": "The integrated Sachs-Wolfe (ISW) effect is a property of the Cosmic Microwave Background (CMB), in which photons from the CMB are gravitationally redshifted, causing the anisotropies in the CMB. An intriguing question is whether one can infer the gravitational perturbations from the ISW effect observed near the Earth. In this work, we address the question using a tomographic reconstruction approach, similar to X-ray CT reconstruction in medical imaging. We develop the mathematical analysis for the stable inversion of the X-ray transform in the cosmological setting. In addition, we provide a numerical study of reconstruction methods, thereby demonstrating the feasibility and potential of the tomography method.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math-ph",
      "cs.NA",
      "math.MP",
      "math.NA"
    ],
    "url": "http://arxiv.org/abs/2507.01399v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01326v1",
    "title": "Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction",
    "authors": [
      "Dong Liang",
      "Xingyu Qiu",
      "Yuzhen Li",
      "Wei Wang",
      "Kuanquan Wang",
      "Suyu Dong",
      "Gongning Luo"
    ],
    "abstract": "MR imaging techniques are of great benefit to disease diagnosis. However, due to the limitation of MR devices, significant intensity inhomogeneity often exists in imaging results, which impedes both qualitative and quantitative medical analysis. Recently, several unsupervised deep learning-based models have been proposed for MR image improvement. However, these models merely concentrate on global appearance learning, and neglect constraints from image structures and smoothness of bias field, leading to distorted corrected results. In this paper, novel structure and smoothness constrained dual networks, named S2DNets, are proposed aiming to self-supervised bias field correction. S2DNets introduce piece-wise structural constraints and smoothness of bias field for network training to effectively remove non-uniform intensity and retain much more structural details. Extensive experiments executed on both clinical and simulated MR datasets show that the proposed model outperforms other conventional and deep learning-based models. In addition to comparison on visual metrics, downstream MR image segmentation tasks are also used to evaluate the impact of the proposed model. The source code is available at: https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.01326v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01323v1",
    "title": "SWinMamba: Serpentine Window State Space Model for Vascular Segmentation",
    "authors": [
      "Rongchang Zhao",
      "Huanchi Liu",
      "Jian Zhang"
    ],
    "abstract": "Vascular segmentation in medical images is crucial for disease diagnosis and surgical navigation. However, the segmented vascular structure is often discontinuous due to its slender nature and inadequate prior modeling. In this paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve accurate vascular segmentation. The proposed SWinMamba innovatively models the continuity of slender vascular structures by incorporating serpentine window sequences into bidirectional state space models. The serpentine window sequences enable efficient feature capturing by adaptively guiding global visual context modeling to the vascular structure. Specifically, the Serpentine Window Tokenizer (SWToken) adaptively splits the input image using overlapping serpentine window sequences, enabling flexible receptive fields (RFs) for vascular structure modeling. The Bidirectional Aggregation Module (BAM) integrates coherent local features in the RFs for vascular continuity representation. In addition, dual-domain learning with Spatial-Frequency Fusion Unit (SFFU) is designed to enhance the feature representation of vascular structure. Extensive experiments on three challenging datasets demonstrate that the proposed SWinMamba achieves superior performance with complete and connected vessels.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.01323v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01304v2",
    "title": "A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies",
    "authors": [
      "Ion Nemteanu",
      "Adir Mancebo Jr.",
      "Leslie Joe",
      "Ryan Lopez",
      "Patricia Lopez",
      "Warren Woodrich Pettine"
    ],
    "abstract": "Artificial intelligence (AI) offers incredible possibilities for patient care, but raises significant ethical issues, such as the potential for bias. Powerful ethical frameworks exist to minimize these issues, but are often developed for academic or regulatory environments and tend to be comprehensive but overly prescriptive, making them difficult to operationalize within fast-paced, resource-constrained environments. We introduce the Scalable Agile Framework for Execution in AI (SAFE-AI) designed to balance ethical rigor with business priorities by embedding ethical oversight into standard Agile-based product development workflows. The framework emphasizes the early establishment of testable acceptance criteria, fairness metrics, and transparency metrics to manage model uncertainty, while also promoting continuous monitoring and re-evaluation of these metrics across the AI lifecycle. A core component of this framework are responsibility metrics using scenario-based probability analogy mapping designed to enhance transparency and stakeholder trust. This ensures that retraining or tuning activities are subject to lightweight but meaningful ethical review. By focusing on the minimum necessary requirements for responsible development, our framework offers a scalable, business-aligned approach to ethical AI suitable for organizations without dedicated ethics teams.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CY"
    ],
    "url": "http://arxiv.org/abs/2507.01304v2"
  },
  {
    "id": "http://arxiv.org/abs/2507.01291v1",
    "title": "PanTS: The Pancreatic Tumor Segmentation Dataset",
    "authors": [
      "Wenxuan Li",
      "Xinze Zhou",
      "Qi Chen",
      "Tianyu Lin",
      "Pedro R. A. S. Bassi",
      "Szymon Plotka",
      "Jaroslaw B. Cwikla",
      "Xiaoxi Chen",
      "Chen Ye",
      "Zheren Zhu",
      "Kai Ding",
      "Heng Li",
      "Kang Wang",
      "Yang Yang",
      "Yucheng Tang",
      "Daguang Xu",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ],
    "abstract": "PanTS is a large-scale, multi-institutional dataset curated to advance research in pancreatic CT analysis. It contains 36,390 CT scans from 145 medical centers, with expert-validated, voxel-wise annotations of over 993,000 anatomical structures, covering pancreatic tumors, pancreas head, body, and tail, and 24 surrounding anatomical structures such as vascular/skeletal structures and abdominal/thoracic organs. Each scan includes metadata such as patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness, etc. AI models trained on PanTS achieve significantly better performance in pancreatic tumor detection, localization, and segmentation compared to those trained on existing public datasets. Our analysis indicates that these gains are directly attributable to the 16x larger-scale tumor annotations and indirectly supported by the 24 additional surrounding anatomical structures. As the largest and most comprehensive resource of its kind, PanTS offers a new benchmark for developing and evaluating AI models in pancreatic CT analysis.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.01291v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01282v1",
    "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care",
    "authors": [
      "Matthew JY Kang",
      "Wenli Yang",
      "Monica R Roberts",
      "Byeong Ho Kang",
      "Charles B Malpas"
    ],
    "abstract": "The recent boom of large language models (LLMs) has re-ignited the hope that artificial intelligence (AI) systems could aid medical diagnosis. Yet despite dazzling benchmark scores, LLM assistants have yet to deliver measurable improvements at the bedside. This scoping review aims to highlight the areas where AI is limited to make practical contributions in the clinical setting, specifically in dementia diagnosis and care.   Standalone machine-learning models excel at pattern recognition but seldom provide actionable, interpretable guidance, eroding clinician trust. Adjacent use of LLMs by physicians did not result in better diagnostic accuracy or speed. Key limitations trace to the data-driven paradigm: black-box outputs which lack transparency, vulnerability to hallucinations, and weak causal reasoning. Hybrid approaches that combine statistical learning with expert rule-based knowledge, and involve clinicians throughout the process help bring back interpretability. They also fit better with existing clinical workflows, as seen in examples like PEIRS and ATHENA-CDS.   Future decision-support should prioritise explanatory coherence by linking predictions to clinically meaningful causes. This can be done through neuro-symbolic or hybrid AI that combines the language ability of LLMs with human causal expertise. AI researchers have addressed this direction, with explainable AI and neuro-symbolic AI being the next logical steps in further advancement in AI. However, they are still based on data-driven knowledge integration instead of human-in-the-loop approaches. Future research should measure success not only by accuracy but by improvements in clinician understanding, workflow fit, and patient outcomes. A better understanding of what helps improve human-computer interactions is greatly needed for AI systems to become part of clinical practice.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2507.01282v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01279v1",
    "title": "Classification based deep learning models for lung cancer and disease using medical images",
    "authors": [
      "Ahmad Chaddad",
      "Jihao Peng",
      "Yihang Wu"
    ],
    "abstract": "The use of deep learning (DL) in medical image analysis has significantly improved the ability to predict lung cancer. In this study, we introduce a novel deep convolutional neural network (CNN) model, named ResNet+, which is based on the established ResNet framework. This model is specifically designed to improve the prediction of lung cancer and diseases using the images. To address the challenge of missing feature information that occurs during the downsampling process in CNNs, we integrate the ResNet-D module, a variant designed to enhance feature extraction capabilities by modifying the downsampling layers, into the traditional ResNet model. Furthermore, a convolutional attention module was incorporated into the bottleneck layers to enhance model generalization by allowing the network to focus on relevant regions of the input images. We evaluated the proposed model using five public datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT $n$=425024 images). To address class imbalance, we used data augmentation techniques to artificially increase the representation of underrepresented classes in the training dataset. The experimental results show that ResNet+ model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\\% on the LC25000 dataset and 99.25/99.13\\% on the IQ-OTH/NCCD dataset. Furthermore, the ResNet+ model saved computational cost compared to the original ResNet series in predicting lung cancer images. The proposed model outperformed the baseline models on publicly available datasets, achieving better performance metrics. Our codes are publicly available at https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.01279v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01213v1",
    "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis",
    "authors": [
      "Adamu Lawan",
      "Juhua Pu",
      "Haruna Yunusa",
      "Jawad Muhammad",
      "Muhammad Lawan"
    ],
    "abstract": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language Processing (NLP) task that extracts aspects from text and determines their associated sentiments, enabling fine-grained analysis of user opinions. Existing ABSA methods struggle to balance computational efficiency with high performance: deep learning models often lack global context, transformers demand significant computational resources, and Mamba-based approaches face CUDA dependency and diminished local correlations. Recent advancements in Extended Long Short-Term Memory (xLSTM) models, particularly their efficient modeling of long-range dependencies, have significantly advanced the NLP community. However, their potential in ABSA remains untapped. To this end, we propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework integrating a bi-directional mLSTM architecture with forward and partially flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context modeling by processing the initial sequence segment in reverse with dedicated parameters, preserving critical short-range patterns. We further introduce an mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that dynamically combines forward mLSTM outputs as query and key with PF-mLSTM outputs as value, optimizing short-range dependency capture while maintaining global context and efficiency. Experimental results on three benchmark datasets demonstrate that MEGA outperforms state-of-the-art baselines, achieving superior accuracy and efficiency in ABSA tasks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.01213v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01210v1",
    "title": "Harnessing coherent-wave control for sensing applications",
    "authors": [
      "Pablo Jara",
      "Arthur Goetschy",
      "Hui Cao",
      "Alexey Yamilov"
    ],
    "abstract": "Imaging techniques such as functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) achieve deep, non-invasive sensing in turbid media, but they are constrained by the photon budget. Wavefront shaping (WFS) can enhance signal strength via interference at specific locations within scattering media, enhancing light-matter interactions and potentially extending the penetration depth of these techniques. Interpreting the resulting measurements rests on the knowledge of optical sensitivity - a relationship between detected signal changes and perturbations at a specific location inside the medium. However, conventional diffusion-based sensitivity models rely on assumptions that become invalid under coherent illumination. In this work, we develop a microscopic theory for optical sensitivity that captures the inherent interference effects that diffusion theory necessarily neglects. We analytically show that under random illumination, the microscopic and diffusive treatments coincide. Using our microscopic approach, we explore WFS strategies for enhancing optical sensitivity beyond the diffusive result. We demonstrate that the input state obtained through phase conjugation at a given point inside the system leads to the largest enhancement of optical sensitivity but requires an input wavefront that depends on the target position. In sharp contrast, the maximum remission eigenchannel leads to a global enhancement of the sensitivity map with a fixed input wavefront. This global enhancement equals to remission enhancement and preserves the spatial distribution of the sensitivity, making it compatible with existing DOT reconstruction algorithms. Our results establish the theoretical foundation for integrating wavefront control with diffuse optical imaging, enabling deeper tissue penetration through improved signal strength in biomedical applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.optics",
      "cond-mat.dis-nn",
      "physics.med-ph"
    ],
    "url": "http://arxiv.org/abs/2507.01210v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01177v1",
    "title": "Bayesian Regression Analysis with the Drift-Diffusion Model",
    "authors": [
      "Zekai Jin",
      "Yaakov Stern",
      "Seonjoo Lee"
    ],
    "abstract": "The Drift-Diffusion Model (DDM) is widely used in neuropsychological studies to understand the decision process by incorporating both reaction times and subjects' responses. Various models have been developed to estimate DDM parameters, with some employing Bayesian inference. However, when examining associations between phenotypes of interest and DDM parameters, most studies adopt a two-step approach: first estimating DDM parameters, then applying a separate statistical model to the estimated values. Despite the potential for bias, this practice remains common, primarily due to researchers' unfamiliarity with Bayesian modeling. To address this issue, this tutorial presents the implementations and advantages of fitting a unified Bayesian hierarchical regression model that integrates trial-level drift-diffusion modeling and subject-level regression between DDM parameters and other variables. The R package RegDDM, developed and demonstrated in this tutorial, facilitates this integrated modeling approach.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.AP"
    ],
    "url": "http://arxiv.org/abs/2507.01177v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01152v1",
    "title": "SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound",
    "authors": [
      "Yunke Ao",
      "Masoud Moghani",
      "Mayank Mittal",
      "Manish Prajapat",
      "Luohong Wu",
      "Frederic Giraud",
      "Fabio Carrillo",
      "Andreas Krause",
      "Philipp Fürnstahl"
    ],
    "abstract": "Ultrasound (US) is a widely used medical imaging modality due to its real-time capabilities, non-invasive nature, and cost-effectiveness. Robotic ultrasound can further enhance its utility by reducing operator dependence and improving access to complex anatomical regions. For this, while deep reinforcement learning (DRL) and imitation learning (IL) have shown potential for autonomous navigation, their use in complex surgical tasks such as anatomy reconstruction and surgical guidance remains limited -- largely due to the lack of realistic and efficient simulation environments tailored to these tasks. We introduce SonoGym, a scalable simulation platform for complex robotic ultrasound tasks that enables parallel simulation across tens to hundreds of environments. Our framework supports realistic and real-time simulation of US data from CT-derived 3D models of the anatomy through both a physics-based and a generative modeling approach. Sonogym enables the training of DRL and recent IL agents (vision transformers and diffusion policies) for relevant tasks in robotic orthopedic surgery by integrating common robotic platforms and orthopedic end effectors. We further incorporate submodular DRL -- a recent method that handles history-dependent rewards -- for anatomy reconstruction and safe reinforcement learning for surgery. Our results demonstrate successful policy learning across a range of scenarios, while also highlighting the limitations of current methods in clinically relevant environments. We believe our simulation can facilitate research in robot learning approaches for such challenging robotic surgery applications. Dataset, codes, and videos are publicly available at https://sonogym.github.io/.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO"
    ],
    "url": "http://arxiv.org/abs/2507.01152v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00994v1",
    "title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
    "authors": [
      "Hippolyte Gisserot-Boukhlef",
      "Nicolas Boizard",
      "Manuel Faysse",
      "Duarte M. Alves",
      "Emmanuel Malherbe",
      "André F. T. Martins",
      "Céline Hudelot",
      "Pierre Colombo"
    ],
    "abstract": "Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.00994v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00962v1",
    "title": "clustra: A multi-platform k-means clustering algorithm for analysis of longitudinal trajectories in large electronic health records data",
    "authors": [
      "Nimish Adhikari",
      "Hanna Gerlovin",
      "George Ostrouchov",
      "Rachel Ehrbar",
      "Alyssa B. Dufour",
      "Brian R. Ferolito",
      "Serkalem Demissie",
      "Lauren Costa",
      "Yuk-Lam Ho",
      "Laura Tarko",
      "Edmon Begoli",
      "Kelly Cho",
      "David R. Gagnon"
    ],
    "abstract": "Background and Objective: Variables collected over time, or longitudinally, such as biologic measurements in electronic health records data, are not simple to summarize with a single time-point, and thus can be more holistically conceptualized as trajectories over time. Cluster analysis with longitudinal data further allows for clinical representation of groups of subjects with similar trajectories and identification of unique characteristics, or phenotypes, that can be investigated as risk factors or disease outcomes. Some of the challenges in estimating these clustered trajectories lie in the handling of observations at inconsistent time intervals and the usability of algorithms across programming languages.   Methods: We propose longitudinal trajectory clustering using a k-means algorithm with thin-plate regression splines, implemented across multiple platforms, the R package clustra and corresponding \\SAS macros. The \\SAS macros accommodate flexible clustering approaches, and also include visualization of the clusters, and silhouette plots for diagnostic evaluation of the appropriate cluster number. The R package, designed in parallel, has similar functionality, with additional multi-core processing and Rand-index-based diagnostics.   Results: The package and macros achieve comparable results when applied to an example of simulated blood pressure measurements based on real data from Veterans Affairs Healthcare recipients who were initiated on anti-hypertensive medication.   Conclusion: The R package clustra and the SAS macros integrate a K-means clustering algorithm for longitudinal trajectories that operates with large electronic health record data. The implementations provide comparable results in both platforms, satisfying the needs of investigators familiar with, or constrained by access to, one or the other platform.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.CO",
      "stat.AP"
    ],
    "url": "http://arxiv.org/abs/2507.00962v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00953v2",
    "title": "From Sentences to Sequences: Rethinking Languages in Biological System",
    "authors": [
      "Ke Liu",
      "Shuaike Shen",
      "Hao Chen"
    ],
    "abstract": "The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "q-bio.BM",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.00953v2"
  },
  {
    "id": "http://arxiv.org/abs/2507.01080v1",
    "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept",
    "authors": [
      "Edouard Lansiaux",
      "Ramy Azzouz",
      "Emmanuel Chazard",
      "Amélie Vromant",
      "Eric Wiel"
    ],
    "abstract": "Triage errors, including undertriage and overtriage, are persistent challenges in emergency departments (EDs). With increasing patient influx and staff shortages, the integration of artificial intelligence (AI) into triage protocols has gained attention. This study compares the performance of three AI models [Natural Language Processing (NLP), Large Language Models (LLM), and Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes against the FRENCH scale and clinical practice.We conducted a retrospective analysis of a prospectively recruited cohort gathering adult patient triage data over a 7-month period at the Roger Salengro Hospital ED (Lille, France). Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2) URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic details, verbatim chief complaints, vital signs, and triage outcomes based on the FRENCH scale and GEMSA coding. The primary outcome was the concordance of AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse triage (-4.343). Secondary analyses highlighted the effectiveness of URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness with structured data versus raw transcripts (either for GEMSA prediction or for FRENCH prediction). LLM architecture, through abstraction of patient representations, offers the most accurate triage predictions among tested models. Integrating AI into ED workflows could enhance patient safety and operational efficiency, though integration into clinical workflows requires addressing model limitations and ensuring ethical transparency.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.PF"
    ],
    "url": "http://arxiv.org/abs/2507.01080v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00868v2",
    "title": "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?",
    "authors": [
      "Simon Reiß",
      "Zdravko Marinov",
      "Alexander Jaus",
      "Constantin Seibold",
      "M. Saquib Sarfraz",
      "Erik Rodner",
      "Rainer Stiefelhagen"
    ],
    "abstract": "In this paper, we explore the potential of visual in-context learning to enable a single model to handle multiple tasks and adapt to new tasks during test time without re-training. Unlike previous approaches, our focus is on training in-context learners to adapt to sequences of tasks, rather than individual tasks. Our goal is to solve complex tasks that involve multiple intermediate steps using a single model, allowing users to define entire vision pipelines flexibly at test time. To achieve this, we first examine the properties and limitations of visual in-context learning architectures, with a particular focus on the role of codebooks. We then introduce a novel method for training in-context learners using a synthetic compositional task generation engine. This engine bootstraps task sequences from arbitrary segmentation datasets, enabling the training of visual in-context learners for compositional tasks. Additionally, we investigate different masking-based training objectives to gather insights into how to train models better for solving complex, compositional tasks. Our exploration not only provides important insights especially for multi-modal medical task sequences but also highlights challenges that need to be addressed.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00868v2"
  },
  {
    "id": "http://arxiv.org/abs/2507.00802v1",
    "title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency",
    "authors": [
      "Minye Shao",
      "Xingyu Miao",
      "Haoran Duan",
      "Zeyu Wang",
      "Jingkun Chen",
      "Yawen Huang",
      "Xian Wu",
      "Jingjing Deng",
      "Yang Long",
      "Yefeng Zheng"
    ],
    "abstract": "3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: https://github.com/VinyehShaw/TRACE.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00802v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00673v1",
    "title": "Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays",
    "authors": [
      "Abduz Zami",
      "Shadman Sobhan",
      "Rounaq Hossain",
      "Md. Sawran Sorker",
      "Mohiuddin Ahmed",
      "Md. Redwan Hossain"
    ],
    "abstract": "Image segmentation plays a vital role in the medical field by isolating organs or regions of interest from surrounding areas. Traditionally, segmentation models are trained on a specific organ or a disease, limiting their ability to handle other organs and diseases. At present, few advanced models can perform multi-organ or multi-disease segmentation, offering greater flexibility. Also, recently, prompt-based image segmentation has gained attention as a more flexible approach. It allows models to segment areas based on user-provided prompts. Despite these advances, there has been no dedicated work on prompt-based interactive multi-organ and multi-disease segmentation, especially for Chest X-rays. This work presents two main contributions: first, generating doodle prompts by medical experts of a collection of datasets from multiple sources with 23 classes, including 6 organs and 17 diseases, specifically designed for prompt-based Chest X-ray segmentation. Second, we introduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple organs and diseases from Chest X-rays. The model incorporates multi-stage feature fusion, enabling it to combine features from various network layers for better spatial and semantic understanding, enhancing segmentation accuracy. Compared to existing pre-trained models for prompt-based image segmentation, our model scores well, providing a reliable solution for segmenting Chest X-rays based on user prompts.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00673v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00585v2",
    "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation",
    "authors": [
      "Tang Hao",
      "Guo ZhiQing",
      "Wang LieJun",
      "Liu Chao"
    ],
    "abstract": "In recent years, it has been found that \"grandmother cells\" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00585v2"
  },
  {
    "id": "http://arxiv.org/abs/2507.00582v1",
    "title": "Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models",
    "authors": [
      "Yi Zhang",
      "Yidong Zhao",
      "Qian Tao"
    ],
    "abstract": "Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use recurrent neural networks (RNNs) to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibrium-seeking mechanism, bridging the gap between classical optimization-based and modern learning-based registration methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00582v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00576v1",
    "title": "DynoStore: A wide-area distribution system for the management of data over heterogeneous storage",
    "authors": [
      "Dante D. Sanchez-Gallegos",
      "J. L. Gonzalez-Compean",
      "Maxime Gonthier",
      "Valerie Hayot-Sasson",
      "J. Gregory Pauloski",
      "Haochen Pan",
      "Kyle Chard",
      "Jesus Carretero",
      "Ian Foster"
    ],
    "abstract": "Data distribution across different facilities offers benefits such as enhanced resource utilization, increased resilience through replication, and improved performance by processing data near its source. However, managing such data is challenging due to heterogeneous access protocols, disparate authentication models, and the lack of a unified coordination framework. This paper presents DynoStore, a system that manages data across heterogeneous storage systems. At the core of DynoStore are data containers, an abstraction that provides standardized interfaces for seamless data management, irrespective of the underlying storage systems. Multiple data container connections create a cohesive wide-area storage network, ensuring resilience using erasure coding policies. Furthermore, a load-balancing algorithm ensures equitable and efficient utilization of storage resources. We evaluate DynoStore using benchmarks and real-world case studies, including the management of medical and satellite data across geographically distributed environments. Our results demonstrate a 10\\% performance improvement compared to centralized cloud-hosted systems while maintaining competitive performance with state-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits superior fault tolerance, withstanding more failures than traditional systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DC"
    ],
    "url": "http://arxiv.org/abs/2507.00576v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00551v1",
    "title": "Whole body dynamic PET kernel reconstruction using nonnegative matrix factorization features",
    "authors": [
      "Alan Miranda",
      "Steven Staelens"
    ],
    "abstract": "The kernel reconstruction is a method that reduces noise in dynamic positron emission tomography (PET) by exploiting spatial correlations in the PET image. Although this method works well for large anatomical regions with relatively slow kinetics, whole body PET reconstruction with the kernel method can produce suboptimal results in regions with fast kinetics and high contrast. In this work we propose a new design of the spatial kernel matrix to improve reconstruction in fast and slow kinetics body regions. We calculate voxels features using nonnegative matrix factorization (NMF) with optimal rank selection. These features are then used to calculate similarities between voxels considering relative differences between features to adapt to a wide range of activity levels. Simulations and whole body mouse scans of high temporal resolution [18F]SynVesT-1, low dose [11C]raclopride, and [18F]Fallypride were performed to assess the performance of the method in different settings. In simulations, bias vs variance tradeoff and contrast was improved using the NMF kernel matrix, compared with the original kernel method. In real data, fast kinetic regions such as the heart, veins and kidneys presented oversmoothing or artifacts with the original kernel method. Our proposed method did not present these effects, while reducing noise. Brain kinetic modeling parametric maps with image derived input function ([18F]SynVesT-1) and with reference region ([11C]raclopride and [18F]Fallypride) also had lower standard error using the proposed kernel matrix compared with other methods. The NMF kernel reconstruction reduces noise and maintains high contrast in whole body PET imaging, outperforming the traditional kernel method.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.med-ph",
      "q-bio.BM"
    ],
    "url": "http://arxiv.org/abs/2507.00551v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00511v1",
    "title": "Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+",
    "authors": [
      "Sayandeep Kanrar",
      "Raja Piyush",
      "Qaiser Razi",
      "Debanshi Chakraborty",
      "Vikas Hassija",
      "GSS Chalapathi"
    ],
    "abstract": "In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two cutting-edge deep learning architectures designed to enhance medical image segmentation. Our approach integrates Squeeze-and-Excitation (SE) and Convolutional Block Attention Module (CBAM) techniques into the traditional VM U-Net framework, significantly improving segmentation accuracy, feature localization, and computational efficiency. Both models show superior performance compared to the baseline VM-Unet across multiple datasets. Notably, VMSEUnet achieves the highest accuracy, IoU, precision, and recall while maintaining low loss values. It also exhibits exceptional computational efficiency with faster inference times and lower memory usage on both GPU and CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a valuable tool for medical image analysis. These findings highlight its potential for real-world clinical applications, emphasizing the importance of further research to optimize accuracy, robustness, and computational efficiency.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.00511v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00449v1",
    "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention",
    "authors": [
      "Zhihao Zhan",
      "Jianan Zhao",
      "Zhaocheng Zhu",
      "Jian Tang"
    ],
    "abstract": "Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, \\emph{joint recall}, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA).",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CL",
      "I.2.7"
    ],
    "url": "http://arxiv.org/abs/2507.00449v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00377v1",
    "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis",
    "authors": [
      "Jianhao Xie",
      "Ziang Zhang",
      "Zhenyu Weng",
      "Yuesheng Zhu",
      "Guibo Luo"
    ],
    "abstract": "Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training data.While diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at https://github.com/JianhaoXie1/MedDiff-FT.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00377v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00316v2",
    "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation",
    "authors": [
      "Siyou Li",
      "Pengyao Qin",
      "Huanan Wu",
      "Dong Nie",
      "Arun J. Thirunavukarasu",
      "Juntao Yu",
      "Le Zhang"
    ],
    "abstract": "Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale $\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The novel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited data for RRG tasks. At the same time, for prompt engineering, we introduce a five-stage, LLM-driven pipeline that converts routine CT reports into paired visual-question-answer triples and citation-linked reasoning narratives, creating a scalable, high-quality supervisory corpus for explainable multimodal radiology LLM. All code, datasets, and models will be publicly available in our official repository. https://github.com/Siyou-Li/u2Tokenizer",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CL",
      "eess.IV"
    ],
    "url": "http://arxiv.org/abs/2507.00316v2"
  },
  {
    "id": "http://arxiv.org/abs/2507.00297v1",
    "title": "Natural language processing for African languages",
    "authors": [
      "David Ifeoluwa Adelani"
    ],
    "abstract": "Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.00297v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00234v1",
    "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations",
    "authors": [
      "Jiztom Kavalakkatt Francis",
      "Matthew J Darr"
    ],
    "abstract": "In this paper, we present a novel framework for enhancing model interpretability by integrating heatmaps produced separately by ResNet and a restructured 2D Transformer with globally weighted input saliency. We address the critical problem of spatial-temporal misalignment in existing interpretability methods, where convolutional networks fail to capture global context and Transformers lack localized precision - a limitation that impedes actionable insights in safety-critical domains like healthcare and industrial monitoring. Our method merges gradient-weighted activation maps (ResNet) and Transformer attention rollout into a unified visualization, achieving full spatial-temporal alignment while preserving real-time performance. Empirical evaluations on clinical (ECG arrhythmia detection) and industrial (energy consumption prediction) datasets demonstrate significant improvements: the hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy Appliance dataset-outperforming standalone ResNet, Transformer, and InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps into domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds suggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L (0.650) scores. By formalizing interpretability as causal fidelity and spatial-temporal alignment, our approach bridges the gap between technical outputs and stakeholder understanding, offering a scalable solution for transparent, time-aware decision-making.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.00234v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00231v1",
    "title": "Observation of Blood Flow in Major Neck Vessels Modulated 1 by Physiological Maneuvers",
    "authors": [
      "Gennadi Saiko",
      "Timothy Burton",
      "Faraz Sadrzadeh-Afsharazar",
      "Shota Yamashita",
      "Kenshin Shimono",
      "Yasuyuki Kakihana",
      "Alexandre Douplik"
    ],
    "abstract": "Large neck vessels (carotid artery and internal jugular vein, IJV) offer a unique opportunity to monitor hemodynamics non-invasively by optical means. The primary shortcoming of past work has been the focus on healthy volunteers in normal physiological conditions and well-controlled environments. To drive the technology closer to the bedside, testing is required under more re-alistic conditions, including in pathologies and real-world environments (e.g., similar toICU or emergency care settings). The primary goal of the current work was to extend the range of physiological maneuvers for blood flow modulation by introducing new maneuvers and ob-serving PPG response to them. The data from the necks of two healthy volunteers in a supine position were collected by clinical PPG and in-house built PPG sensors, accompanied by ECG signal collection. Seven maneuvers (abdominojugular test, breath holding, Valsalva, proximal occlusion of right IJV, distal occlusion of right IJV, proximal occlusion of left IJV, distal occlusion of left IJV) were performed in sequence with 1 min allocated for each maneuver. The 1 min was split into three segments: baseline (15 s), experiment (15 s), and recovery (30 s). Thus, the overall du-ration of the experiment was 7 min. AC amplitude from clinical PPG, DC amplitudes from in-house built PPG, and ECG signal were compared during all seven physiological maneuvers. Newly proposed maneuvers (Valsalva and IJV occlusions) demonstrated modulation of blood flow, which was more significant than previously reported maneuvers (abdominojugular test and breath holding). The proposed physiological maneuvers demonstrate high potential as instruments for modulating blood flow in major neck vessels.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.med-ph",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "url": "http://arxiv.org/abs/2507.00231v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00214v1",
    "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning",
    "authors": [
      "Mads Henrichsen",
      "Rasmus Krebs"
    ],
    "abstract": "Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q->A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.00214v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00206v1",
    "title": "Towards 3D Semantic Image Synthesis for Medical Imaging",
    "authors": [
      "Wenwu Tang",
      "Khaled Seyam",
      "Bin Yang"
    ],
    "abstract": "In the medical domain, acquiring large datasets is challenging due to both accessibility issues and stringent privacy regulations. Consequently, data availability and privacy protection are major obstacles to applying machine learning in medical imaging. To address this, our study proposes the Med-LSDM (Latent Semantic Diffusion Model), which operates directly in the 3D domain and leverages de-identified semantic maps to generate synthetic data as a method of privacy preservation and data augmentation. Unlike many existing methods that focus on generating 2D slices, Med-LSDM is designed specifically for 3D semantic image synthesis, making it well-suited for applications requiring full volumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D image generation process by applying a diffusion model within the latent space of a pre-trained VQ-GAN. By operating in the compressed latent space, the model significantly reduces computational complexity while still preserving critical 3D spatial details. Our approach demonstrates strong performance in 3D semantic medical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional Duke Breast dataset and similar Dice scores (0.70964) to those of real images (0.71496). These results demonstrate that the synthetic data from our model have a small domain gap with real data and are useful for data augmentation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00206v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00205v1",
    "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability",
    "authors": [
      "Periklis Petridis",
      "Georgios Margaritis",
      "Vasiliki Stoumpou",
      "Dimitris Bertsimas"
    ],
    "abstract": "With the increasing interest in deploying Artificial Intelligence in medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework that fuses multimodal data to solve downstream clinical tasks. However, HAIM uses data in a task-agnostic manner and lacks explainability. To address these limitations, we introduce xHAIM (Explainable HAIM), a novel framework leveraging Generative AI to enhance both prediction and explainability through four structured steps: (1) automatically identifying task-relevant patient data across modalities, (2) generating comprehensive patient summaries, (3) using these summaries for improved predictive modeling, and (4) providing clinical explanations by linking predictions to patient-specific medical knowledge. Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9% to 90.3% across chest pathology and operative tasks. Importantly, xHAIM transforms AI from a black-box predictor into an explainable decision support system, enabling clinicians to interactively trace predictions back to relevant patient data, bridging AI advancements with clinical utility.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.00205v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.00185v1",
    "title": "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)",
    "authors": [
      "Yang Zhou",
      "Chrystie Wan Ning Quek",
      "Jun Zhou",
      "Yan Wang",
      "Yang Bai",
      "Yuhe Ke",
      "Jie Yao",
      "Laura Gutierrez",
      "Zhen Ling Teo",
      "Darren Shu Jeng Ting",
      "Brian T. Soetikno",
      "Christopher S. Nielsen",
      "Tobias Elze",
      "Zengxiang Li",
      "Linh Le Dinh",
      "Lionel Tim-Ee Cheng",
      "Tran Nguyen Tuan Anh",
      "Chee Leong Cheng",
      "Tien Yin Wong",
      "Nan Liu",
      "Iain Beehuat Tan",
      "Tony Kiat Hon Lim",
      "Rick Siow Mong Goh",
      "Yong Liu",
      "Daniel Shu Wei Ting"
    ],
    "abstract": "Current artificial intelligence models for medical imaging are predominantly single modality and single disease. Attempts to create multimodal and multi-disease models have resulted in inconsistent clinical accuracy. Furthermore, training these models typically requires large, labour-intensive, well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal, multi-specialty foundation model trained using self-supervised learning and a memory module. MerMED-FM was trained on 3.3 million medical images from over ten specialties and seven modalities, including computed tomography (CT), chest X-rays (CXR), ultrasound (US), pathology patches, color fundus photography (CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was evaluated across multiple diseases and compared against existing foundational models. Strong performance was achieved across all modalities, with AUROCs of 0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894 (CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable, versatile, cross-specialty foundation model that enables robust medical imaging interpretation across diverse medical disciplines.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.00185v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.24062v1",
    "title": "Scout-Dose-TCM: Direct and Prospective Scout-Based Estimation of Personalized Organ Doses from Tube Current Modulated CT Exams",
    "authors": [
      "Maria Jose Medrano",
      "Sen Wang",
      "Liyan Sun",
      "Abdullah-Al-Zubaer Imran",
      "Jennie Cao",
      "Grant Stevens",
      "Justin Ruey Tse",
      "Adam S. Wang"
    ],
    "abstract": "This study proposes Scout-Dose-TCM for direct, prospective estimation of organ-level doses under tube current modulation (TCM) and compares its performance to two established methods. We analyzed contrast-enhanced chest-abdomen-pelvis CT scans from 130 adults (120 kVp, TCM). Reference doses for six organs (lungs, kidneys, liver, pancreas, bladder, spleen) were calculated using MC-GPU and TotalSegmentator. Based on these, we trained Scout-Dose-TCM, a deep learning model that predicts organ doses corresponding to discrete cosine transform (DCT) basis functions, enabling real-time estimates for any TCM profile. The model combines a feature learning module that extracts contextual information from lateral and frontal scouts and scan range with a dose learning module that output DCT-based dose estimates. A customized loss function incorporated the DCT formulation during training. For comparison, we implemented size-specific dose estimation per AAPM TG 204 (Global CTDIvol) and its organ-level TCM-adapted version (Organ CTDIvol). A 5-fold cross-validation assessed generalizability by comparing mean absolute percentage dose errors and r-squared correlations with benchmark doses. Average absolute percentage errors were 13% (Global CTDIvol), 9% (Organ CTDIvol), and 7% (Scout-Dose-TCM), with bladder showing the largest discrepancies (15%, 13%, and 9%). Statistical tests confirmed Scout-Dose-TCM significantly reduced errors vs. Global CTDIvol across most organs and improved over Organ CTDIvol for the liver, bladder, and pancreas. It also achieved higher r-squared values, indicating stronger agreement with Monte Carlo benchmarks. Scout-Dose-TCM outperformed Global CTDIvol and was comparable to or better than Organ CTDIvol, without requiring organ segmentations at inference, demonstrating its promise as a tool for prospective organ-level dose estimation in CT.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.med-ph"
    ],
    "url": "http://arxiv.org/abs/2506.24062v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.24046v1",
    "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy",
    "authors": [
      "Olivia Richards",
      "Keith L. Obstein",
      "Nabil Simaan"
    ],
    "abstract": "New endoscopists require a large volume of expert-proctored colonoscopies to attain minimal competency. Developing multi-fingered, synchronized control of a colonoscope requires significant time and exposure to the device. Current training methods inhibit this development by relying on tool hand-off for expert demonstrations. There is a need for colonoscopy training tools that enable in-hand expert guidance in real-time. We present a new concept of a tandem training system that uses a telemanipulated preceptor colonoscope to guide novice users as they perform a colonoscopy. This system is capable of dual-control and can automatically toggle between expert and novice control of a standard colonoscope's angulation control wheels. Preliminary results from a user study with novice and expert users show the effectiveness of this device as a skill acquisition tool. We believe that this device has the potential to accelerate skill acquisition for colonoscopy and, in the future, enable individualized instruction and responsive teaching through bidirectional actuation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO",
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2506.24046v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.24034v1",
    "title": "Supervised Diffusion-Model-Based PET Image Reconstruction",
    "authors": [
      "George Webber",
      "Alexander Hammers",
      "Andrew P King",
      "Andrew J Reader"
    ],
    "abstract": "Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PET's Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [$^{18}$F]FDG brain PET data.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.med-ph",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.24034v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.24003v1",
    "title": "ShapeKit",
    "authors": [
      "Junqi Liu",
      "Dongli He",
      "Wenxuan Li",
      "Ningyu Wang",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ],
    "abstract": "In this paper, we present a practical approach to improve anatomical shape accuracy in whole-body medical segmentation. Our analysis shows that a shape-focused toolkit can enhance segmentation performance by over 8%, without the need for model re-training or fine-tuning. In comparison, modifications to model architecture typically lead to marginal gains of less than 3%. Motivated by this observation, we introduce ShapeKit, a flexible and easy-to-integrate toolkit designed to refine anatomical shapes. This work highlights the underappreciated value of shape-based tools and calls attention to their potential impact within the medical segmentation community.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.24003v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23958v1",
    "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages",
    "authors": [
      "Ikechukwu Ogbonna",
      "Lesley Davidson",
      "Soumya Banerjee",
      "Abhishek Dasgupta",
      "Laurence Kenney",
      "Vikranth Harthikote Nagaraja"
    ],
    "abstract": "Millions of people in African countries face barriers to accessing healthcare due to language and literacy gaps. This research tackles this challenge by transforming complex medical documents -- in this case, prosthetic device user manuals -- into accessible formats for underserved populations. This case study in cross-cultural translation is particularly pertinent/relevant for communities that receive donated prosthetic devices but may not receive the accompanying user documentation. Or, if available online, may only be available in formats (e.g., language and readability) that are inaccessible to local populations (e.g., English-language, high resource settings/cultural context). The approach is demonstrated using the widely spoken Pidgin dialect, but our open-source framework has been designed to enable rapid and easy extension to other languages/dialects. This work presents an AI-powered framework designed to process and translate complex medical documents, e.g., user manuals for prosthetic devices, into marginalised languages. The system enables users -- such as healthcare workers or patients -- to upload English-language medical equipment manuals, pose questions in their native language, and receive accurate, localised answers in real time. Technically, the system integrates a Retrieval-Augmented Generation (RAG) pipeline for processing and semantic understanding of the uploaded manuals. It then employs advanced Natural Language Processing (NLP) models for generative question-answering and multilingual translation. Beyond simple translation, it ensures accessibility to device instructions, treatment protocols, and safety information, empowering patients and clinicians to make informed healthcare decisions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2506.23958v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23952v2",
    "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support",
    "authors": [
      "Stefan Buijsman",
      "Sarah Carter",
      "Juan Pablo Bermúdez"
    ],
    "abstract": "AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "url": "http://arxiv.org/abs/2506.23952v2"
  },
  {
    "id": "http://arxiv.org/abs/2506.23810v1",
    "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP",
    "authors": [
      "Mahshid Shiri",
      "Cigdem Beyan",
      "Vittorio Murino"
    ],
    "abstract": "An innovative few-shot anomaly detection approach is presented, leveraging the pre-trained CLIP model for medical data, and adapting it for both image-level anomaly classification (AC) and pixel-level anomaly segmentation (AS). A dual-branch design is proposed to separately capture normal and abnormal features through learnable adapters in the CLIP vision encoder. To improve semantic alignment, learnable text prompts are employed to link visual features. Furthermore, SigLIP loss is applied to effectively handle the many-to-one relationship between images and unpaired text prompts, showcasing its adaptation in the medical field for the first time. Our approach is validated on multiple modalities, demonstrating superior performance over existing methods for AC and AS, in both same-dataset and cross-dataset evaluations. Unlike prior work, it does not rely on synthetic data or memory banks, and an ablation study confirms the contribution of each component. The code is available at https://github.com/mahshid1998/MadCLIP.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23810v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23705v1",
    "title": "Single Image Test-Time Adaptation via Multi-View Co-Training",
    "authors": [
      "Smriti Joshi",
      "Richard Osuala",
      "Lidia Garrucho",
      "Kaisar Kushibar",
      "Dimitri Kessler",
      "Oliver Diaz",
      "Karim Lekadir"
    ],
    "abstract": "Test-time adaptation enables a trained model to adjust to a new domain during inference, making it particularly valuable in clinical settings where such on-the-fly adaptation is required. However, existing techniques depend on large target domain datasets, which are often impractical and unavailable in medical scenarios that demand per-patient, real-time inference. Moreover, current methods commonly focus on two-dimensional images, failing to leverage the volumetric richness of medical imaging data. Bridging this gap, we propose a Patch-Based Multi-View Co-Training method for Single Image Test-Time adaptation. Our method enforces feature and prediction consistency through uncertainty-guided self-training, enabling effective volumetric segmentation in the target domain with only a single test-time image. Validated on three publicly available breast magnetic resonance imaging datasets for tumor segmentation, our method achieves performance close to the upper bound supervised benchmark while also outperforming all existing state-of-the-art methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly share our accessible codebase, readily integrable with the popular nnUNet framework, at https://github.com/smriti-joshi/muvi.git.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23705v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23701v1",
    "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction",
    "authors": [
      "Lingtong Zhang",
      "Mengdie Song",
      "Xiaohan Hao",
      "Huayu Mai",
      "Bensheng Qiu"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical diagnostics. As the latest generative models, diffusion models (DMs) have struggled to produce high-fidelity images due to their stochastic nature in image domains. Latent diffusion models (LDMs) yield both compact and detailed prior knowledge in latent domains, which could effectively guide the model towards more effective learning of the original data distribution. Inspired by this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by pre-trained LDMs to enhance data consistency in MRI reconstruction tasks. Specifically, we first construct a Visual-Mamba-based backbone, which enables efficient encoding and reconstruction of under-sampled images. Then pre-trained LDMs are integrated to provide conditional priors in both latent and image domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion in multi-level latent domains. Simultaneously, to effectively utilize a prior in both the k-space and image domain, under-sampled images are fused with generated full-sampled images by the Dual-domain Fusion Branch (DFB) for self-adaption guidance. Lastly, to further enhance the data consistency, we propose a k-space regularization strategy based on the non-auto-calibration signal (NACS) set. Extensive experiments on two public MRI datasets fully demonstrate the effectiveness of the proposed methodology. The code is available at https://github.com/Zolento/MDPG.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23701v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23700v1",
    "title": "MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation",
    "authors": [
      "Peiting Tian",
      "Xi Chen",
      "Haixia Bi",
      "Fan Li"
    ],
    "abstract": "Medical image segmentation plays a crucial role in clinical diagnosis and treatment planning, where accurate boundary delineation is essential for precise lesion localization, organ identification, and quantitative assessment. In recent years, deep learning-based methods have significantly advanced segmentation accuracy. However, two major challenges remain. First, the performance of these methods heavily relies on large-scale annotated datasets, which are often difficult to obtain in medical scenarios due to privacy concerns and high annotation costs. Second, clinically challenging scenarios, such as low contrast in certain imaging modalities and blurry lesion boundaries caused by malignancy, still pose obstacles to precise segmentation. To address these challenges, we propose MedSAM-CA, an architecture-level fine-tuning approach that mitigates reliance on extensive manual annotations by adapting the pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA introduces two key components: the Convolutional Attention-Enhanced Boundary Refinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block (Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover boundary information potentially overlooked by long-range attention mechanisms, leveraging hierarchical convolutional processing. Atte-FFB, embedded in the MedSAM decoder, fuses multi-level fine-grained features from skip connections in CBR-Net with global representations upsampled within the decoder to enhance boundary delineation accuracy. Experiments on publicly available datasets covering dermoscopy, CT, and MRI imaging modalities validate the effectiveness of MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only 2% of full training data, reaching 97.25% of full-data training performance, demonstrating strong effectiveness in low-resource clinical settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23700v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23688v1",
    "title": "GUSL: A Novel and Efficient Machine Learning Model for Prostate Segmentation on MRI",
    "authors": [
      "Jiaxin Yang",
      "Vasileios Magoulianitis",
      "Catherine Aurelia Christie Alexander",
      "Jintang Xue",
      "Masatomo Kaneko",
      "Giovanni Cacciamani",
      "Andre Abreu",
      "Vinay Duddalwar",
      "C. -C. Jay Kuo",
      "Inderbir S. Gill",
      "Chrysostomos Nikias"
    ],
    "abstract": "Prostate and zonal segmentation is a crucial step for clinical diagnosis of prostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation are based on the deep learning (DL) paradigm. However, deep neural networks are perceived as \"black-box\" solutions by physicians, thus making them less practical for deployment in the clinical setting. In this paper, we introduce a feed-forward machine learning model, named Green U-shaped Learning (GUSL), suitable for medical image segmentation without backpropagation. GUSL introduces a multi-layer regression scheme for coarse-to-fine segmentation. Its feature extraction is based on a linear model, which enables seamless interpretability during feature extraction. Also, GUSL introduces a mechanism for attention on the prostate boundaries, which is an error-prone region, by employing regression to refine the predictions through residue correction. In addition, a two-step pipeline approach is used to mitigate the class imbalance, an issue inherent in medical imaging problems. After conducting experiments on two publicly available datasets and one private dataset, in both prostate gland and zonal segmentation tasks, GUSL achieves state-of-the-art performance among other DL-based models. Notably, GUSL features a very energy-efficient pipeline, since it has a model size several times smaller and less complexity than the rest of the solutions. In all datasets, GUSL achieved a Dice Similarity Coefficient (DSC) performance greater than $0.9$ for gland segmentation. Considering also its lightweight model size and transparency in feature extraction, it offers a competitive and practical package for medical imaging applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV"
    ],
    "url": "http://arxiv.org/abs/2506.23688v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23664v1",
    "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation",
    "authors": [
      "Fangyijie Wang",
      "Kevin Whelan",
      "Félix Balado",
      "Guénolé Silvestre",
      "Kathleen M. Curran"
    ],
    "abstract": "Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and 94.38\\% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23664v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23641v1",
    "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation",
    "authors": [
      "Peng Huang",
      "Junhu Fu",
      "Bowen Guo",
      "Zeju Li",
      "Yuanyuan Wang",
      "Yi Guo"
    ],
    "abstract": "As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chain-of-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2506.23641v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23627v1",
    "title": "Brain Tumor Detection through Thermal Imaging and MobileNET",
    "authors": [
      "Roham Maiti",
      "Debasmita Bhoumik"
    ],
    "abstract": "Brain plays a crucial role in regulating body functions and cognitive processes, with brain tumors posing significant risks to human health. Precise and prompt detection is a key factor in proper treatment and better patient outcomes. Traditional methods for detecting brain tumors, that include biopsies, MRI, and CT scans often face challenges due to their high costs and the need for specialized medical expertise. Recent developments in machine learning (ML) and deep learning (DL) has exhibited strong capabilities in automating the identification and categorization of brain tumors from medical images, especially MRI scans. However, these classical ML models have limitations, such as high computational demands, the need for large datasets, and long training times, which hinder their accessibility and efficiency. Our research uses MobileNET model for efficient detection of these tumors. The novelty of this project lies in building an accurate tumor detection model which use less computing re-sources and runs in less time followed by efficient decision making through the use of image processing technique for accurate results. The suggested method attained an average accuracy of 98.5%.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2506.23627v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23584v1",
    "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation",
    "authors": [
      "Renjie Liang",
      "Zhengkang Fan",
      "Jinqian Pan",
      "Chenkun Sun",
      "Russell Terry",
      "Jie Xu"
    ],
    "abstract": "Generating radiology reports from CT scans remains a complex task due to the nuanced nature of medical imaging and the variability in clinical documentation. In this study, we propose a two-stage framework for generating renal radiology reports from 2D CT slices. First, we extract structured abnormality features using a multi-task learning model trained to identify lesion attributes such as location, size, enhancement, and attenuation. These extracted features are subsequently combined with the corresponding CT image and fed into a fine-tuned vision-language model to generate natural language report sentences aligned with clinical findings. We conduct experiments on a curated dataset of renal CT studies with manually annotated sentence-slice-feature triplets and evaluate performance using both classification metrics and natural language generation metrics. Our results demonstrate that the proposed model outperforms random baselines across all abnormality types, and the generated reports capture key clinical content with reasonable textual accuracy. This exploratory work highlights the feasibility of modular, feature-informed report generation for renal imaging. Future efforts will focus on extending this pipeline to 3D CT volumes and further improving clinical fidelity in multimodal medical AI systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23584v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23545v1",
    "title": "Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research",
    "authors": [
      "Barbara Karpowicz",
      "Maciej Grzeszczuk",
      "Adam Kuzdraliński",
      "Monika Kornacka",
      "Aliaksandr Marozau",
      "Wiktor Stawski",
      "Pavlo Zinevych",
      "Grzegorz Marcin Wójcik",
      "Tomasz Kowalewski",
      "Grzegorz Pochwatko",
      "Wiesław Kopeć"
    ],
    "abstract": "Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.CE"
    ],
    "url": "http://arxiv.org/abs/2506.23545v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23506v1",
    "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI",
    "authors": [
      "Bowen Xin",
      "Rohan Hickey",
      "Tamara Blake",
      "Jin Jin",
      "Claire E Wainwright",
      "Thomas Benkert",
      "Alto Stemmer",
      "Peter Sly",
      "David Coman",
      "Jason Dowling"
    ],
    "abstract": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE) represents a recent breakthrough in lung structure imaging, providing image resolution and quality comparable to computed tomography (CT). Due to the absence of ionising radiation, MRI is often preferred over CT in paediatric diseases such as cystic fibrosis (CF), one of the most common genetic disorders in Caucasians. To assess structural lung damage in CF imaging, CT scoring systems provide valuable quantitative insights for disease diagnosis and progression. However, few quantitative scoring systems are available in structural lung MRI (e.g., UTE-MRI). To provide fast and accurate quantification in lung MRI, we investigated the feasibility of novel Artificial intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3) lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification and reporting. The results shows that our APL scoring took 8.2 minutes per subject, which was more than twice as fast as the previous grid-level scoring. Additionally, our pixel-level scoring was statistically more accurate (p=0.021), while strongly correlating with grid-level scoring (R=0.973, p=5.85e-9). This tool has great potential to streamline the workflow of UTE lung MRI in clinical settings, and be extended to other structural lung MRI sequences (e.g., BLADE MRI), and for other lung diseases (e.g., bronchopulmonary dysplasia).",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "physics.med-ph"
    ],
    "url": "http://arxiv.org/abs/2506.23506v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23467v1",
    "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays",
    "authors": [
      "Chenlang Yi",
      "Zizhan Xiong",
      "Qi Qi",
      "Xiyuan Wei",
      "Girish Bathla",
      "Ching-Long Lin",
      "Bobak Jack Mortazavi",
      "Tianbao Yang"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2506.23467v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23466v1",
    "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction",
    "authors": [
      "Qiqing Liu",
      "Guoquan Wei",
      "Zekun Zhou",
      "Yiyang Wen",
      "Liu Shi",
      "Qiegen Liu"
    ],
    "abstract": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "url": "http://arxiv.org/abs/2506.23466v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23460v1",
    "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation",
    "authors": [
      "Dewen Zeng",
      "Xinrong Hu",
      "Yu-Jen Chen",
      "Yawen Wu",
      "Xiaowei Xu",
      "Yiyu Shi"
    ],
    "abstract": "Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23460v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23395v1",
    "title": "FastSet: Parallel Claim Settlement",
    "authors": [
      "Xiaohong Chen",
      "Grigore Rosu"
    ],
    "abstract": "FastSet is an actor-based distributed protocol for decentralized finance and settlement, which is inspired from blockchains. Account holders cooperate by making claims, which can include payments, holding and transferring assets, accessing and updating shared data, medical records, digital identity, and mathematical theorems, among many others. The claims are signed by their owners and are broadcast to a decentralized network of validators, which validate and settle them. Validators replicate the global state of the accounts and need not communicate with each other. In sharp contrast to blockchains, strong consistency is purposely given up as a requirement. Yet, many if not most of the blockchain benefits are preserved. The protocol is proved to be correct, despite its massively parallel nature.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DC"
    ],
    "url": "http://arxiv.org/abs/2506.23395v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23334v1",
    "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation",
    "authors": [
      "Hongyi Pan",
      "Ziliang Hong",
      "Gorkem Durak",
      "Ziyue Xu",
      "Ulas Bagci"
    ],
    "abstract": "Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23334v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23315v1",
    "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)",
    "authors": [
      "Shouvon Sarker",
      "Xishuang Dong",
      "Lijun Qian"
    ],
    "abstract": "Identification of key variables such as medications, diseases, relations from health records and clinical notes has a wide range of applications in the clinical domain. n2c2 2022 provided shared tasks on challenges in natural language processing for clinical data analytics on electronic health records (EHR), where it built a comprehensive annotated clinical data Contextualized Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of this challenge that is to detect and classify medication events from clinical notes through building a novel BERT-based ensemble model. It started with pretraining BERT models on different types of big data such as Wikipedia and MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED training data. These fine-tuned BERT models were employed to accomplish medication event classification on CMED testing data with multiple predictions. These multiple predictions generated by these fine-tuned BERT models were integrated to build final prediction with voting strategies. Experimental results demonstrated that BERT-based ensemble models can effectively improve strict Micro-F score by about 5% and strict Macro-F score by about 6%, respectively.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2506.23315v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23311v1",
    "title": "Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction",
    "authors": [
      "Perla Mayo",
      "Carolin M. Pirkl",
      "Alin Achim",
      "Bjoern Menze",
      "Mohammad Golbabaee"
    ],
    "abstract": "We introduce MRF-DiPh, a novel physics informed denoising diffusion approach for multiparametric tissue mapping from highly accelerated, transient-state quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our method is derived from a proximal splitting formulation, incorporating a pretrained denoising diffusion model as an effective image prior to regularize the MRF inverse problem. Further, during reconstruction it simultaneously enforces two key physical constraints: (1) k-space measurement consistency and (2) adherence to the Bloch response model. Numerical experiments on in-vivo brain scans data show that MRF-DiPh outperforms deep learning and compressed sensing MRF baselines, providing more accurate parameter maps while better preserving measurement fidelity and physical model consistency-critical for solving reliably inverse problems in medical imaging.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.LG",
      "physics.med-ph"
    ],
    "url": "http://arxiv.org/abs/2506.23311v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23298v2",
    "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification",
    "authors": [
      "Xing Shen",
      "Justin Szeto",
      "Mingyang Li",
      "Hengguan Huang",
      "Tal Arbel"
    ],
    "abstract": "Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs' predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV"
    ],
    "url": "http://arxiv.org/abs/2506.23298v2"
  },
  {
    "id": "http://arxiv.org/abs/2506.23208v1",
    "title": "Multi-Source COVID-19 Detection via Variance Risk Extrapolation",
    "authors": [
      "Runtian Yuan",
      "Qingqiu Li",
      "Junlin Hou",
      "Jilan Xu",
      "Yuejie Zhang",
      "Rui Feng",
      "Hao Chen"
    ],
    "abstract": "We present our solution for the Multi-Source COVID-19 Detection Challenge, which aims to classify chest CT scans into COVID and Non-COVID categories across data collected from four distinct hospitals and medical centers. A major challenge in this task lies in the domain shift caused by variations in imaging protocols, scanners, and patient populations across institutions. To enhance the cross-domain generalization of our model, we incorporate Variance Risk Extrapolation (VREx) into the training process. VREx encourages the model to maintain consistent performance across multiple source domains by explicitly minimizing the variance of empirical risks across environments. This regularization strategy reduces overfitting to center-specific features and promotes learning of domain-invariant representations. We further apply Mixup data augmentation to improve generalization and robustness. Mixup interpolates both the inputs and labels of randomly selected pairs of training samples, encouraging the model to behave linearly between examples and enhancing its resilience to noise and limited data. Our method achieves an average macro F1 score of 0.96 across the four sources on the validation set, demonstrating strong generalization.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23208v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.01991v1",
    "title": "FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports",
    "authors": [
      "Muhammad Bilal Zafar"
    ],
    "abstract": "The proliferation of artificial intelligence (AI) in financial services has prompted growing demand for tools that can systematically detect AI-related disclosures in corporate filings. While prior approaches often rely on keyword expansion or document-level classification, they fall short in granularity, interpretability, and robustness. This study introduces FinAI-BERT, a domain-adapted transformer-based language model designed to classify AI-related content at the sentence level within financial texts. The model was fine-tuned on a manually curated and balanced dataset of 1,586 sentences drawn from 669 annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect classification performance (accuracy of 99.37 percent, F1 score of 0.993), outperforming traditional baselines such as Logistic Regression, Naive Bayes, Random Forest, and XGBoost. Interpretability was ensured through SHAP-based token attribution, while bias analysis and robustness checks confirmed the model's stability across sentence lengths, adversarial inputs, and temporal samples. Theoretically, the study advances financial NLP by operationalizing fine-grained, theme-specific classification using transformer architectures. Practically, it offers a scalable, transparent solution for analysts, regulators, and scholars seeking to monitor the diffusion and framing of AI across financial institutions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "q-fin.CP",
      "cs.CL",
      "econ.GN",
      "q-fin.EC",
      "q-fin.GN"
    ],
    "url": "http://arxiv.org/abs/2507.01991v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23145v1",
    "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings",
    "authors": [
      "Shahad Hardan",
      "Darya Taratynova",
      "Abdelmajid Essofi",
      "Karthik Nandakumar",
      "Mohammad Yaqub"
    ],
    "abstract": "Privacy preservation in AI is crucial, especially in healthcare, where models rely on sensitive patient data. In the emerging field of machine unlearning, existing methodologies struggle to remove patient data from trained multimodal architectures, which are widely used in healthcare. We propose Forget-MI, a novel machine unlearning method for multimodal medical data, by establishing loss functions and perturbation techniques. Our approach unlearns unimodal and joint representations of the data requested to be forgotten while preserving knowledge from the remaining data and maintaining comparable performance to the original model. We evaluate our results using performance on the forget dataset, performance on the test dataset, and Membership Inference Attack (MIA), which measures the attacker's ability to distinguish the forget dataset from the training dataset. Our model outperforms the existing approaches that aim to reduce MIA and the performance on the forget dataset while keeping an equivalent performance on the test set. Specifically, our approach reduces MIA by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305, respectively. Additionally, our performance on the test set matches that of the retrained model, while allowing forgetting. Code is available at https://github.com/BioMedIA-MBZUAI/Forget-MI.git",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23145v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23121v1",
    "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation",
    "authors": [
      "Xinlei Yu",
      "Chanmiao Wang",
      "Hui Jin",
      "Ahmed Elazab",
      "Gangyong Jia",
      "Xiang Wan",
      "Changqing Zou",
      "Ruiquan Ge"
    ],
    "abstract": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\\_SAM2.git.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2506.23121v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23075v1",
    "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding",
    "authors": [
      "Yuchen Zhou",
      "Jiamin Wu",
      "Zichen Ren",
      "Zhouheng Yao",
      "Weiheng Lu",
      "Kunyu Peng",
      "Qihao Zheng",
      "Chunfeng Song",
      "Wanli Ouyang",
      "Chao Gou"
    ],
    "abstract": "Understanding and decoding brain activity from electroencephalography (EEG) signals is a fundamental challenge in neuroscience and AI, with applications in cognition, emotion recognition, diagnosis, and brain-computer interfaces. While recent EEG foundation models advance generalized decoding via unified architectures and large-scale pretraining, they adopt a scale-agnostic dense modeling paradigm inherited from NLP and vision. This design neglects a core property of neural activity: cross-scale spatiotemporal structure. EEG task patterns span a wide range of temporal and spatial scales, from short bursts to slow rhythms, and from localized cortical responses to distributed interactions. Ignoring this diversity leads to suboptimal representations and weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features from localized temporal windows and anatomical brain regions into compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which captures cross-window and cross-region dependencies, enhancing scale diversity while removing spurious correlations. CST and SSA are alternately stacked to progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks across 16 datasets show that CSBrain consistently outperforms task-specific and foundation model baselines. These results establish cross-scale modeling as a key inductive bias and position CSBrain as a robust backbone for future brain-AI research.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.LG",
      "eess.SP",
      "q-bio.NC"
    ],
    "url": "http://arxiv.org/abs/2506.23075v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23051v1",
    "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition",
    "authors": [
      "João Lucas Luz Lima Sarcinelli",
      "Marina Lages Gonçalves Teixeira",
      "Jade Bortot de Paiva",
      "Diego Furtado Silva"
    ],
    "abstract": "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task that aims to identify and classify entity mentions in texts across different categories. While languages such as English possess a large number of high-quality resources for this task, Brazilian Portuguese still lacks in quantity of gold-standard NER datasets, especially when considering specific domains. Particularly, this paper considers the importance of NER for analyzing historical texts in the context of digital humanities. To address this gap, this work outlines the construction of MariNER: \\textit{Mapeamento e Anota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of Historical Records for NER), the first gold-standard dataset for early 20th-century Brazilian Portuguese, with more than 9,000 manually annotated sentences. We also assess and compare the performance of state-of-the-art NER models for the dataset.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2506.23051v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23040v1",
    "title": "Treatment, evidence, imitation, and chat",
    "authors": [
      "Samuel J. Weisenthal"
    ],
    "abstract": "Large language models are thought to have potential to aid in medical decision making. We investigate this here. We start with the treatment problem, the patient's core medical decision-making task, which is solved in collaboration with a healthcare provider. We discuss approaches to solving the treatment problem, including -- within evidence-based medicine -- trials and observational data. We then discuss the chat problem, and how this differs from the treatment problem -- in particular as it relates to imitation. We then discuss how a large language model might be used to solve the treatment problem and highlight some of the challenges that emerge. We finally discuss how these challenges relate to evidence-based medicine, and how this might inform next steps.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.OT",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2506.23040v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.23038v1",
    "title": "Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation",
    "authors": [
      "Xinrong Hu",
      "Yiyu Shi"
    ],
    "abstract": "Collecting pixel-level labels for medical datasets can be a laborious and expensive process, and enhancing segmentation performance with a scarcity of labeled data is a crucial challenge. This work introduces AugPaint, a data augmentation framework that utilizes inpainting to generate image-label pairs from limited labeled data. AugPaint leverages latent diffusion models, known for their ability to generate high-quality in-domain images with low overhead, and adapts the sampling process for the inpainting task without need for retraining. Specifically, given a pair of image and label mask, we crop the area labeled with the foreground and condition on it during reversed denoising process for every noise level. Masked background area would gradually be filled in, and all generated images are paired with the label mask. This approach ensures the accuracy of match between synthetic images and label masks, setting it apart from existing dataset generation methods. The generated images serve as valuable supervision for training downstream segmentation models, effectively addressing the challenge of limited annotations. We conducted extensive evaluations of our data augmentation method on four public medical image segmentation datasets, including CT, MRI, and skin imaging. Results across all datasets demonstrate that AugPaint outperforms state-of-the-art label-efficient methodologies, significantly improving segmentation performance.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.23038v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.22972v1",
    "title": "Adaptable Non-parametric Approach for Speech-based Symptom Assessment: Isolating Private Medical Data in a Retrieval Datastore",
    "authors": [
      "Yu-Wen Chen",
      "Julia Hirschberg"
    ],
    "abstract": "The automatic assessment of health-related acoustic cues has the potential to improve healthcare accessibility and affordability. Although parametric models are promising, they face challenges in privacy and adaptability. To address these, we propose a NoN-Parametric framework for Speech-based symptom Assessment (NoNPSA). By isolating medical data in a retrieval datastore, NoNPSA avoids encoding private information in model parameters and enables efficient data updates. A self-supervised learning (SSL) model pre-trained on general-purpose datasets extracts features, which are used for similarity-based retrieval. Metadata-aware refinement filters the retrieved data, and associated labels are used to compute an assessment score. Experimental results show that NoNPSA achieves competitive performance compared to fine-tuning SSL-based methods, while enabling greater privacy, update efficiency, and adaptability--showcasing the potential of non-parametric approaches in healthcare.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.AS"
    ],
    "url": "http://arxiv.org/abs/2506.22972v1"
  },
  {
    "id": "http://arxiv.org/abs/2506.22955v1",
    "title": "YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging",
    "authors": [
      "Haniyeh Nikkhah",
      "Jafar Tanha",
      "Mahdi Zarrin",
      "SeyedEhsan Roshan",
      "Amin Kazempour"
    ],
    "abstract": "Medical image segmentation poses significant challenges due to class imbalance and the complex structure of medical images. To address these challenges, this study proposes YM-WML, a novel model for cardiac image segmentation. The model integrates a robust backbone for effective feature extraction, a YOLOv11 neck for multi-scale feature aggregation, and an attention-based segmentation head for precise and accurate segmentation. To address class imbalance, we introduce the Weighted Multi-class Exponential (WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity Coefficient of 91.02, outperforming state-of-the-art methods. The model demonstrates stable training, accurate segmentation, and strong generalization, setting a new benchmark in cardiac segmentation tasks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2506.22955v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02861v1",
    "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans",
    "authors": [
      "Zhening Huang",
      "Xiaoyang Wu",
      "Fangcheng Zhong",
      "Hengshuang Zhao",
      "Matthias Nießner",
      "Joan Lasenby"
    ],
    "abstract": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "url": "http://arxiv.org/abs/2507.02861v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02860v1",
    "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching",
    "authors": [
      "Xin Zhou",
      "Dingkang Liang",
      "Kaijin Chen",
      "Tianrui Feng",
      "Xiwu Chen",
      "Hongkai Lin",
      "Yikang Ding",
      "Feiyang Tan",
      "Hengshuang Zhao",
      "Xiang Bai"
    ],
    "abstract": "Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02860v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02859v1",
    "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation",
    "authors": [
      "Jiaer Xia",
      "Bingkui Tong",
      "Yuhang Zang",
      "Rui Shao",
      "Kaiyang Zhou"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02859v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02858v1",
    "title": "Requirements Elicitation Follow-Up Question Generation",
    "authors": [
      "Yuchen Shen",
      "Anmol Singhal",
      "Travis Breaux"
    ],
    "abstract": "Interviews are a widely used technique in eliciting requirements to gather stakeholder needs, preferences, and expectations for a software system. Effective interviewing requires skilled interviewers to formulate appropriate interview questions in real time while facing multiple challenges, including lack of familiarity with the domain, excessive cognitive load, and information overload that hinders how humans process stakeholders' speech. Recently, large language models (LLMs) have exhibited state-of-the-art performance in multiple natural language processing tasks, including text summarization and entailment. To support interviewers, we investigate the application of GPT-4o to generate follow-up interview questions during requirements elicitation by building on a framework of common interviewer mistake types. In addition, we describe methods to generate questions based on interviewee speech. We report a controlled experiment to evaluate LLM-generated and human-authored questions with minimal guidance, and a second controlled experiment to evaluate the LLM-generated questions when generation is guided by interviewer mistake types. Our findings demonstrate that, for both experiments, the LLM-generated questions are no worse than the human-authored questions with respect to clarity, relevancy, and informativeness. In addition, LLM-generated questions outperform human-authored questions when guided by common mistakes types. This highlights the potential of using LLMs to help interviewers improve the quality and ease of requirements elicitation interviews in real time.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE",
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02858v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02857v1",
    "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
    "authors": [
      "Ziye Li",
      "Hao Luo",
      "Xincheng Shuai",
      "Henghui Ding"
    ],
    "abstract": "Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02857v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02855v1",
    "title": "Subtyping in DHOL -- Extended preprint",
    "authors": [
      "Colin Rothgang",
      "Florian Rabe"
    ],
    "abstract": "The recently introduced dependent typed higher-order logic (DHOL) offers an interesting compromise between expressiveness and automation support. It sacrifices the decidability of its type system in order to significantly extend its expressiveness over standard HOL. Yet it retains strong automated theorem proving support via a sound and complete translation to HOL.   We leverage this design to extend DHOL with refinement and quotient types. Both of these are commonly requested by practitioners but rarely provided by automated theorem provers. This is because they inherently require undecidable typing and thus are very difficult to retrofit to decidable type systems. But with DHOL already doing the heavy lifting, adding them is not only possible but elegant and simple.   Concretely, we add refinement and quotient types as special cases of subtyping. This turns the associated canonical inclusion resp. projection maps into identity maps and thus avoids costly changes in representation. We present the syntax, semantics, and translation to HOL for the extended language, including the proofs of soundness and completeness.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LO",
      "cs.AI",
      "cs.FL"
    ],
    "url": "http://arxiv.org/abs/2507.02855v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02854v1",
    "title": "Diffeomorphic approximation of piecewise affine homeomorphisms",
    "authors": [
      "Daniel Campbell",
      "Luigi D'Onofrio",
      "Tomáš Vítek"
    ],
    "abstract": "Given any $f$ a locally finitely piecewise affine homeomorphism of $\\Omega \\subset \\mathbb{R}^d$ onto $\\Delta \\subset \\mathbb{R}^d$ (for $d=3, 4$) such that $f\\in W^{1,p}(\\Omega, \\mathbb{R}^d)$ and $f^{-1}\\in W^{1,q}(\\Delta, \\mathbb{R}^d)$, $1\\leq p ,q < \\infty$ and any $\\epsilon >0$ we construct a diffeomorphism $\\tilde{f}$ such that   $$\\|f-\\tilde{f}\\|_{W^{1,p}(\\Omega,\\mathbb{R}^d)} + \\|f^{-1}-\\tilde{f}^{-1}\\|_{W^{1,q}(\\Delta,\\mathbb{R}^d)} < \\epsilon.$$",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AP"
    ],
    "url": "http://arxiv.org/abs/2507.02854v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02853v1",
    "title": "Imprints of information scrambling on eigenstates of a quantum chaotic system",
    "authors": [
      "Bikram Pain",
      "Ratul Thakur",
      "Sthitadhi Roy"
    ],
    "abstract": "How are the spatial and temporal patterns of information scrambling in locally interacting quantum many-body systems imprinted on the eigenstates of the system's time-evolution operator? We address this question by identifying statistical correlations among sets of minimally four eigenstates that provide a unified framework for various measures of information scrambling. These include operator mutual information and operator entanglement entropy of the time-evolution operator, as well as more conventional diagnostics such as two-point dynamical correlations and out-of-time-ordered correlators. We demonstrate this framework by deriving exact results for eigenstate correlations in a minimal model of quantum chaos -- Floquet dual-unitary circuits. These results reveal not only the butterfly effect and the information lightcone, but also finer structures of scrambling within the lightcone. Our work thus shows how the eigenstates of a chaotic system can encode the full spatiotemporal anatomy of quantum chaos, going beyond the descriptions offered by random matrix theory and the eigenstate thermalisation hypothesis.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "quant-ph",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "url": "http://arxiv.org/abs/2507.02853v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02852v1",
    "title": "Proof of a magnificent conjecture",
    "authors": [
      "M. Kool",
      "J. V. Rennemo"
    ],
    "abstract": "Motivated by super-Yang-Mills theory on a Calabi-Yau 4-fold, Nekrasov and Piazzalunga have assigned weights to $r$-tuples of solid partitions and conjectured a formula for their weighted generating function.   We define $K$-theoretic virtual invariants of Quot schemes of 0-dimensional quotients of $\\mathcal{O}_{\\mathbb{C}^4}^{\\oplus r}$ by realizing them as zero loci of isotropic sections of orthogonal bundles on non-commutative Quot schemes. Via the Oh-Thomas localization formula, we recover Nekrasov-Piazzalunga's weights and derive their sign rule.   Our proof passes through refining the $K$-theoretic invariants to sheaves and describing them via Clifford modules, which lets us show that they arise from a factorizable sequence of sheaves in the sense of Okounkov. Taking limits of the equivariant parameters, we then deduce the Nekrasov-Piazzalunga conjecture from its 3-dimensional analog.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AG",
      "hep-th",
      "math.CO"
    ],
    "url": "http://arxiv.org/abs/2507.02852v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02848v1",
    "title": "Quantum jet Hopf algebroids by cotwist",
    "authors": [
      "Xiao Han",
      "Shahn Majid"
    ],
    "abstract": "We introduce a cotwist construction for Hopf algebroids that also entails cotwisting or `quantisation' of the base and which is dual to a previous twisting construction of Xu. Whereas the latter applied the construction to the algebra of differential operators on a classical base $B$, we show that the dual of this is the algebra of sections $J^\\infty(B)$ of the jet bundle and hence the latter forms a Hopf algebroid, which we identify as a quotient of the pair Hopf algebroid $B\\otimes B$. This classical jet bundle is then quantised by our cotwist construction to give a noncommutative jet Hopf algebroid over a noncommutative base. We also observe in the commutative case that $J^k(B)$ for jets of order $k$ can be identified with $J^1(B_k)$ where $B_k$ denotes $B$ equipped with a certain non-standard first order differential calculus.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.QA"
    ],
    "url": "http://arxiv.org/abs/2507.02848v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02845v1",
    "title": "Enhancement of the effects due to the Schrödinger-Newton equation",
    "authors": [
      "Davide Giordano Ario Altamura",
      "José Luis Gaona-Reyes",
      "Elliot Simcox",
      "Hendrik Ulbricht",
      "Angelo Bassi"
    ],
    "abstract": "The Schr\\\"odinger-Newton (SN) equation introduces a nonlinear self-gravitational term to the standard Schr\\\"odinger equation, offering a paradigmatic model for semiclassical gravity. However, the small deviations it predicts from standard quantum mechanics pose significant experimental challenges. We propose a novel method to amplify such deviations through periodic modulation of the trapping frequency in a levitated mechanical oscillator. We identify specific regimes where the SN-induced effects on the dynamics of second moments are significantly enhanced-by up to six orders of magnitude compared to unmodulated setups. We show that this protocol remains feasible within current magnetic levitation technologies and enables distinguishability between standard and SN dynamics using measurable quantities such as the position variance. Our results pave the way for a viable experimental test of the SN equation, offering a new route to probe the interface between quantum mechanics and gravity.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02845v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02844v1",
    "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection",
    "authors": [
      "Ziqi Miao",
      "Yi Ding",
      "Lijun Li",
      "Jing Shao"
    ],
    "abstract": "With the emergence of strong visual-language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: visual-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct visual-focused strategies, dynamically generating auxiliary images when necessary to construct a visual-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The code is available at https://github.com/Dtc7w3PQ/Visco-Attack.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.CL",
      "cs.CR"
    ],
    "url": "http://arxiv.org/abs/2507.02844v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02842v1",
    "title": "On the Structure of Replicable Hypothesis Testers",
    "authors": [
      "Anders Aamand",
      "Maryam Aliakbarpour",
      "Justin Y. Chen",
      "Shyam Narayanan",
      "Sandeep Silwal"
    ],
    "abstract": "A hypothesis testing algorithm is replicable if, when run on two different samples from the same distribution, it produces the same output with high probability. This notion, defined by by Impagliazzo, Lei, Pitassi, and Sorell [STOC'22], can increase trust in testing procedures and is deeply related to algorithmic stability, generalization, and privacy. We build general tools to prove lower and upper bounds on the sample complexity of replicable testers, unifying and quantitatively improving upon existing results.   We identify a set of canonical properties, and prove that any replicable testing algorithm can be modified to satisfy these properties without worsening accuracy or sample complexity. A canonical replicable algorithm computes a deterministic function of its input (i.e., a test statistic) and thresholds against a uniformly random value in $[0,1]$. It is invariant to the order in which the samples are received, and, if the testing problem is ``symmetric,'' then the algorithm is also invariant to the labeling of the domain elements, resolving an open question by Liu and Ye [NeurIPS'24]. We prove new lower bounds for uniformity, identity, and closeness testing by reducing to the case where the replicable algorithm satisfies these canonical properties.   We systematize and improve upon a common strategy for replicable algorithm design based on test statistics with known expectation and bounded variance. Our framework allow testers which have been extensively analyzed in the non-replicable setting to be made replicable with minimal overhead. As direct applications of our framework, we obtain constant-factor optimal bounds for coin testing and closeness testing and get replicability for free in a large parameter regime for uniformity testing.   We also give state-of-the-art bounds for replicable Gaussian mean testing, and, unlike prior work, our algorithm runs in polynomial time.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DS"
    ],
    "url": "http://arxiv.org/abs/2507.02842v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02840v1",
    "title": "Neutrino mixing parameters and masses from $Δ(96)\\rtimes H_{CP}$ in the tri-direct CP approach",
    "authors": [
      "Li-Na Yan",
      "Xiang-Yan Gao",
      "Gao-Da Liu",
      "Cai-Chang Li"
    ],
    "abstract": "We present a comprehensive model independent analysis of all breaking patterns resulting from $\\Delta(96)\\rtimes H_{CP}$ in the tri-direct CP approach of the minimal seesaw model with two right-handed neutrinos. The three generations of left-handed lepton doublets are assumed to transform as the irreducible triplet $\\bm{3_{0}}$ of $\\Delta(96)$, and the two right-handed neutrinos are assigned to singlets. In the case that both flavon fields $\\phi_{\\text{atm}}$ and $\\phi_{\\text{sol}}$ transform as triplet $\\bm{\\bar{3}_{0}}$, only one phenomenologically viable lepton mixing pattern is obtained for normal ordering neutrino masses. The lepton mixing matrix is predicted to be TM1 pattern, with neutrino masses, mixing angles, and CP violation phases depending on only three real input parameters. When $\\phi_{\\text{sol}}$ is assigned to the $\\bm{\\bar{3}_{1}}$ representation, an additional real parameter $x$ must be included. Then we find 42 independent phenomenologically interesting mixing patterns, and the corresponding predictions for lepton mixing parameters and neutrino masses are obtained. Furthermore, we perform a detailed numerical analysis for five example breaking patterns with some benchmark values of $x$. The absolute values of the first columns of PMNS matrix are fixed to be $\\left(\\sqrt{\\frac{2}{3}},\\frac{1}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right)^{T}$, $\\frac{1}{5}\\left(\\sqrt{17},2,2\\right)^{T}$, $\\frac{1}{\\sqrt{38}}\\left(5,2,3\\right)^{T}$, $\\frac{1}{\\sqrt{57}}\\left(\\sqrt{37},\\sqrt{10},\\sqrt{10}\\right)^{T}$ and $\\frac{1}{3}\\left(\\sqrt{6},1,\\sqrt{2}\\right)^{T}$, respectively.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "hep-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02840v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02839v1",
    "title": "Stiefel optimization is NP-hard",
    "authors": [
      "Zehua Lai",
      "Lek-Heng Lim",
      "Tianyun Tang"
    ],
    "abstract": "We show that linearly constrained linear optimization over a Stiefel or Grassmann manifold is NP-hard in general. We show that the same is true for unconstrained quadratic optimization over a Stiefel manifold. We will establish the nonexistence of FPTAS for these optimization problems over a Stiefel manifold. As an aside we extend our results to flag manifolds. Combined with earlier findings, this shows that manifold optimization is a difficult endeavor -- even the simplest problems like LP and unconstrained QP are already NP-hard on the most common manifolds.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.OC",
      "cs.CC",
      "03D15, 90C26, 90C23, 65K10, 68Q25"
    ],
    "url": "http://arxiv.org/abs/2507.02839v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02838v1",
    "title": "Early time hydrodynamic attractor in a nearly-unitary Fermi gas",
    "authors": [
      "Michal P. Heller",
      "Clemens Werthmann"
    ],
    "abstract": "The hydrodynamic attractors paradigm aims to explain fast applicability of hydrodynamics at ultrarelativistic nuclear collisions at RHIC and LHC in terms of the emergence of a universal behaviour across different initial states. The attractors observed in theoretical models are first driven by a rapid expansion of matter, which later gives way to the decay of exponentially damped transient excitations into the viscous fluid regime. Recently a theoretical proposal was made how to realize hydrodynamic attractors in cold atomic gases focusing on the decay of transients and opening prospects for a future experimental realization in table top experiments. Motivated by this, in the present paper we show how the far-from-equilibrium, expansion-driven part of the hydrodynamic attractor emerges in a model of a nearly-unitary Fermi gas dynamics under full theoretical control.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "hep-th",
      "cond-mat.quant-gas",
      "hep-ph",
      "nucl-th"
    ],
    "url": "http://arxiv.org/abs/2507.02838v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02837v1",
    "title": "Free boundary regularity for a tumor growth model with obstacle",
    "authors": [
      "Giulia Bevilacqua",
      "Matteo Carducci"
    ],
    "abstract": "We develop an existence and regularity theory for solutions to a geometric free boundary problem motivated by models of tumor growth. In this setting, the tumor invades an accessible region $D$, its motion is directed along a constant vector $V$, and it cannot penetrate another region $K$ acting as an obstacle to the spread of the tumor. Due to the non variational structure of the problem, we show existence of viscosity solutions via Perron's method. Subsequently, we prove interior regularity for the free boundary near regular points by means of an improvement of flatness argument. We further analyze the boundary regularity and we prove that the free boundary meets the obstacle as a $C^{1,\\alpha}$ graph. A key step in the analysis of the boundary regularity involves the study of a thin obstacle problem with oblique boundary conditions, for which we establish $C^{1,\\alpha}$ estimates.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AP",
      "math-ph",
      "math.MP",
      "35R35, 35Q92, 76D27"
    ],
    "url": "http://arxiv.org/abs/2507.02837v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02836v1",
    "title": "On the Boundary Harnack Principle for operators with different lower order terms",
    "authors": [
      "Daniela De Silva",
      "Ovidiu Savin"
    ],
    "abstract": "We provide the classical Boundary Harnack principle in Lipschitz domains for solutions to two different linear uniformly elliptic equations with the same principal part.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AP"
    ],
    "url": "http://arxiv.org/abs/2507.02836v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02835v1",
    "title": "Revealing a transitional epoch of large-scale cosmic anisotropy in the quasar distribution",
    "authors": [
      "Amit Mondal",
      "Biswajit Pandey",
      "Krishna Ghosh"
    ],
    "abstract": "The Cosmological Principle posits that the Universe is isotropic on the largest scales. While widely supported, this foundational assumption remains testable. We analyse the angular distribution of over one million quasars from the Gaia-unWISE catalogue using Renyi entropy, a multiscale statistical measure sensitive to higher-order clustering. Dividing the sample into three redshift bins, we find that both the low- and high-redshift distributions are statistically consistent with isotropy. However, at intermediate redshift ($1 \\leq z < 2.2$), we detect a statistically significant and scale-dependent anisotropy that persists under stringent masking, suggesting a physical origin. We interpret this as evidence for a transitional epoch in cosmic history, during which large-scale structures such as superclusters became prominent before their growth was gradually damped by the onset of accelerated expansion. These findings position Renyi entropy as a powerful probe of cosmic evolution and highlight the potential thermodynamic links between structure formation, entropy dissipation, and the emergence of large-scale isotropy.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.CO"
    ],
    "url": "http://arxiv.org/abs/2507.02835v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02831v1",
    "title": "Trace Formulas for Deformed W-Algebras",
    "authors": [
      "Fabrizio Nieri"
    ],
    "abstract": "We investigate trace formulas in $\\varepsilon$-deformed W-algebras, highlighting a novel connection to the modular double of $q$-deformed W-algebras. In particular, we show that torus correlators in the additive (Yangian) setting reproduce sphere correlators in the trigonometric setup, possibly with the inclusion of a non-perturbative completion. From a dual perspective, this mechanism implements a gauge theoretic 2d$\\to$3d uplift, where a circle direction in the world-sheet transmutes to a compact space-time direction in a non-trivial manner. We further discuss a unified picture of deformed W-algebras driven by trace formulas, suggesting a deeper algebraic layer related to the massive and massless form-factor approach to integrable QFT and 2d CFT.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "hep-th",
      "math.QA"
    ],
    "url": "http://arxiv.org/abs/2507.02831v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02830v1",
    "title": "Electric Field Induced Superconductivity in Bilayer Octagraphene",
    "authors": [
      "Yitong Yao",
      "Jun Li",
      "Jiacheng Ye",
      "Fan Yang",
      "Dao-Xin Yao"
    ],
    "abstract": "We investigate the energy bands, magnetism, and superconductivity of bilayer octagraphene with A-A stacking under a perpendicular electric field. A tight-binding model is used to analyze the band structure of the system. The doubling of the unit cell results in each band of the single layer splitting into two. We find that applying a perpendicular electric field increases the band splitting. As the electric field strength increases, the nesting of the Fermi Surface(FS) weakens, eventually disrupting the antiferromagnetic order and bilayer octagraphene exhibits superconductivity. Spin fluctuations can induce unconventional superconductivity with s+--wave pairing. Applying a perpendicular electric field to bilayer octagraphene parent weakens the nesting of the FS, ultimately killing the spin-density-wave (SDW) ordered state and transitioning it into the superconducting state, whichworks as a doping effect. We use the random-phase approximation approach to obtain the pairing eigenvalues and pairing symmetries of the perpendicular electric field-tuned bilayer octagraphene in the weak coupling limit. By tuning the strength of the perpendicular electric field, the critical interaction strength for SDW order can be modified, which in turn may promote the emergence of unconventional superconductivity.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.supr-con"
    ],
    "url": "http://arxiv.org/abs/2507.02830v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02829v1",
    "title": "Enhancing Noisy Quantum Sensing by GHZ State Partitioning",
    "authors": [
      "Allen Zang",
      "Tian-Xing Zheng",
      "Peter C. Maurer",
      "Frederic T. Chong",
      "Martin Suchara",
      "Tian Zhong"
    ],
    "abstract": "Presence of harmful noise is inevitable in entanglement-enhanced sensing systems, requiring careful allocation of resources to optimize sensing performance in practical scenarios. We advocate a simple but effective strategy to improve sensing performance in the presence of noise. Given a fixed number of quantum sensors, we partition the preparation of GHZ states by preparing smaller, independent sub-ensembles of GHZ states instead of a GHZ state across all sensors. We perform extensive analytical studies of the phase estimation performance when using partitioned GHZ states under realistic noise -- including state preparation error, particle loss during parameter encoding, and sensor dephasing during parameter encoding. We derive simple, closed-form expressions that quantify the optimal number of sub-ensembles for partitioned GHZ states. We also examine the explicit noisy quantum sensing dynamics under dephasing and loss, where we demonstrate the advantage from partitioning for maximal QFI, short-time QFI increase, and the sensing performance in the sequential scheme. The results offer quantitative insights into the sensing performance impact of different noise sources and reinforce the importance of resource allocation optimization in realistic quantum applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02829v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02828v1",
    "title": "Designs from magic-augmented Clifford circuits",
    "authors": [
      "Yuzhen Zhang",
      "Sagar Vijay",
      "Yingfei Gu",
      "Yimu Bao"
    ],
    "abstract": "We introduce magic-augmented Clifford circuits -- architectures in which Clifford circuits are preceded and/or followed by constant-depth circuits of non-Clifford (``magic\") gates -- as a resource-efficient way to realize approximate $k$-designs, with reduced circuit depth and usage of magic. We prove that shallow Clifford circuits, when augmented with constant-depth circuits of magic gates, can generate approximate unitary and state $k$-designs with $\\epsilon$ relative error. The total circuit depth for these constructions on $N$ qubits is $O(\\log (N/\\epsilon)) +2^{O(k\\log k)}$ in one dimension and $O(\\log\\log(N/\\epsilon))+2^{O(k\\log k)}$ in all-to-all circuits using ancillas, which improves upon previous results for small $k \\geq 4$. Furthermore, our construction of relative-error state $k$-designs only involves states with strictly local magic. The required number of magic gates is parametrically reduced when considering $k$-designs with bounded additive error. As an example, we show that shallow Clifford circuits followed by $O(k^2)$ single-qubit magic gates, independent of system size, can generate an additive-error state $k$-design. We develop a classical statistical mechanics description of our random circuit architectures, which provides a quantitative understanding of the required depth and number of magic gates for additive-error state $k$-designs. We also prove no-go theorems for various architectures to generate designs with bounded relative error.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "quant-ph",
      "cond-mat.stat-mech",
      "cond-mat.str-el",
      "cs.IT",
      "hep-th",
      "math.IT"
    ],
    "url": "http://arxiv.org/abs/2507.02828v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02827v1",
    "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network",
    "authors": [
      "Ying Yu",
      "Hang Xiao",
      "Siyao Li",
      "Jiarui Li",
      "Haotian Tang",
      "Hanyu Liu",
      "Chao Li"
    ],
    "abstract": "The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02827v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02825v1",
    "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
    "authors": [
      "Yuxuan Zhu",
      "Tengjun Jin",
      "Yada Pruksachatkun",
      "Andy Zhang",
      "Shu Liu",
      "Sasha Cui",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Kevin Meng",
      "Rebecca Weiss",
      "Fazl Barez",
      "Rahul Gupta",
      "Jwala Dhamala",
      "Jacob Merizian",
      "Mario Giulianelli",
      "Harry Coppock",
      "Cozmin Ududec",
      "Jasjeet Sekhon",
      "Jacob Steinhardt",
      "Antony Kellerman",
      "Sarah Schwettmann",
      "Matei Zaharia",
      "Ion Stoica",
      "Percy Liang",
      "Daniel Kang"
    ],
    "abstract": "Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "A.1; I.2.m"
    ],
    "url": "http://arxiv.org/abs/2507.02825v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02823v1",
    "title": "Osculating Geometry and Higher-Order Distance Loci",
    "authors": [
      "Sandra Di Rocco",
      "Kemal Rose",
      "Luca Sodomaco"
    ],
    "abstract": "We discuss the problem of optimizing the distance function from a given point, subject to polynomial constraints. A key algebraic invariant that governs its complexity is the Euclidean distance degree, which pertains to first-order tangency. We focus on the data locus of points possessing at least one critical point of the distance function that is normal to a higher-order osculating space. We propose a novel definition of higher-order distance degree as an intersection-theoretic invariant involving jet bundles and higher-order polar classes. Our research yields closed formulas for generic maps, Veronese embeddings, and toric embeddings. We place particular emphasis on the Bombieri-Weyl metric, revealing that the chosen metric profoundly influences both the degree and birationality of the higher-order projection maps. Additionally, we introduce a tropical framework that represents these degrees as stable intersections with Bergman fans, facilitating effective combinatorial computation in toric settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AG"
    ],
    "url": "http://arxiv.org/abs/2507.02823v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02821v1",
    "title": "Relativistic accretion and burdened primordial black holes",
    "authors": [
      "Suvashis Maity"
    ],
    "abstract": "We examine the joint effects of relativistic accretion and memory burdened evaporation on the evolution of primordial black holes (PBHs). The memory burden effect, which delays the evaporation by inducing a backreaction and making the evaporation rate scale as an inverse power law of the PBH entropy, opens up a new window that allows PBHs with $M \\lesssim 10^{15}~\\mathrm{g}$ to survive until the present epoch. Meanwhile, accretion increases the mass of PBHs, thereby enhancing their chances of survival for a given initial mass. We consider two main scenarios: one where PBHs evaporate completely before big bang nucleosynthesis, and another where PBHs persist until today. In the case of evaporation, we analyse the emission of dark matter (DM) and dark radiation (DR) during the process of evaporation. Conversely, in the other case, the surviving PBHs themselves can contribute as DM. We further investigate how relativistic and non-relativistic accretion, together with memory burdened evaporation, impact the parameter space of the emitted DM, the abundance of stable PBHs as DM, and the contribution of DR to the effective number of relativistic degrees of freedom, $\\Delta N_{\\mathrm{eff}}$.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.CO",
      "hep-th"
    ],
    "url": "http://arxiv.org/abs/2507.02821v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02820v1",
    "title": "Classification and Reduction of Homogeneous Star Products",
    "authors": [
      "Marvin Dippell",
      "Chiara Esposito",
      "Jonas Schnitzer"
    ],
    "abstract": "We present a classification of homogeneous star products on duals of Lie algebroids in terms of the second Lie algebroid cohomology. Moreover, we extend this classification to projectable star products, i.e., to quantizations compatible with (coisotropic) reduction. This implies that quantization commutes with reduction in the considered setting.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.QA",
      "math-ph",
      "math.DG",
      "math.MP",
      "53D55, 53D17, 53D20"
    ],
    "url": "http://arxiv.org/abs/2507.02820v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02816v1",
    "title": "Helicons in tilted-Weyl semimetals",
    "authors": [
      "Shiv Kumar Ram",
      "Amit Gupta"
    ],
    "abstract": "Helicons are transverse electromagnetic modes in three-dimensional (3D) electron systems in the presence of a static magnetic field. These modes have been proposed in isotropic or single Weyl semimetals(WSMs) (Francesco M.D. Pellegrino et al, Phys.\\ Rev.\\ B {\\bf 92}, 201407(R) (2015)). In this work, we introduce a tilt term to investigate helicon modes in gapless WSMs within a semiclassical Boltzmann approach and show that the helicon modes exist in tilted WSMs.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.str-el"
    ],
    "url": "http://arxiv.org/abs/2507.02816v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02813v1",
    "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion",
    "authors": [
      "Fangfu Liu",
      "Hao Li",
      "Jiawei Chi",
      "Hanyang Wang",
      "Minghui Yang",
      "Fudong Wang",
      "Yueqi Duan"
    ],
    "abstract": "Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02813v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02812v1",
    "title": "Fluid dynamics of a liquid mirror space telescope",
    "authors": [
      "Israel Gabay",
      "Omer Luria",
      "Edward Balaban",
      "Amir D. Gat",
      "Moran Bercovici"
    ],
    "abstract": "Large aperture telescopes are pivotal for exploring the universe, yet even with state-of-the-art manufacturing and launch technology, their size is limited to several meters. As we aim to build larger telescopes - extending tens of meters - designs in which the main mirror is based on liquid deployment in space are emerging as promising candidates. However, alongside their enormous potential advantages, liquid-based surfaces present new challenges in material science, mechanics, and fluid dynamics. One of the fundamental questions is whether it is possible for such surfaces to maintain their precise optical shape over long durations, and in particular under the forces induced by the telescope's accelerations. In this paper, we present a model and a closed-form analytical solution for the non-self-adjoint problem of the dynamics of a thin liquid film pinned within a finite circular domain. We use the 50-meter Fluidic Telescope (FLUTE) concept as the case study, and examine the liquid dynamics of the telescope under both slewing actuation and relaxation regimes, elucidating the role of geometrical parameters and liquid properties. The solutions reveal a maneuvering 'budget' wherein the degradation of the mirror surface is directly linked to the choice of maneuvers and their sequence. By simulating ten years of typical operation, we show that, while the maximal deformation might reach several microns, the spatial distribution of the deformation and their propagation rate allows the telescope to maintain its optical functionality for years, with at least a substantial portion of the aperture remaining suitable for astronomical observations. The model provides valuable insights and guidelines into the performance of liquid-film space telescopes, marking a crucial step toward realizing the potential of this innovative concept.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.IM"
    ],
    "url": "http://arxiv.org/abs/2507.02812v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02811v1",
    "title": "Bayesian frequency estimation at the fundamental quantum limit",
    "authors": [
      "James W. Gardner",
      "Tuvia Gefen",
      "Ethan Payne",
      "Su Direkci",
      "Sander M. Vermeulen",
      "Simon A. Haine",
      "Joseph J. Hope",
      "Lee McCuller",
      "Yanbei Chen"
    ],
    "abstract": "Searching for a weak signal at an unknown frequency is a canonical task in experiments probing fundamental physics such as gravitational-wave observatories and ultra-light dark matter haloscopes. These state-of-the-art sensors are limited by quantum noise arising from the fundamental uncertainty about the state of the device. Classically, frequency estimation suffers from a threshold effect in the signal-to-noise ratio such that weak signals are extremely hard to localise in frequency. We show that this phenomenon persists at the fundamental quantum limit but that the classical approach, a quadrature measurement, can nevertheless be beaten by a coherent protocol of projecting onto the \"quantum whitened\" possible quantum states. Quantum whitening is a covariant measurement, and we examine it analytically in the wide-prior limit and numerically for finite-width priors. Beyond accelerating searches for unknown frequencies, quantum whitening may be used generally to sense the parameter of a unitary encoding given no prior information about the parameter.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "quant-ph",
      "gr-qc"
    ],
    "url": "http://arxiv.org/abs/2507.02811v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02809v1",
    "title": "Block triangular preconditioning for inverse source problems in time-space fractional diffusion equations",
    "authors": [
      "Monoswini Majumdar",
      "Stefano Serra-Capizzano",
      "Rosita L. Sormani"
    ],
    "abstract": "The current work investigates the effectiveness of block triangular preconditioners in accelerating and stabilizing the numerical solution of inverse source problems governed by time-space fractional diffusion equations (TSFDEs). We focus on the recovery of an unknown spatial source function in a multi-dimensional TSFDE, incorporating Caputo time-fractional derivatives and the fractional Laplacian. The inherent ill-posedness is addressed via a quasi-boundary value regularization, followed by a finite difference discretization that leads to large, structured linear systems. We develop and analyze a block triangular preconditioning strategy that mimics the coefficient matrix, while simplifying its structure for computational efficiency. Numerical experiments using the GMRES solver demonstrate that the proposed preconditioner significantly improve convergence rates, robustness, and accuracy, making it well-suited for large-scale, real-world inverse problems involving fractional modeling.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.NA",
      "cs.NA",
      "65F10 (65F08, 15A18, 15B05, 65R32)"
    ],
    "url": "http://arxiv.org/abs/2507.02809v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02805v1",
    "title": "Tailoring the Electronic Properties of Monoclinic (InxAl1-x)2O3 Alloys via Substitutional Donors and Acceptors",
    "authors": [
      "Mohamed Abdelilah Fadla",
      "Myrta Grüning",
      "Lorenzo Stella"
    ],
    "abstract": "Ultra-wide bandgap semiconductors such as \\b{eta}-Ga2O3 are ideal materials for next-generation power electronic devices. Electronic and mechanical properties of \\b{eta}-Ga2O3 can be tuned by alloying with other sesquioxides, notably Al2O3 and In2O3. Moreover, by tuning the In content of a (InxAl1-x)2O3 alloy, its lattice constants can be matched to those of Ga2O3, while preserving a large conduction-band offset. In view of potential applications to \\b{eta}-Ga2O3-based heterostructure, we performed atomistic modelling of (InxAl1-x)2O3 alloys using density functional theory to investigate thermodynamic and electrical properties of conventional group IV dopants (Si, Sn, C, Ge), alternative metal donors (Ta, Zr, Hf), and acceptors (Mg, Zn, Cu). The hybrid Heyd-Scuseria-Ernzerhof functional (HSE06) is used to accurately quantify the defect formation energies, ionization levels, and concentrations over a wide range of experimentally relevant conditions for the oxygen chemical potential and temperature. In our atomistic models, Hf and Zr show favourable properties as alternative donors to Si and other group IV impurities, especially under oxygen-poor conditions. Our findings also suggest that acceptors Mg, Zn, and Cu, while they cannot promote p-doping, can be still beneficial for the compensation of unintentionally n-doped materials, e.g., to generate semi-insulating layers and improve rectification.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mtrl-sci"
    ],
    "url": "http://arxiv.org/abs/2507.02805v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02803v1",
    "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars",
    "authors": [
      "Gent Serifi",
      "Marcel C. Bühler"
    ],
    "abstract": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.GR"
    ],
    "url": "http://arxiv.org/abs/2507.02803v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02802v1",
    "title": "AREE-Based Decoupled Design of Hybrid Beamformers in mmWave XL-MIMO Systems",
    "authors": [
      "Jiazhe Li",
      "Nicolò Decarli",
      "Francesco Guidi",
      "Heng Dong",
      "Anna Guerra",
      "Alessandro Bazzi",
      "Zhuoming Li"
    ],
    "abstract": "Hybrid beamforming has been widely employed in mmWave communications such as vehicular-to-everything (V2X) scenarios, as a compromise between hardware complexity and spectral efficiency. However, the inherent coupling between analog and digital precoders in hybrid array architecture significantly limits the computational and spectral efficiency of existing algorithms. To address this issue, we propose an alternating residual error elimination (AREE) algorithm, which decomposes the hybrid beamforming problem into two low-dimensional subproblems, each exhibiting a favorable matrix structure that enables effective decoupling of analog and digital precoders from the matrix product formulation. These subproblems iteratively eliminate each other's residual errors, driving the original problem toward the optimal hybrid beamforming performance. The proposed initialization ensures rapid convergence, while a low-complexity geometric channel SVD algorithm is developed by transforming the high-dimensional sparse channel into a low-dimensional equivalent, thereby simplifying the derivation of subproblems. Simulation results demonstrate that the AREE algorithm effectively decouples analog and digital precoders with low complexity, achieves fast convergence, and offers higher spectral efficiency than existing beamforming methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.SP"
    ],
    "url": "http://arxiv.org/abs/2507.02802v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02800v1",
    "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding",
    "authors": [
      "Ebrahim Feghhi",
      "Shreyas Kaasyap",
      "Nima Hadidi",
      "Jonathan C. Kao"
    ],
    "abstract": "Speech neuroprostheses aim to restore communication for people with severe paralysis by decoding speech directly from neural activity. To accelerate algorithmic progress, a recent benchmark released intracranial recordings from a paralyzed participant attempting to speak, along with a baseline decoding algorithm. Prior work on the benchmark showed impressive accuracy gains. However, these gains increased computational costs and were not demonstrated in a real-time decoding setting. Here, we make three contributions that pave the way towards accurate, efficient, and real-time neural speech decoding. First, we incorporate large amounts of time masking during training. On average, over $50\\%$ of each trial is masked. Second, we replace the gated recurrent unit (GRU) architecture used in the baseline algorithm with a compact Transformer. The Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory usage by $36\\%$ relative, and is significantly faster to calibrate relative to the GRU. Third, we design a lightweight variant of an existing test-time adaptation method developed for decoding handwriting from neural activity. Our variant adapts the model using multiple time masked augmentations of a single trial and requires only one gradient step per trial. Together, these contributions reduce word error rate by $19.5\\%$ and effectively mitigate performance degradations across held-out days in a real-time decoding setting while substantially lowering computational costs.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2507.02800v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02799v1",
    "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models",
    "authors": [
      "Riccardo Cantini",
      "Nicola Gabriele",
      "Alessio Orsino",
      "Domenico Talia"
    ],
    "abstract": "Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitation. We systematically evaluate state-of-the-art RLMs across diverse sociocultural dimensions, using an LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak techniques to assess the strength of built-in safety mechanisms. Our evaluation addresses three key questions: (i) how the introduction of reasoning capabilities affects model fairness and robustness; (ii) whether models fine-tuned for reasoning exhibit greater safety than those relying on CoT prompting at inference time; and (iii) how the success rate of jailbreak attacks targeting bias elicitation varies with the reasoning mechanisms employed. Our findings reveal a nuanced relationship between reasoning capabilities and bias safety. Surprisingly, models with explicit reasoning, whether via CoT prompting or fine-tuned reasoning traces, are generally more vulnerable to bias elicitation than base models without such mechanisms, suggesting reasoning may unintentionally open new pathways for stereotype reinforcement. Reasoning-enabled models appear somewhat safer than those relying on CoT prompting, which are particularly prone to contextual reframing attacks through storytelling prompts, fictional personas, or reward-shaped instructions. These results challenge the assumption that reasoning inherently improves robustness and underscore the need for more bias-aware approaches to reasoning design.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02799v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02797v1",
    "title": "A Highly Carbon-Rich Dayside and Disequilibrium Chemistry in the Ultra-Hot Jupiter WASP-19b",
    "authors": [
      "Suman Saha",
      "James S. Jenkins"
    ],
    "abstract": "Ultra-hot Jupiters (UHJs) offer exceptional opportunities for detailed atmospheric characterization via emission spectroscopy. Here we present a comprehensive analysis of the dayside atmosphere of WASP-19b$\\unicode{x2014}$one of the shortest-period UHJs$\\unicode{x2014}$using archival JWST NIRSpec/PRISM observations spanning 0.6-5.3 $\\mu$m. We report robust detections of H$_2$O (16.44 $\\sigma$), CO (5.47 $\\sigma$), and CO$_2$ (10.72 $\\sigma$), along with marginal detections of CH$_4$ (3.76 $\\sigma$) and C$_2$H$_2$ (2.45 $\\sigma$). The retrieved composition reveals a highly carbon-rich atmosphere with a tightly constrained super-solar C/O ratio of 0.94$\\pm$0.03. Elevated abundances of carbon-bearing species provide strong evidence (11.69 $\\sigma$) for disequilibrium chemistry. We also detect condensate clouds, likely Al$_2$O$_3$(c), at high significance (17.28 $\\sigma$), and constrain the atmospheric metallicity to 1.7$_{-0.7}^{+1.2}$ $\\times$ solar. These results establish a precise benchmark for modeling dayside conditions in extreme irradiated atmospheres and demonstrate JWST's transformative capabilities for exoplanet science.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.EP"
    ],
    "url": "http://arxiv.org/abs/2507.02797v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02796v1",
    "title": "Random Flights and Anomalous Diffusion: A Non-Markovian Take on Lorentz Processes",
    "authors": [
      "Lorenzo Facciaroni",
      "Costantino Ricciuti",
      "Enrico Scalas",
      "Bruno Toaldo"
    ],
    "abstract": "We study Lorentz processes in two different settings. Both cases are characterized by infinite expectation of the free-flight times, contrary to what happens in the classical Gallavotti-Spohn models. Under a suitable Boltzmann-Grad type scaling limit, they converge to non-Markovian random-flight processes with superdiffusive behavior. A further scaling limit yields another non Markovian process, i.e., a superdiffusion obtained by a suitable time-change of Brownian motion. Furthermore, we obtain the governing equations for our random flights and anomalous diffusion, which represent a non-local counterpart for the linear-Boltzmann and diffusion equations arising in the classical theory. It turns out that these equations have the form of fractional kinetic equations in both time and space. To prove these results, we develop a technique based on mixtures of Feller semigroups.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.PR",
      "math-ph",
      "math.MP"
    ],
    "url": "http://arxiv.org/abs/2507.02796v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02795v1",
    "title": "Boosting the NOx production in microwave air plasma: A synergy of chemistry and vibrational kinetics",
    "authors": [
      "Qinghao Shen",
      "Aleksandr Pikalev",
      "Jonas Gans",
      "Lex Kuijpers",
      "Ashley Hughes",
      "Vasco Guerra",
      "M. C. M van de Sanden"
    ],
    "abstract": "This study employs a quasi-1.5D multi-temperature model to investigate the mechanisms governing NOx production and energy costs in microwave plasma reactors operating at 80 mbar, focusing on the interplay of vibrational, chemical and electron kinetics, thermodynamics, and transport processes across the discharge and afterglow. In the plasma discharge zone, non-thermal processes enhance NOx production as electrons transfer energy effectively to the vibrational mode of N2. However, the non-thermal enhancement is found to diminish rapidly within the central-afterglow region. The simulation results show good agreement with experimental data for both the temperature profile and energy cost. Turbulent effects facilitate radial NO diffusion into cooler regions while simultaneously enhancing cooling of the axial region. These findings highlight the potential to improve NOx synthesis efficiency by optimizing turbulence and maintaining non-thermal conditions, offering new opportunities for the advancement of plasma-based chemical processes.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.plasm-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02795v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02794v1",
    "title": "Vanishing Vertical Viscosity in Two-Dimensional Anisotropic Navier-Stokes Equations with No-Slip Boundary Conditions: An $L^p$ result",
    "authors": [
      "Chongsheng Cao",
      "Yanqiu Guo"
    ],
    "abstract": "This paper studies the inviscid limit problem for the two-dimensional Navier-Stokes equations with anisotropic viscosity. The fluid is assumed to be bounded above and below by impenetrable walls, with a no-slip boundary condition imposed on the bottom wall. For $H^2$ initial velocity, we establish strong convergence in the $L^p$ norm to the limiting problem as the vertical viscosity approaches zero, for any $2\\leq p <\\infty$. The main challenge lies in the mismatch of boundary conditions - specifically, the no-slip condition in the original problem versus the slip condition in the limiting problem.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AP",
      "35Q35, 35B35, 76D05, 76D10"
    ],
    "url": "http://arxiv.org/abs/2507.02794v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02793v1",
    "title": "Ultrafast optical excitation of magnons in 2D antiferromagnets via spin torque exerted by photocurrent of excitons: Signatures in charge pumping and THz emission",
    "authors": [
      "Jalil Varela-Manjarres",
      "Yafei Ren",
      "Branislav K. Nikolic"
    ],
    "abstract": "Recent experiments observing femtosecond laser pulse (fsLP) exciting magnons within two-dimensional (2D) antiferromagnetic (AF) semiconductors -- such as CrSBr, NiPS$_3$, and MnPS$_3$, or their van der Waals heterostructures -- suggest exciton-mediation of such an effect. However, its microscopic details remain obscure as resonant coupling of magnons, living in the sub-meV energy range, to excitons, living in \\mbox{$\\sim 1$ eV} range, can hardly be operative. Here, we develop a quantum transport theory of this effect, in which time-dependent nonequilibrium Green's function (TDNEF) for electrons driven by classical vector potential of fsLP are coupled to the Landau-Lifshitz-Gilbert (LLG) equation describing classical dynamics of localized magnetic moments (LMMs) within 2D AF semiconductor. Our TDNEGF+LLG theory explains how fsLP, with central frequency above the semiconductor gap, generates a photocurrent that subsequently exerts spin-transfer torque (STT) onto LMMs as a genuine nonequilibrium spintronic effect. The collective motion of LMMs analyzed by windowed Fast Fourier transform (FFT) reveals frequencies of excited magnons, as well as their lifetime governed by nonlocal damping due to the bath of electrons. In addition, the TDNEGF part of our TDNEGF+LLG self-consistent loop computes a time-dependent density matrix whose off-diagonal elements are utilized to describe, at the mean-field level, inter-orbital Coulomb interaction binding electrons and holes into excitons. Our TDNEGF+LLG theory predicts how excited magnons {\\em pump} charge current into the attached electrodes, or locally within AF semiconductor responsible for microwave emission. The windowed FFT of the former signal contains imprints of excited magnons, as well as their interaction with excitons, which could be exploited as a novel probe in future experiments.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "url": "http://arxiv.org/abs/2507.02793v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02792v1",
    "title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation",
    "authors": [
      "Liheng Zhang",
      "Lexi Pang",
      "Hang Ye",
      "Xiaoxuan Ma",
      "Yizhou Wang"
    ],
    "abstract": "Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., depth or pose maps) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. By revisiting existing methods, we identify a core limitation: the synchronous injection of condition features fails to account for the trade-off between domain alignment and structural preservation during denoising. Inspired by this observation, we propose a flexible feature injection framework that decouples the injection timestep from the denoising process. At its core is a structure-rich injection module, which enables the model to better adapt to the evolving interplay between alignment and structure preservation throughout the diffusion steps, resulting in more faithful structural generation. In addition, we introduce appearance-rich prompting and a restart refinement strategy to further enhance appearance control and visual quality. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art performance across diverse zero-shot conditioning scenarios.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02792v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02790v1",
    "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding",
    "authors": [
      "Xiangfeng Wang",
      "Xiao Li",
      "Yadong Wei",
      "Xueyu Song",
      "Yang Song",
      "Xiaoqiang Xia",
      "Fangrui Zeng",
      "Zaiyi Chen",
      "Liu Liu",
      "Gu Xu",
      "Tong Xu"
    ],
    "abstract": "The rapid growth of online video content, especially on short video platforms, has created a growing demand for efficient video editing techniques that can condense long-form videos into concise and engaging clips. Existing automatic editing methods predominantly rely on textual cues from ASR transcripts and end-to-end segment selection, often neglecting the rich visual context and leading to incoherent outputs. In this paper, we propose a human-inspired automatic video editing framework (HIVE) that leverages multimodal narrative understanding to address these limitations. Our approach incorporates character extraction, dialogue analysis, and narrative summarization through multimodal large language models, enabling a holistic understanding of the video content. To further enhance coherence, we apply scene-level segmentation and decompose the editing process into three subtasks: highlight detection, opening/ending selection, and pruning of irrelevant content. To facilitate research in this area, we introduce DramaAD, a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips. Experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisement-oriented editing tasks, significantly narrowing the quality gap between automatic and human-edited videos.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02790v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02789v1",
    "title": "New components of Hilbert schemes of points and 2-step ideals",
    "authors": [
      "Franco Giovenzana",
      "Luca Giovenzana",
      "Michele Graffeo",
      "Paolo Lella"
    ],
    "abstract": "This paper presents new examples of elementary and non-elementary irreducible components of the Hilbert scheme of points and its nested variants. The results are achieved via a careful analysis of the deformations of a class of finite colength ideals that are introduced in this paper and referred to as 2-step ideals. The most notable reducibility results pertain to the 4-nested Hilbert scheme of points on a smooth surface, the reducibility of $\\text{Hilb}^{3,7}\\mathbb{A}^4$, and a method to detect a large number of generically reduced elementary components. To demonstrate the feasibility of this approach, we provide an explicit description of 215 new generically reduced elementary components in dimensions 4, 5 and 6.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AG",
      "math.AC",
      "14C05 (Primary) 13D02, 13C05 (Secondary)"
    ],
    "url": "http://arxiv.org/abs/2507.02789v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02788v1",
    "title": "Moral Responsibility or Obedience: What Do We Want from AI?",
    "authors": [
      "Joseph Boland"
    ],
    "abstract": "As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "cs.CY",
      "I.2.0; K.4.1"
    ],
    "url": "http://arxiv.org/abs/2507.02788v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02787v1",
    "title": "Stability of non-supersymmetric vacua from calibrations",
    "authors": [
      "Vincent Menet",
      "Alessandro Tomasiello"
    ],
    "abstract": "Supersymmetric vacua are protected from vacuum decay by energy positivity. No such argument is known for any non-supersymmetric vacua. In this paper, we try to extend to the latter a simpler argument based on calibrations, to at least protect them from decays mediated by D-brane bubbles, including their abelian bound states. We examine several classes of AdS$_4$ and AdS$_5$ solutions in type II string theory, including some new ones, involving coset spaces, sphere fibrations, K\\\"ahler--Einstein manifolds. Many of these vacua have resisted against all the decay channels we were able to assess. We also show how to use calibrations for the stability of D-branes already present in a non-supersymmetric solution.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "hep-th"
    ],
    "url": "http://arxiv.org/abs/2507.02787v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02786v1",
    "title": "Dynamical group approach to conformal field theory",
    "authors": [
      "G. A. Kerimov"
    ],
    "abstract": "We propose a purely group-theoretical method for describing the S-matrix in quantum field theory with dynamical symmetry. In this approach, the Heisenberg S-matrix in a QFT with dynamical symmetry is an intertwining operator between unitary representations of the underlining dynamical group acting in Hilbert spaces spanned by the 'in' and 'out' states. As application we consider scalar Euclidean QFT with conformal symmetry.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "hep-th"
    ],
    "url": "http://arxiv.org/abs/2507.02786v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02785v1",
    "title": "Metric dimension reduction modulus for logarithmic distortion",
    "authors": [
      "Dylan J. Altschuler",
      "Konstantin Tikhomirov"
    ],
    "abstract": "Given parameters $n$ and $\\alpha$, the metric dimension reduction modulus $k^\\alpha_n(\\ell_\\infty)$ is defined as the smallest $k$ such that every $n$--point metric space can be embedded into some $k$-dimensional normed space $X$ with bi--Lipschitz distortion at most $\\alpha$. A fundamental task in the theory of metric embeddings is to obtain sharp asymptotics for $k^\\alpha_n(\\ell_\\infty)$ for all choices of $\\alpha$ and $n$, with the range $\\alpha=\\Theta(\\log n)$ bearing special importance. While advances in the theory lead to the upper bound $k^{\\alpha}_n(\\ell_\\infty) = O(\\log n)$ for $\\alpha=\\Theta(\\log n)$, obtaining a matching lower bound has remained an open problem. We prove that $k^{\\beta\\log n}_n(\\ell_\\infty) = \\Omega(\\log n)$ for every constant $\\beta>0$, thereby closing the long--standing gap and resolving a question from the 2018 ICM plenary lecture of Naor.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.MG",
      "math.CO",
      "math.FA"
    ],
    "url": "http://arxiv.org/abs/2507.02785v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02784v1",
    "title": "Quasinormal modes of Floquet media slabs",
    "authors": [
      "Benjamin Vial",
      "Richard V. Craster"
    ],
    "abstract": "Exploiting non-Hermitian wave-matter interactions in time-modulated media to enable the dynamic control of electromagnetic waves requires advanced theoretical tools. In this article we bridge concepts from photonic quasinormal modes (QNMs) and time-varying metamaterials providing the foundation for designing dynamic optical devices with prescribed scattering properties. Establishing the QNM framework for slabs with time-periodic permittivity, and solving the associated nonlinear eigenvalue problem, allows us to derive the QNM expansion capturing the resonant features of the system. This reduced-order model enables highly efficient computation of scattered fields while revealing insight into how modulation couples to resonant modes, creating tailored gain-loss engineering. Our approach is validated through numerical experiments on time-modulated systems, and we design strategies to engineer tailored excitations selectively amplifying or suppressing specific modal contributions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.optics"
    ],
    "url": "http://arxiv.org/abs/2507.02784v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02783v1",
    "title": "Uniform semiclassical observable error bound of Trotterization without the Egorov theorem: a simple algebraic proof",
    "authors": [
      "Di Fang",
      "Conrad Qu"
    ],
    "abstract": "Efficient simulation of the semiclassical Schr\\\"odinger equation has garnered significant attention in the numerical analysis community. While controlling the error in the unitary evolution or the wavefunction typically requires the time step size to shrink as the semiclassical parameter $h$ decreases, it has been observed -- and proved for first- and second-order Trotterization schemes -- that the error in certain classes of observables admits a time step size independent of $h$. In this work, we explicitly characterize this class of observables and present a new, simple algebraic proof of uniform-in-$h$ error bounds for arbitrarily high-order Trotterization schemes. Our proof relies solely on the algebraic structure of the underlying operators in both the continuous and discrete settings. Unlike previous analyses, it avoids Egorov-type theorems and bypasses heavy semiclassical machinery. To our knowledge, this is the first proof of uniform-in-$h$ observable error bounds for Trotterization in the semiclassical regime that relies only on algebraic structure, without invoking the semiclassical limit.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.NA",
      "cs.NA",
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02783v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02781v1",
    "title": "From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images",
    "authors": [
      "Danrong Zhang",
      "Huili Huang",
      "N. Simrill Smith",
      "Nimisha Roy",
      "J. David Frost"
    ],
    "abstract": "In the aftermath of earthquakes, social media images have become a crucial resource for disaster reconnaissance, providing immediate insights into the extent of damage. Traditional approaches to damage severity assessment in post-earthquake social media images often rely on classification methods, which are inherently subjective and incapable of accounting for the varying extents of damage within an image. Addressing these limitations, this study proposes a novel approach by framing damage severity assessment as a semantic segmentation problem, aiming for a more objective analysis of damage in earthquake-affected areas. The methodology involves the construction of a segmented damage severity dataset, categorizing damage into three degrees: undamaged structures, damaged structures, and debris. Utilizing this dataset, the study fine-tunes a SegFormer model to generate damage severity segmentations for post-earthquake social media images. Furthermore, a new damage severity scoring system is introduced, quantifying damage by considering the varying degrees of damage across different areas within images, adjusted for depth estimation. The application of this approach allows for the quantification of damage severity in social media images in a more objective and comprehensive manner. By providing a nuanced understanding of damage, this study enhances the ability to offer precise guidance to disaster reconnaissance teams, facilitating more effective and targeted response efforts in the aftermath of earthquakes.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.SI"
    ],
    "url": "http://arxiv.org/abs/2507.02781v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02780v1",
    "title": "Massive Interacting Binaries Enhance Feedback in Star-Forming Regions",
    "authors": [
      "Claude Cournoyer-Cloutier",
      "Eric P. Andersson",
      "Sabrina M. Appel",
      "Natalia Lahén",
      "Brooke Polak",
      "Antti Rantala",
      "Silvia Toonen",
      "Alison Sills",
      "Steven Rieder",
      "Simon Portegies Zwart",
      "Mordecai-Mark Mac Low",
      "William E. Harris"
    ],
    "abstract": "We present a new framework to incorporate feedback from massive interacting binaries in simulations of star cluster formation. Our new feedback model adds binary stellar evolution to the cluster formation code Torch, and couples it in AMUSE to the pre-existing modules for collisional stellar dynamics, magnetohydrodynamics, and mechanical and radiative feedback. Our model accounts for the effects of mass transfer on the stars' mass loss rates, their radiation spectra, and the timing of core-collapse supernovae. It also injects mass lost through non-conservative mass transfer and common envelope ejection into the interstellar medium. We demonstrate the use of our feedback model through simulations of isolated binaries in a gaseous medium, and of embedded clusters of massive binaries. Feedback from interacting binaries efficiently couples with the surrounding interstellar medium. It increases the size of HII regions, increases the kinetic and thermal energy of the gas, and increases the pressure within HII regions compared to models that use single star stellar evolution. Those differences arise from the ionizing radiation, which increases by three orders of magnitude, resulting in HII regions that expand due to thermal pressure rather than radiation pressure. The effects of stellar dynamics and the gravitational potential of the background gas cause the evolution of individual binaries to deviate from the predictions made by secular evolution, impacting the subsequent feedback from the binary. We conclude that massive interacting binaries are an important source of feedback in cluster-forming regions, and must be considered when studying the emerging timescales of young star clusters.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "url": "http://arxiv.org/abs/2507.02780v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02779v1",
    "title": "Analytic Phase Solution and Point Vortex Model for Dipolar Quantum Vortices",
    "authors": [
      "Ryan Doran",
      "Thomas Bland"
    ],
    "abstract": "We derive an analytic expression for the phase of a quantum vortex in a dipolar Bose-Einstein condensate, capturing anisotropic effects from long-range dipole-dipole interactions. This solution provides a foundation for a dipolar point vortex model (DPVM), incorporating both phase-driven flow and dipolar forces. The DPVM reproduces key features of vortex pair dynamics seen in full simulations, including anisotropic velocities, deformed orbits, and directional motion, offering a minimal and accurate model for dipolar vortex dynamics. Our results open the door to analytic studies of vortices in dipolar quantum matter and establish a new platform for exploring vortex dynamics and turbulence in these systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.quant-gas"
    ],
    "url": "http://arxiv.org/abs/2507.02779v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02777v1",
    "title": "Advanced techniques of searching for flares of ultra-high-energy photons from point sources",
    "authors": [
      "Jaroslaw Stasielak",
      "Chaitanya Priyadarshi",
      "Dariusz Góra",
      "Nataliia Borodai",
      "Marcus Niechciol",
      "Jan Pękala"
    ],
    "abstract": "Astrophysical flares are one of the possible prominent source classes of ultra-high-energy (UHE, $E > 10^{17}$ eV) cosmic rays, which can be detected by recording clusters of extensive air showers in arrays of detectors. The search for sources of neutral particles offers distinct advantages over searching for sources of charged particles, as the former traverse cosmic distances undeflected by magnetic fields. While no cosmic-ray photons exceeding $10^{17}$ eV have been definitively detected, identifying the clustering of events in cosmic-ray data would provide compelling evidence for their existence. We compare two analysis methods for detecting direction-time clustering in UHE extensive air showers: an approach that examines multiplets, and the stacking method, which analyzes sets of doublets that are not necessarily consecutive, thus making it sensitive to multiple flares. Both techniques combine time-clustering algorithms with unbinned likelihood study. Background events (initiated by hadrons) can be more efficiently distinguished from photon-induced events (signals) by using a photon tag that employs probability distribution functions to classify each event as more likely to be initiated by either a photon or a hadron. We demonstrate that these methods can effectively distinguish between events initiated by photons and those initiated by hadrons (background), and can accurately reproduce both the number of photon events within flares and their duration. We calculate the discovery potentials, i.e., the number of events required to identify a photon flare. The methods discussed can be used to search for cosmic ray sources and/or improve limits on the fluxes of UHE photons.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.HE"
    ],
    "url": "http://arxiv.org/abs/2507.02777v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02776v1",
    "title": "Splitting algorithm and normed convergence for drawing the random Loewner curves",
    "authors": [
      "Jiaming Chen",
      "Vlad Margarint"
    ],
    "abstract": "Recent advances in Schramm-Loewner evolution have driven increasing interest in non-standard Loewner flows. In this work, we propose a novel splitting algorithm to simulate random Loewner curves with rigorous convergence analysis in sup-norm and $L^p$. The algorithm is further extended to explore fractional SLE, driven by fractional Brownian motion, and noise-reinforced SLE, incorporating the effect on long-term memory. These exploratory and numerical extensions enable theoretical predictions on fractal dimensions and other statistical phenomena, providing new insights into such dynamics and opening directions for future research.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.PR"
    ],
    "url": "http://arxiv.org/abs/2507.02776v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02775v1",
    "title": "On the two-dimensional Navier-Stokes equations with horizontal viscosity",
    "authors": [
      "Chongsheng Cao",
      "Yanqiu Guo"
    ],
    "abstract": "This paper is concerned with a 2D channel flow that is periodic horizontally but bounded above and below by hard walls. We assume the presence of horizontal viscosity only. We study the well-posedness, large-time behavior, and stability of solutions. For global well-posedness, we aim to assume less differentiability on initial velocity $(u_0, v_0)$: in particular, we assume $u_0,v_0\\in L^2(\\Omega)$ and $\\partial_y u_0 \\in L^2(\\Omega)$.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.AP"
    ],
    "url": "http://arxiv.org/abs/2507.02775v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02774v1",
    "title": "Connected k-Median with Disjoint and Non-disjoint Clusters",
    "authors": [
      "Jan Eube",
      "Kelin Luo",
      "Dorian Reineccius",
      "Heiko Röglin",
      "Melanie Schmidt"
    ],
    "abstract": "The connected $k$-median problem is a constrained clustering problem that combines distance-based $k$-clustering with connectivity information. The problem allows to input a metric space and an unweighted undirected connectivity graph that is completely unrelated to the metric space. The goal is to compute $k$ centers and corresponding clusters such that each cluster forms a connected subgraph of $G$, and such that the $k$-median cost is minimized.   The problem has applications in very different fields like geodesy (particularly districting), social network analysis (especially community detection), or bioinformatics. We study a version with overlapping clusters where points can be part of multiple clusters which is natural for the use case of community detection. This problem variant is $\\Omega(\\log n)$-hard to approximate, and our main result is an $\\mathcal{O}(k^2 \\log n)$-approximation algorithm for the problem. We complement it with an $\\Omega(n^{1-\\epsilon})$-hardness result for the case of disjoint clusters without overlap with general connectivity graphs, as well as an exact algorithm in this setting if the connectivity graph is a tree.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DS"
    ],
    "url": "http://arxiv.org/abs/2507.02774v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02770v1",
    "title": "NVIDIA GPU Confidential Computing Demystified",
    "authors": [
      "Zhongshu Gu",
      "Enriquillo Valdez",
      "Salman Ahmed",
      "Julian James Stephen",
      "Michael Le",
      "Hani Jamjoom",
      "Shixuan Zhao",
      "Zhiqiang Lin"
    ],
    "abstract": "GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA Hopper Architecture, extending the trust boundary beyond traditional CPU-based confidential computing. This innovation enables GPUs to securely process AI workloads, providing a robust and efficient solution for handling sensitive data. For end users, transitioning to GPU-CC mode is seamless, requiring no modifications to existing AI applications. However, this ease of adoption contrasts sharply with the complexity of the underlying proprietary systems. The lack of transparency presents significant challenges for security researchers seeking a deeper understanding of GPU-CC's architecture and operational mechanisms.   The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of detailed specifications, the proprietary nature of the ecosystem, and the complexity of product design. In this paper, we aim to demystify the implementation of NVIDIA GPU-CC system by piecing together the fragmented and incomplete information disclosed from various sources. Our investigation begins with a high-level discussion of the threat model and security principles before delving into the low-level details of each system component. We instrument the GPU kernel module -- the only open-source component of the system -- and conduct a series of experiments to identify the security weaknesses and potential exploits. For certain components that are out of reach through experiments, we propose well-reasoned speculations about their inner working mechanisms. We have responsibly reported all security findings presented in this paper to the NVIDIA PSIRT Team.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR"
    ],
    "url": "http://arxiv.org/abs/2507.02770v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02769v1",
    "title": "The Local Structure Theorem for Graph Minors with Finite Index",
    "authors": [
      "Christophe Paul",
      "Evangelos Protopapas",
      "Dimitrios M. Thilikos",
      "Sebastian Wiederrecht"
    ],
    "abstract": "The Local Structure Theorem (LST) for graph minors roughly states that every $H$-minor free graph $G$ that contains a sufficiently large wall $W$, there is a set of few vertices $A$ such that, upon removing $A$, the resulting graph $G':=G - A$ admits an \"almost embedding\" $\\delta$ into a surface $\\Sigma$ in which $H$ does not embed. By almost embedding, we mean that there exists a hypergraph $\\mathcal{H}$ whose vertex set is a subset of the vertex set of $G$ and an embedding of $\\mathcal{H}$ in $\\Sigma$ such that 1) the drawing of each hyperedge of $\\mathcal{H}$ corresponds to a cell of $\\delta$, 2) the boundary of each cell intersects only the vertices of the corresponding hyperedge, and 3) all remaining vertices and edges of $G'$ are drawn in the interior of cells. The cells corresponding to hyperedges of arity at least $4$, called vortices, are few in number and have small \"depth\", while a \"large\" part of the wall $W$ is drawn outside the vortices and is \"grounded\" in the embedding $\\delta$. Now suppose that the subgraphs drawn inside each of the non-vortex cells are equipped with some finite index, i.e., each such cell is assigned a color from a finite set. We prove a version of the LST in which the set $C$ of colors assigned to the non-vortex cells exhibits \"large\" bidimensionality: The graph $G'$ contains a minor model of a large grid $\\Gamma$ where each bag corresponding to a vertex $v$ of $\\Gamma$, contains the subgraph drawn within a cell carrying color $\\alpha$, for every color $\\alpha \\in C$. Moreover, the grid $\\Gamma$ can be chosen in a way that is \"well-connected\" to the original wall $W$.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.CO",
      "cs.DM",
      "05C83, 05C85, 05C10, 05C75, 68R10",
      "G.2.2"
    ],
    "url": "http://arxiv.org/abs/2507.02769v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02768v1",
    "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment",
    "authors": [
      "Ke-Han Lu",
      "Zhehuai Chen",
      "Szu-Wei Fu",
      "Chao-Han Huck Yang",
      "Sung-Feng Huang",
      "Chih-Kai Yang",
      "Chee-En Yu",
      "Chun-Wei Chen",
      "Wei-Chih Chen",
      "Chien-yu Huang",
      "Yi-Cheng Lin",
      "Yu-Xiang Lin",
      "Chi-An Fu",
      "Chun-Yi Kuan",
      "Wenze Ren",
      "Xuanjun Chen",
      "Wei-Ping Huang",
      "En-Pei Hu",
      "Tzu-Quan Lin",
      "Yuan-Kuei Wu",
      "Kuan-Po Huang",
      "Hsiao-Ying Huang",
      "Huang-Cheng Chou",
      "Kai-Wei Chang",
      "Cheng-Han Chiang",
      "Boris Ginsburg",
      "Yu-Chiang Frank Wang",
      "Hung-yi Lee"
    ],
    "abstract": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these approaches have often suffered from the catastrophic forgetting of the LLM's original language abilities. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLM's native language proficiency while establishing effective audio-text alignment, thereby enabling zero-shot generalization without task-specific tuning. Using DeSTA, we construct DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training samples derived from 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music. DeSTA2.5-Audio achieves state-of-the-art or competitive performance across a wide range of audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate that our self-generated strategy outperforms widely adopted data construction and training strategies in both auditory perception and instruction-following capabilities. Our findings underscore the importance of carefully designed data construction in LALM development and offer practical insights for building robust, general-purpose LALMs.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "url": "http://arxiv.org/abs/2507.02768v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02767v1",
    "title": "A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)",
    "authors": [
      "Tiziano Dalmonte",
      "Marianna Girlando"
    ],
    "abstract": "Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and Olkhovikov, aims at providing a constructive analysis of conditional reasoning. In this framework, the would and the might conditional operators are no longer interdefinable. The intuitionistic conditional logics considered in the literature are defined by setting Chellas' conditional logic CK, whose semantics is defined using selection functions, within the constructive and intuitionistic framework introduced for intuitionistic modal logics. This operation gives rise to a constructive and an intuitionistic variant of (might-free-) CK, which we call CCKbox and IntCK respectively. Building on the proof systems defined for CK and for intuitionistic modal logics, in this paper we introduce a nested calculus for IntCK and a sequent calculus for CCKbox. Based on the sequent calculus, we define CCK, a conservative extension of Weiss' logic CCKbox with the might operator. We introduce a class of models and an axiomatization for CCK, and extend these result to several extensions of CCK.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LO"
    ],
    "url": "http://arxiv.org/abs/2507.02767v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02766v1",
    "title": "Computational Modelling of Thixotropic Multiphase Fluids",
    "authors": [
      "Andres Santiago Espinosa-Moreno",
      "Nicolás Moreno",
      "Marco Ellero"
    ],
    "abstract": "Multiphase systems are ubiquitous in engineering, biology, and materials science, where understanding their complex interactions and rheological behavior is crucial for advancing applications ranging from emulsion stability to cellular phase separation. This study presents a numerical methodology for modeling thixotropic multiphase fluids, emphasizing the transient behavior of viscosity and the intricate interactions between phases. The model incorporates phase-dependent viscosities, interfacial tension effects, and the dynamics of phase separation, coalescence, and break-up, making it suitable for simulating systems with complex flow regimes. A key feature of the methodology is its ability to capture thixotropic behavior, where viscosity evolves over time due to microstructural changes induced by shear history. This approach enables the simulation of aging and recovery processes in materials such as gels, emulsions, and biological tissues. The model is rigorously validated against benchmark cases, demonstrating its accuracy in predicting multiphase systems under static and dynamic conditions. Subsequently, the methodology is applied to investigate systems with varying levels of microstructural evolution, revealing the impact of thixotropic dynamics on overall system behavior. The results provide new insights into the time-dependent rheology of multiphase fluids and highlight the versatility of the model for applications in industrial and biological systems involving complex fluid interactions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.flu-dyn"
    ],
    "url": "http://arxiv.org/abs/2507.02766v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02765v1",
    "title": "Spin Caloritronics in irradiated chiral ferromagnetic systems",
    "authors": [
      "Sudin Ganguly",
      "Moumita Dey",
      "Santanu K. Maiti"
    ],
    "abstract": "We study the charge and spin-dependent thermoelectric response of a ferromagnetic helical system irradiated by arbitrarily polarized light, using a tight-binding framework and the Floquet-Bloch formalism. Transport properties for individual spin channels are determined by employing the non-equilibrium Green's function technique, while phonon thermal conductance is evaluated using a mass-spring model with different lead materials. The findings reveal that that light irradiation induces spin-split transmission features, suppresses thermal conductance, and yields favorable spin thermopower and figure of merit (FOM). The spin FOM consistently outperforms its charge counterpart under various light conditions. Moreover, long-range hopping is shown to enhance the spin thermoelectric performance, suggesting a promising strategy for efficient energy conversion in related ferromagnetic systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mes-hall",
      "cond-mat.dis-nn",
      "physics.comp-ph",
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02765v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02764v1",
    "title": "Terahertz Chip-Scale Meta-Networks with LSPR Routing: A Theoretical Framework",
    "authors": [
      "Maryam Khodadadi",
      "Hamidreza Taghvaee",
      "Pei Xiao",
      "Gabriele Gradoni",
      "Mohsen Khalily"
    ],
    "abstract": "Efficient chip-scale interconnects are essential for modern microelectronic-photonic systems, supporting high bandwidth and low-latency processing. Traditional wired links face high resistivity and latency, while millimeter-wave wireless solutions suffer from bandwidth congestion and interference. Terahertz (THz) plasmonic communication, based on surface plasmon polaritons (SPPs), offers high data rates and broad bandwidth, and is compatible with nanophotonic platforms. This work introduces a Binary Field-Driven Meta-Routing Method supported by a semi-analytical framework that models the tunable interaction between THz plasmonic phenomena and graphene's electromagnetic properties. By modulating graphene's impedance, the method enables dynamic coupling and routing of localized surface plasmon resonances (LSPRs) across a meta-network, facilitating real-time beam steering in chip-scale systems. Combining analytical conductivity models, coupled-mode theory, and algorithmic control, the approach enables predictive configuration of LSPR-based steering in reconfigurable graphene metasurfaces. Four meta-pixel antenna configurations Y-MetaRouter, MetaSwitcher, Penta-MetaEmitter, and CP-MetaCore are designed to support unidirectional radiation, bi-directional steering, frequency-driven transitions, and circular polarization, respectively. Chemical potential modulation creates reconfigurable LSPR pathways and virtual SPP channels. A Coupled-Mode Theory for Field-Driven LSPR Meta-Networks is proposed to model current distributions and predict far-field characteristics. Results show strong agreement between theory and full-wave simulations. A point-to-point meta-wireless link is analyzed, demonstrating scalability for low-latency, high-performance THz communication in WiNoC and chiplet applications. System-level metrics confirm feasibility for space-constrained, high-speed interconnects.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.optics",
      "eess.SP",
      "physics.app-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02764v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02763v1",
    "title": "A Leptonic Interpretation of the UHE Gamma-ray Emission from V4641 Sgr",
    "authors": [
      "Su-Yu Wan",
      "Jie-shuang Wang",
      "Ruo-Yu Liu"
    ],
    "abstract": "Recently, the microquasar V4641 Sgr and its surrounding is detected at TeV-PeV gamma-ray band. Interestingly, the spectrum follows a power-law function continuing up to 0.8 PeV as reported by LHAASO, and the morphology of the emission appears a puzzling elongated structure. In this work, we propose that the elongated UHE emission from V4641 Sgr could originate from the inverse Compton radiation of electrons with a very hard spectrum, which may result from shear acceleration mechanism in the jets driven by V4641 Sgr. We also calculate the corresponding X-ray synchrotron emission from the same electron population, predicting the potential range of non-thermal X-ray flux of the source. The recent observation by XRISM toward the central part of the UHE source could pose a constraint on the model parameters. In the future, a full coverage of the source by sensitive X-ray instrument and high-resolution TeV-PeV gamma-ray instrument may provide a critical test of the model.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.HE"
    ],
    "url": "http://arxiv.org/abs/2507.02763v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02761v1",
    "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization",
    "authors": [
      "Long Xu",
      "Choilam Wong",
      "Mengke Zhang",
      "Junxiao Lin",
      "Fei Gao"
    ],
    "abstract": "We present an efficient hierarchical motion planning pipeline for differential drive mobile manipulators. Our approach first searches for multiple collisionfree and topologically distinct paths for the mobile base to extract the space in which optimal solutions may exist. Further sampling and optimization are then conducted in parallel to explore feasible whole-body trajectories. For trajectory optimization, we employ polynomial trajectories and arc length-yaw parameterization, enabling efficient handling of the nonholonomic dynamics while ensuring optimality.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO"
    ],
    "url": "http://arxiv.org/abs/2507.02761v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02742v1",
    "title": "Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals",
    "authors": [
      "G. Buriola",
      "D. Cantone",
      "G. Cincotti",
      "E. G. Omodeo",
      "G. T. Spartà"
    ],
    "abstract": "This paper enriches preexisting satisfiability tests for unquantified languages, which in turn augment a fragment of Tarski's elementary algebra with unary real functions possessing a continuous first derivative.   Two sorts of individual variables are available, one ranging over real numbers and the other one ranging over the functions of interest. Numerical terms are built from real variables through constructs designating the four basic arithmetic operations and through the function-application constructs $f(t)$ and $D[\\,f\\,](t)$, where $f$ stands for a function variable, $t$ for a numerical term, and $D[\\,\\sqdot\\,]$ designates the differentiation operator. Comparison relators can be placed between numerical terms. An array of predicate symbols are also available, designating various relationships between functions, as well as function properties, that may hold over intervals of the real line; those are: (pointwise) function comparisons, strict and nonstrict monotonicity~/~convexity~/~concavity properties, comparisons between the derivative of a function and a real term--here, w.r.t.\\ earlier research, they are extended to (semi)-open intervals.   The decision method we propose consists in preprocessing the given formula into an equisatisfiable quantifier-free formula of the elementary algebra of real numbers, whose satisfiability can then be checked by means of Tarski's decision method. No direct reference to functions will appear in the target formula, each function variable having been superseded by a collection of stub real variables; hence, in order to prove that the proposed translation is satisfiability-preserving, we must figure out a sufficiently flexible family of interpolating $C^1$ functions that can accommodate a model for the source formula whenever the target formula turns out to be satisfiable.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LO",
      "03B25, 26A06"
    ],
    "url": "http://arxiv.org/abs/2507.02742v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02718v1",
    "title": "A Systematic Search for Spectral Hardening in Blazar Flares with the Fermi-Large Area Telescope",
    "authors": [
      "Adithiya Dinesh",
      "Alberto Dominguez",
      "V. Paliya",
      "J. L. Contreras",
      "S. Buson",
      "M. Ajello"
    ],
    "abstract": "Blazars are a subclass of active galactic nuclei (AGN) that emit non-thermal radiation through relativistic jets, characterized by rapid flux and polarization variability. Extreme high synchrotron-peaked blazars (EHSPs), with synchrotron peaks exceeding 10$^{17}$ Hz, are essential for understanding the full range of blazar phenomena and testing jet physics models. However, the number of known extreme blazars is small, so this class of objects remains poorly studied. This work aims to systematically identify and characterize the most extreme $\\gamma$-ray blazars using data from the Large Area Telescope (LAT) on board the Fermi Gamma-ray Space Telescope. The focus is on spectral hardening, where the $\\gamma$-ray spectrum becomes harder at higher energies, particularly during flaring episodes. This represents the first dedicated analysis of spectral hardening across a population of EHSPs, as previous studies explored it only in individual sources. We analyze 138 blazars selected from the 4FGL-DR2 catalog with high synchrotron peak frequencies and well-sampled light curves. Flaring periods are automatically identified, and each flare is analyzed, with the significance of spectral hardening assessed through a test statistic based on the likelihood ratio of two spectral models. We identify two flaring episodes with indications of spectral hardening, in 4FGL J0238.4$-$3116 and PKS 2155$-$304, the latter detected independently by both methods but referring to the same period. These events are consistent with expectations from statistical fluctuations, suggesting that spectral hardening is a rare occurrence (< 0.1 %). These results constrain its frequency and support a smoothly varying power-law blazar emission model, motivating future multi-wavelength studies to clarify whether these rare flares reflect distinct physical processes within blazar jets.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.HE"
    ],
    "url": "http://arxiv.org/abs/2507.02718v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02709v1",
    "title": "XPPLORE: Import, visualize, and analyze XPPAUT data in MATLAB",
    "authors": [
      "Matteo Martin",
      "Anna Kishida Thomas",
      "George Bard Ermentrout"
    ],
    "abstract": "The analysis of ordinary differential equation (ODE) dynamical systems, particularly in applied disciplines such as mathematical biology and neuroscience, often requires flexible computational workflows tailored to model-specific questions. XPPAUT is a widely used tool combining numerical integration and continuation methods. Various XPPAUT toolboxes have emerged to customize analyses, however, they typically rely on summary '.dat' files and cannot parse the more informative '.auto' files, which contain detailed continuation data, e.g. periodic orbits and boundary value problem solutions. We present XPPLORE, a user-friendly and structured MATLAB toolbox overcoming this limitation through the handling of '.auto' files. This free software enables post-processing of continuation results, facilitates analyses such as manifold reconstruction and averaging, and it supports the creation of high-quality visualizations suitable for scientific publications. This paper introduces the core data structures of XPPLORE and demonstrates the software's exploration capabilities, highlighting its value as a customizable and accessible extension for researchers working with ODE-based dynamical systems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.DS"
    ],
    "url": "http://arxiv.org/abs/2507.02709v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02699v1",
    "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents",
    "authors": [
      "Jiangrong Wu",
      "Yuhong Nan",
      "Jianliang Wu",
      "Zitong Yao",
      "Zibin Zheng"
    ],
    "abstract": "The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.   In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.   To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR"
    ],
    "url": "http://arxiv.org/abs/2507.02699v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02695v1",
    "title": "Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms",
    "authors": [
      "Sahar Ahmadisakha",
      "Lech Bialek",
      "Mohamed Soliman",
      "Vasilios Andrikopoulos"
    ],
    "abstract": "In recent years, sustainability in software systems has gained significant attention, especially with the rise of cloud computing and the shift towards cloud-based architectures. This shift has intensified the need to identify sustainability in architectural discussions to take informed architectural decisions. One source to see these decisions is in online Q&A forums among practitioners' discussions. However, recognizing sustainability concepts within software practitioners' discussions remains challenging due to the lack of clear and distinct guidelines for this task. To address this issue, we introduce the notion of sustainability flags as pointers in relevant discussions, developed through thematic analysis of multiple sustainability best practices from cloud providers. This study further evaluates the effectiveness of these flags in identifying sustainability within cloud architecture posts, using a controlled experiment. Preliminary results suggest that the use of flags results in classifying fewer posts as sustainability-related compared to a control group, with moderately higher certainty and significantly improved performance. Moreover, sustainability flags are perceived as more useful and understandable than relying solely on definitions for identifying sustainability.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE"
    ],
    "url": "http://arxiv.org/abs/2507.02695v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02694v1",
    "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers",
    "authors": [
      "Zhijian Xu",
      "Yilun Zhao",
      "Manasi Patwardhan",
      "Lovekesh Vig",
      "Arman Cohan"
    ],
    "abstract": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL"
    ],
    "url": "http://arxiv.org/abs/2507.02694v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02685v1",
    "title": "Axion USR Inflation",
    "authors": [
      "Alireza Talebian",
      "Hassan Firouzjahi"
    ],
    "abstract": "We consider a model of inflation in which the inflaton field is a rolling axion with a potential which is flat enough to support an intermediate phase of USR inflation. Because of the Chern-Simons interaction, one polarization of the gauge field experiences the tachyonic growth during the first slow-roll stage, inducing large corrections in curvature perturbations via the inverse decay effect. A non-trivial feature of our setup is that once the system enters the USR phase, the instability parameter falls off rapidly, terminating the gauge field production efficiently. Consequently, the power spectrum involves two separate peaks, the first peak is induced by the gauge field particles production while the second peak is due to standard USR mechanism. We show that the power spectrum at the intermediate scales develops strong scale-dependence $\\propto k^m$ with the index $m >4$. Calculating the bispectrum, we demonstrate that non-Gaussianities with non-trivial shapes and multiple peaks are generated in this setup.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.CO",
      "gr-qc",
      "hep-ph",
      "hep-th"
    ],
    "url": "http://arxiv.org/abs/2507.02685v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02683v1",
    "title": "Fractional order entropy-based decision-making models under risk",
    "authors": [
      "Poulami Paul",
      "Chanchal Kundu"
    ],
    "abstract": "The construction of an efficient portfolio with a good level of return and minimal risk depends on selecting the optimal combination of stocks. This paper introduces a novel decision-making framework for stock selection based on fractional order entropy due to Ubriaco. By tuning the fractional parameter, the model captures varying attitudes of individuals toward risk. Values of fractional parameter near one indicate high risk tolerance (adventurous attitude), while those near zero reflect risk aversion (conservative attitude). The sensitivity of the fractional order entropy to changing risk preferences of decision makers is demonstrated through four real world portfolio models, namely, large cap, mid cap, diversified, and hypothetical. Furthermore, two new risk measures, termed as expected utility fractional entropy (EU FE) and expected utility fractional entropy and variance (EU FEV), are introduced to develop decision models aligned with investors risk preferences. The effectiveness of the decision model is further tested with financial stock market data of PSI index by finding efficient frontiers of portfolio with the aid of artificial neural network.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.ST",
      "stat.TH",
      "Primary 94A17, Secondary 62B10"
    ],
    "url": "http://arxiv.org/abs/2507.02683v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02682v1",
    "title": "A wireless, inexpensive optical tracker for the CAVE",
    "authors": [
      "Ehud Sharlin",
      "Pablo Figueroa",
      "Mark Green",
      "Benjamin Watson"
    ],
    "abstract": "CAVE displays offer many advantages over other virtual reality (VR) displays, including a large, unencumbering viewing space. Unfortunately, the typical tracking subsystems used with CAVE displays tether the user and lessen this advantage. We have designed a simple, low-cost feet tracker that is wireless, leaving the user free to move. The tracker can be assembled for less than $200 US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested the prototype with two applications: a visualization supporting close visual inspection, and a walkthrough of the campus. Although the tracking was convincing, it was clear that the tracker's limitations make it less than ideal for applications requiring precise visual inspection. However, the freedom of motion allowed by the tracker was a compelling supplement to our campus walkthrough, allowing users to stroll and look around corners.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.ET"
    ],
    "url": "http://arxiv.org/abs/2507.02682v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02640v1",
    "title": "Two-Sample Covariance Inference in High-Dimensional Elliptical Models",
    "authors": [
      "Nina Dörnemann"
    ],
    "abstract": "We propose a two-sample test for large-dimensional covariance matrices in generalized elliptical models. The test statistic is based on a U-statistic estimator of the squared Frobenius norm of the difference between the two population covariance matrices. This statistic was originally introduced by Li and Chen (2012, AoS) for the independent component model. As a key theoretical contribution, we establish a new central limit theorem for the U-statistics under elliptical data, valid under both the null and alternative hypotheses. This result enables asymptotic control of the test level and facilitates a power analysis. To the best of our knowledge, the proposed test is the first such method to be supported by theoretical guarantees for elliptical data. Our approach imposes only mild assumptions on the covariance matrices and does neither require sparsity nor explicit growth conditions on the dimension-to-sample-size ratio. We illustrate our theoretical findings through applications to both synthetic and real-world data.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.ST",
      "math.PR",
      "stat.TH"
    ],
    "url": "http://arxiv.org/abs/2507.02640v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02630v1",
    "title": "Insights into Chromospheric Large-Scale Flows using Nobeyama 17 GHz Radio Observations I. The Differential Rotation Profile",
    "authors": [
      "Srinjana Routh",
      "Anshu Kumari",
      "Vaibhav Pant",
      "Jaydeep Kandekar",
      "Dipankar Banerjee",
      "Mohd. Saleem Khan",
      "Dibya Kirti Mishra"
    ],
    "abstract": "Although the differential rotation rate on the solar surface has long been studied using optical and extreme ultraviolet (EUV) observations, associating these measurements to specific atmospheric heights remains challenging due to the temperature-dependent emission of tracers observed in EUV wavelengths. Radio observations, being primarily influenced by coherent plasma processes and/or thermal bremsstrahlung, offer a more height-stable diagnostic and thus provide an independent means to test and validate rotational trends observed at other EUV wavelengths. We aim to characterize the differential rotation profile of the upper chromosphere using cleaned solar full-disc 17 GHz radio imaging from the Nobeyama Radioheliograph (NoRH), spanning a little over two solar cycles (1992 - 2020). A tracer-independent method based on automated image correlation was employed on daily full-disc 17 GHz radio maps. Our results suggest that the upper chromosphere rotates significantly faster than the photosphere at all latitudes, with a relatively flatter latitudinal profile. A very weak anti-correlation between the equatorial rotation rate and solar activity is also observed. Our findings reaffirm the potential of radio observations to probe the dynamics of the solar chromosphere with reduced height ambiguity. The overlap of the equatorial rotation rate found in this study with that for $304$ {\\AA} in the EUV regime lends additional support to the view that the equatorial rotation rates increase with height above the photosphere. Future coordinated studies at wavelengths with better-constrained height formation will be crucial for further understanding the complex dynamics of the solar atmosphere.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.SR"
    ],
    "url": "http://arxiv.org/abs/2507.02630v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02612v1",
    "title": "Mechanical enhancement of quantum oscillations",
    "authors": [
      "Maximilian Daschner",
      "Ivan Kokanović",
      "F. Malte Grosche"
    ],
    "abstract": "We investigate quantum oscillation measurements in the Dirac nodal-line semimetal TaNiTe$_5$ which exhibit a strongly enhanced amplitude in the magnetoresistance. We show that mechanical properties of the measurement setup in combination with de Haas - van Alphen oscillations in the magnetic torque can cause this enhancement in the measured resistance, without involvement of any topological properties in this material. To support the empirical data, a numerical model is provided, showing good agreement.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mtrl-sci"
    ],
    "url": "http://arxiv.org/abs/2507.02612v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02602v1",
    "title": "Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development",
    "authors": [
      "Riccardo Gallon",
      "Fabian Schiemenz",
      "Alessandra Menicucci",
      "Eberhard Gill"
    ],
    "abstract": "The increasing importance of Vision-Based Navigation (VBN) algorithms in space missions raises numerous challenges in ensuring their reliability and operational robustness. Sensor faults can lead to inaccurate outputs from navigation algorithms or even complete data processing faults, potentially compromising mission objectives. Artificial Intelligence (AI) offers a powerful solution for detecting such faults, overcoming many of the limitations associated with traditional fault detection methods. However, the primary obstacle to the adoption of AI in this context is the lack of sufficient and representative datasets containing faulty image data.   This study addresses these challenges by focusing on an interplanetary exploration mission scenario. A comprehensive analysis of potential fault cases in camera sensors used within the VBN pipeline is presented. The causes and effects of these faults are systematically characterized, including their impact on image quality and navigation algorithm performance, as well as commonly employed mitigation strategies. To support this analysis, a simulation framework is introduced to recreate faulty conditions in synthetically generated images, enabling a systematic and controlled reproduction of faulty data. The resulting dataset of fault-injected images provides a valuable tool for training and testing AI-based fault detection algorithms. The final link to the dataset will be added after an embargo period. For peer-reviewers, this private link is available.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02602v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02582v1",
    "title": "Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms",
    "authors": [
      "Junli Jiang",
      "Pavel Naumov"
    ],
    "abstract": "Responsibility has long been a subject of study in law and philosophy. More recently, it became a focus of AI literature. The article investigates the computational complexity of two important properties of responsibility in collective decision-making: diffusion and gap. It shows that the sets of diffusion-free and gap-free decision-making mechanisms are $\\Pi_2$-complete and $\\Pi_3$-complete, respectively. At the same time, the intersection of these classes is $\\Pi_2$-complete.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02582v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02560v1",
    "title": "Tertiary Education Completion and Financial Aid Assistance: Evidence from an Information Experiment",
    "authors": [
      "Luca Bonacini",
      "Giuseppe Pignataro",
      "Veronica Rattini"
    ],
    "abstract": "Understanding the role of information among disadvantaged students is crucial in explaining their investment decisions in higher education. Indeed, information barriers on the returns and the gains from completing college may explain a substantial share of variation in students' degree completion. We conduct a field experiment with 7,806 university students in Italy who benefit from financial aid assistance, by providing information, either on the labor market returns of completing college or on the education returns of meeting the academic requirements attached to the financial aid. Our results suggest that only the latter information treatment has a positive effect on academic performance, increasing the number of credits obtained by around 3, and by decreasing the probability of dropout by around 4 percentage points. We also find that the results are mediated by an aspiration lift generated by our treatment.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "econ.GN",
      "q-fin.EC"
    ],
    "url": "http://arxiv.org/abs/2507.02560v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02552v1",
    "title": "Covariance scanning for adaptively optimal change point detection in high-dimensional linear models",
    "authors": [
      "Haeran Cho",
      "Housen Li"
    ],
    "abstract": "This paper investigates the detection and estimation of a single change in high-dimensional linear models. We derive minimax lower bounds for the detection boundary and the estimation rate, which uncover a phase transition governed the sparsity of the covariance-weighted differential parameter. This form of \"inherent sparsity\" captures a delicate interplay between the covariance structure of the regressors and the change in regression coefficients on the detectability of a change point. Complementing the lower bounds, we introduce two covariance scanning-based methods, McScan and QcSan, which achieve minimax optimal performance (up to possible logarithmic factors) in the sparse and the dense regimes, respectively. In particular, QcScan is the first method shown to achieve consistency in the dense regime and further, we devise a combined procedure which is adaptively minimax optimal across sparse and dense regimes without the knowledge of the sparsity. Computationally, covariance scanning-based methods avoid costly computation of Lasso-type estimators and attain worst-case computation complexity that is linear in the dimension and sample size. Additionally, we consider the post-detection estimation of the differential parameter and the refinement of the change point estimator. Simulation studies support the theoretical findings and demonstrate the computational and statistical efficiency of the proposed covariance scanning methods.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "url": "http://arxiv.org/abs/2507.02552v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02536v1",
    "title": "Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain",
    "authors": [
      "Azmat Ullah",
      "Maria Ilaria Lunesu",
      "Lodovica Marchesi",
      "Roberto Tonelli"
    ],
    "abstract": "This paper presents a blockchain-based Internet of Things (IoT) system for monitoring pizza production in restaurants. IoT devices track temperature and humidity in real-time, while blockchain ensures secure and tamper-proof data. A Raspberry Pi processes sensor data, captures images, triggers alerts, and interacts with smart contracts. The system detects abnormal conditions, enabling quick responses. Blockchain adds transparency and traceability, supporting compliance and audits. Experiments show improved ingredient management, reduced waste, and increased kitchen efficiency.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR",
      "cs.ET"
    ],
    "url": "http://arxiv.org/abs/2507.02536v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02533v1",
    "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models",
    "authors": [
      "Miguel Romero-Arjona",
      "José A. Parejo",
      "Juan C. Alonso",
      "Ana B. Sánchez",
      "Aitor Arrieta",
      "Sergio Segura"
    ],
    "abstract": "Fairness--the absence of unjustified bias--is a core principle in the development of Artificial Intelligence (AI) systems, yet it remains difficult to assess and enforce. Current approaches to fairness testing in large language models (LLMs) often rely on manual evaluation, fixed templates, deterministic heuristics, and curated datasets, making them resource-intensive and difficult to scale. This work aims to lay the groundwork for a novel, automated method for testing fairness in LLMs, reducing the dependence on domain-specific resources and broadening the applicability of current approaches. Our approach, Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to uncover bias by examining how model outputs vary in response to controlled modifications of input prompts, defined by metamorphic relations (MRs). Second, we propose exploiting the potential of LLMs for both test case generation and output evaluation, leveraging their capability to generate diverse inputs and classify outputs effectively. The proposal is complemented by three open-source tools supporting LLM-driven generation, execution, and evaluation of test cases. We report the findings of several experiments involving 12 pre-trained LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases. The results show that Meta-Fair is effective in uncovering bias in LLMs, achieving an average precision of 92% and revealing biased behaviour in 29% of executions. Additionally, LLMs prove to be reliable and consistent evaluators, with the best-performing models achieving F1-scores of up to 0.79. Although non-determinism affects consistency, these effects can be mitigated through careful MR design. While challenges remain to ensure broader applicability, the results indicate a promising path towards an unprecedented level of automation in LLM testing.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.SE"
    ],
    "url": "http://arxiv.org/abs/2507.02533v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02532v1",
    "title": "Robust feedback-based quantum optimization: analysis of coherent control errors",
    "authors": [
      "Mirko Legnini",
      "Julian Berberich"
    ],
    "abstract": "The Feedback-based Algorithm for Quantum Optimization (FALQON) is a Lyapunov inspired quantum algorithm proposed to tackle combinatorial optimization problems. In this paper, we examine the robustness of FALQON against coherent control errors, a class of multiplicative errors that affect the control input. We show that the algorithm is asymptotically robust with respect to systematic errors, and we derive robustness bounds for independent errors. Finally, we propose a robust version of FALQON which minimizes a regularized Lyapunov function. Our theoretical results are supported through simulations.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.SY",
      "cs.SY"
    ],
    "url": "http://arxiv.org/abs/2507.02532v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02504v1",
    "title": "Strategies and statistical evaluation of Italy's regional model for COVID-19 restrictions",
    "authors": [
      "Giuseppe Drago",
      "Giulia Marcon",
      "Alberto Lombardo",
      "Giuseppe Aiello"
    ],
    "abstract": "This study presents a comprehensive assessment of the Italian risk model used during the COVID-19 pandemic to guide regional mobility restrictions through a colour-coded classification system. The research focuses on evaluating the variables selected by the Italian Ministry of Health for this purpose and their effectiveness in supporting public health decision-making. The analysis adopts a statistical framework which combines data reduction and regression modelling techniques to enhance interpretability and predictive accuracy. Dimensionality reduction is applied to address multicollinearity and simplify complex variable structures, while an ordinal regression model is employed to investigate the relationship between the reduced set of variables and the colour regional classifications. Model performance is evaluated using classification error metrics, providing insights into the adequacy of the selected variables in explaining the decision-making process. Results reveal significant redundancy within the variables chosen by the Italian Ministry of Health, suggesting that excessive predictors may compromise information. To address this, the study proposes refined and robust predictive models for regional classification, offering a reliable tool of the proposed framework and to support public health decision-makers. This study contributes to the ongoing development of quantitative methodologies aimed at improving the effectiveness of statistical models in guiding public health policies. The findings offer valuable insights for refining data-driven decision-making processes during health crises and improving the quality of information available to policymakers.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.AP",
      "physics.soc-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02504v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02500v1",
    "title": "Goal-oriented optimal sensor placement for PDE-constrained inverse problems in crisis management",
    "authors": [
      "Marco Mattuschka",
      "Noah An der Lan",
      "Max von Danwitz",
      "Daniel Wolff",
      "Alexander Popp"
    ],
    "abstract": "This paper presents a novel framework for goal-oriented optimal static sensor placement and dynamic sensor steering in PDE-constrained inverse problems, utilizing a Bayesian approach accelerated by low-rank approximations. The framework is applied to airborne contaminant tracking, extending recent dynamic sensor steering methods to complex geometries for computational efficiency. A C-optimal design criterion is employed to strategically place sensors, minimizing uncertainty in predictions. Numerical experiments validate the approach's effectiveness for source identification and monitoring, highlighting its potential for real-time decision-making in crisis management scenarios.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.NA",
      "cs.NA"
    ],
    "url": "http://arxiv.org/abs/2507.02500v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02491v1",
    "title": "Engineering an LTLf Synthesis Tool",
    "authors": [
      "Alexandre Duret-Lutz",
      "Shufang Zhu",
      "Nir Piterman",
      "Giuseppe de Giacomo",
      "Moshe Y Vardi"
    ],
    "abstract": "The problem of LTLf reactive synthesis is to build a transducer, whose output is based on a history of inputs, such that, for every infinite sequence of inputs, the conjoint evolution of the inputs and outputs has a prefix that satisfies a given LTLf specification. We describe the implementation of an LTLf synthesizer that outperforms existing tools on our benchmark suite. This is based on a new, direct translation from LTLf to a DFA represented as an array of Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based representation can be interpreted directly as a reachability game that is solved on-the-fly during its construction.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.FL"
    ],
    "url": "http://arxiv.org/abs/2507.02491v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02482v1",
    "title": "Curvature Rigidity Through Level Sets of Lyapunov Exponents in Geodesic Flows",
    "authors": [
      "Sergio Romaña"
    ],
    "abstract": "In this paper, we establish new geometric rigidity results through the study of Lyapunov exponent level sets via invariant measures. First, we prove that for a manifold $M$ without focal points, if the zero Lyapunov exponent level set has full measure with respect to a fully supported invariant measure, then $M$ must be flat. This result recovers and extends a result of Freire and Ma\\~n\\'e (cf. [9]). Second, we prove that if the level set of the Lyapunov exponents has full measure with respect to some fully supported measure, then the sectional curvature must be constant. This advances the resolution of Conjecture 1 in [17]. Furthermore, we establish curvature relationships between manifolds with $1$-equivalent geodesic flows, yielding a new criterion to obstruct smooth conjugacy for flows on manifolds without conjugate points. Our techniques provide unified proofs for all rigidity results in [17] as corollaries, and additionally yield rigidity theorems for totally geodesic submanifolds in settings without conjugate points. Notably, several key results hold without compactness assumptions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "math.DS",
      "37D40, 53C20"
    ],
    "url": "http://arxiv.org/abs/2507.02482v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02474v1",
    "title": "Optimisation of amplification and gas mixture for directional Dark Matter searches with the CYGNO/INITIUM project",
    "authors": [
      "Giorgio Dho"
    ],
    "abstract": "Astrophysical and cosmological observations suggest the existence of beyond standard model ingredient known as dark matter (DM). One of the most supported class of theories suggests that DM is composed of weakly interactive massive particles (WIMPs), possibly detectable via weak interaction with standard matter resulting in the recoil of the latter. The motion of the Sun and Earth with respect to the Galactic Centre is expected to induce a strong directional dependence in the recoil spectrum. Direct detection experiments capable of measuring the angular features of the recoils gain access to a wide range of advantages such as the possibility to positively claim a discovery of DM. The CYGNO project sets into this context, with the aim of deploying a large directional detector for rare event searches as DM. It exploits a gaseous time projection chamber filled with a He:CF4 gas mixture with a segmented amplification stage and granular optical readout. In this thesis, it is presented the work carried out with small CYGNO prototypes to maximise the light yield without degrading spatial and energy resolution, the addition of highly electronegative gases to induce a reduction of the electron cloud diffusion while drifting towards the amplification stage, very relevant to precisely measure the topological information of the recoil tracks. Moreover, the potential performances of directional detectors in the context of a direct DM search are analysed with the use of rigorous statistical tools both in the improvement in setting limits in the WIMP to nucleon sensitivity and in the capability of discerning two different DM models exploiting a directional detector.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.ins-det",
      "astro-ph.IM"
    ],
    "url": "http://arxiv.org/abs/2507.02474v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02453v1",
    "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?",
    "authors": [
      "Jueun Lee",
      "Martin Flipe",
      "Philipp Lepold",
      "Tobias Röddiger",
      "Michael Beigl"
    ],
    "abstract": "Wearable haptic interventions offer promising support for relaxation through slow, vibrotactile biofeedback. Despite their potential, current applications focus on stress-inducing procedures and fixed vibration patterns, with limited consideration of body location and dynamic biofeedback during restful states. This study investigates the effects of haptic biofeedback adjusted from real-time heart rate during eyes-closed wakeful rest, comparing four wearable body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave activity on the ear, subjective restfulness, and vibration experience were measured across these conditions. Results show that biofeedback reduced heart rate at the wrist, shoulder, and forearm, while alpha power measured at the ear remained unchanged. Subjective restfulness was rated highest at the shoulder and forearm, which were also the most preferred locations. In addition, participants reported greater comfort, relaxation, and further increased sleepiness at the forearm compared to the wrist, which was more easily recognizable. These findings suggest that the forearm and shoulder are ideal for unobtrusive relaxation feedback for wakeful rest, while the wrist may require design improvements for subjective experience.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2507.02453v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02438v1",
    "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints",
    "authors": [
      "Shivam Chaubey",
      "Francesco Verdoja",
      "Shankar Deka",
      "Ville Kyrki"
    ],
    "abstract": "Shared control combines human intention with autonomous decision-making, from low-level safety overrides to high-level task guidance, enabling systems that adapt to users while ensuring safety and performance. This enhances task effectiveness and user experience across domains such as assistive robotics, teleoperation, and autonomous driving. However, existing shared control methods, based on e.g. Model Predictive Control, Control Barrier Functions, or learning-based control, struggle with feasibility, scalability, or safety guarantees, particularly since the user input is unpredictable.   To address these challenges, we propose an assistive controller framework based on Constrained Optimal Control Problem that incorporates an offline-computed Control Invariant Set, enabling online computation of control actions that ensure feasibility, strict constraint satisfaction, and minimal override of user intent. Moreover, the framework can accommodate structured class of non-convex constraints, which are common in real-world scenarios. We validate the approach through a large-scale user study with 66 participants--one of the most extensive in shared control research--using a computer game environment to assess task load, trust, and perceived control, in addition to performance. The results show consistent improvements across all these aspects without compromising safety and user intent.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO",
      "cs.HC",
      "cs.SY",
      "eess.SY"
    ],
    "url": "http://arxiv.org/abs/2507.02438v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02432v1",
    "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset",
    "authors": [
      "Jueun Lee",
      "Dennis Moschina",
      "Supraja Ramesh",
      "Tobias Röddiger",
      "Kai Kunze",
      "Michael Beigl"
    ],
    "abstract": "We investigate the use of musically structured, closed-loop vibration patterns as a passive biofeedback intervention for relaxation and sleep initiation. By encoding rhythmic meter structures into smartwatch vibrations and adapting their frequency to be slightly slower than the user's real-time heart rate, our system aims to reduce arousal through tactile entrainment, offering a non-invasive alternative to auditory or open-loop approaches previously used in sleep and anxiety contexts. In the first study (N=20), we compared five adaptive vibration rhythms for their effects on heart rate and subjective perceptions of relaxation in a resting context. In the second study (N=28), we evaluated the most promising pattern from Study 1 in a prolonged sleep initiation setting. Results showed increased parasympathetic activity and perceived relaxation during short-term stimulation, but no significant effects on sleep-related measures during the sleep onset phase. This work contributes to the understanding of how wearable haptic feedback can support relaxation and sleep, offering design insights and identifying methodological considerations for effectively integrating haptic interaction into self-directed interventions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2507.02432v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02425v1",
    "title": "Dealing with separation problem in hidden Markov models with covariates based on a penalized maximum likelihood approach",
    "authors": [
      "Luca Brusa",
      "Fulvia Pennoni",
      "Francesco Bartolucci",
      "Romina Peruilh Bagolini"
    ],
    "abstract": "A penalized maximum likelihood estimation approach is proposed for discrete-time hidden Markov models where covariates affect the observed responses and serial dependence is considered. The proposed penalized maximum likelihood method addresses the issue of latent state separation that typically occurs when this model is applied to binary and categorical response variables with a limited number of categories, resulting in extremely large estimates of the support points of the latent variable assumed with a discrete, left unspecified distribution. We also propose a cross-validation approach for jointly selecting the number of hidden states and the roughness of the penalty term. The proposal is illustrated through a simulation study comparing parameter estimation accuracy and computational efficiency across different estimation procedures. We also demonstrate the potential of this class of models through the analysis of longitudinal data collected during spinal anesthesia to monitor the occurrence of hypotension in patients, and we compare the results with those obtained from other standard models.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.ME"
    ],
    "url": "http://arxiv.org/abs/2507.02425v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02424v1",
    "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool",
    "authors": [
      "Francesco Blefari",
      "Cristian Cosentino",
      "Francesco Aurelio Pironti",
      "Angelo Furfaro",
      "Fabrizio Marozzo"
    ],
    "abstract": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming security analysts with logs that demand deep, rapidly evolving domain expertise. Conventional machine-learning detectors trim the alert volume but still yield high false-positive rates, while standard single-pass Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify their predictions. To overcome these shortcomings, we present CyberRAG, a modular, agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to a distinct attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that continuously queries a domain-specific knowledge base until the evidence is both relevant and self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic design that enables dynamic control flow and adaptive reasoning. This agent-centric architecture refines its threat labels and natural-language justifications autonomously, reducing false positives and enhancing interpretability. The framework is fully extensible: new attack types can be supported by simply adding a classifier without retraining the core agent. CyberRAG has been evaluated achieving over 94% accuracy per class and pushing final classification accuracy to 94.92% through semantic orchestration. Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation. These results show that agentic, specialist-oriented RAG can pair high detection accuracy with trustworthy, SOC-ready prose, offering a practical and scalable path toward semi-autonomous cyber-defence workflows.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02424v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02411v1",
    "title": "3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices",
    "authors": [
      "Zhurong Chen",
      "Jinhua Chen",
      "Wei Zhuo",
      "Wufeng Xue",
      "Dong Ni"
    ],
    "abstract": "Echocardiography (echo) plays an indispensable role in the clinical practice of heart diseases. However, ultrasound imaging typically provides only two-dimensional (2D) cross-sectional images from a few specific views, making it challenging to interpret and inaccurate for estimation of clinical parameters like the volume of left ventricle (LV). 3D ultrasound imaging provides an alternative for 3D quantification, but is still limited by the low spatial and temporal resolution and the highly demanding manual delineation.   To address these challenges, we propose an innovative framework for reconstructing personalized 3D heart anatomy from 2D echo slices that are frequently used in clinical practice. Specifically, a novel 3D reconstruction pipeline is designed, which alternatively optimizes between the 3D pose estimation of these 2D slices and the 3D integration of these slices using an implicit neural network, progressively transforming a prior 3D heart shape into a personalized 3D heart model.   We validate the method with two datasets. When six planes are used, the reconstructed 3D heart can lead to a significant improvement for LV volume estimation over the bi-plane method (error in percent: 1.98\\% VS. 20.24\\%). In addition, the whole reconstruction framework makes even an important breakthrough that can estimate RV volume from 2D echo slices (with an error of 5.75\\% ). This study provides a new way for personalized 3D structure and function analysis from cardiac ultrasound and is of great potential in clinical practice.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.IV",
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02411v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02404v1",
    "title": "Alps, a versatile research infrastructure",
    "authors": [
      "Maxime Martinasso",
      "Mark Klein",
      "Thomas C. Schulthess"
    ],
    "abstract": "The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition of delivering top-tier high-performance computing systems, exemplified by the Piz Daint supercomputer. However, the increasing diversity of scientific needs has exposed limitations in traditional vertically integrated HPC architectures, which often lack flexibility and composability. To address these challenges, CSCS developed Alps, a next-generation HPC infrastructure designed with a transformative principle: resources operate as independent endpoints within a high-speed network. This architecture enables the creation of independent tenant-specific and platform-specific services, tailored to diverse scientific requirements.   Alps incorporates heterogeneous hardware, including CPUs and GPUs, interconnected by a high-performance Slingshot network, and offers a modular storage system. A key innovation is the versatile software-defined cluster (vCluster) technology, which bridges cloud and HPC paradigms. By abstracting infrastructure, service management, and user environments into distinct layers, vClusters allow for customized platforms that support diverse workloads. Current platforms on Alps serve various scientific domains, including numerical weather prediction, and AI research.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DC"
    ],
    "url": "http://arxiv.org/abs/2507.02404v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02394v1",
    "title": "On the Adversarial Robustness of Online Importance Sampling",
    "authors": [
      "Yotam Kenneth-Mordoch",
      "Shay Sapir"
    ],
    "abstract": "This paper studies the adversarial-robustness of importance-sampling (aka sensitivity sampling); a useful algorithmic technique that samples elements with probabilities proportional to some measure of their importance. A streaming or online algorithm is called adversarially-robust if it succeeds with high probability on input streams that may change adaptively depending on previous algorithm outputs. Unfortunately, the dependence between stream elements breaks the analysis of most randomized algorithms, and in particular that of importance-sampling algorithms. Previously, Braverman et al. [NeurIPS 2021] suggested that streaming algorithms based on importance-sampling may be adversarially-robust; however, they proved it only for well-behaved inputs.   We focus on the adversarial-robustness of online importance-sampling, a natural variant where sampling decisions are irrevocable and made as data arrives. Our main technical result shows that, given as input an adaptive stream of elements $x_1,\\ldots,x_T\\in \\mathbb{R}_+$, online importance-sampling maintains a $(1\\pm\\epsilon)$-approximation of their sum while matching (up to lower order terms) the storage guarantees of the oblivious (non-adaptive) case. We then apply this result to develop adversarially-robust online algorithms for two fundamental problems: hypergraph cut sparsification and $\\ell_p$ subspace embedding.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DS"
    ],
    "url": "http://arxiv.org/abs/2507.02394v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02379v1",
    "title": "An AI-native experimental laboratory for autonomous biomolecular engineering",
    "authors": [
      "Mingyu Wu",
      "Zhaoguo Wang",
      "Jiabin Wang",
      "Zhiyuan Dong",
      "Jingkai Yang",
      "Qingting Li",
      "Tianyu Huang",
      "Lei Zhao",
      "Mingqiang Li",
      "Fei Wang",
      "Chunhai Fan",
      "Haibo Chen"
    ],
    "abstract": "Autonomous scientific research, capable of independently conducting complex experiments and serving non-specialists, represents a long-held aspiration. Achieving it requires a fundamental paradigm shift driven by artificial intelligence (AI). While autonomous experimental systems are emerging, they remain confined to areas featuring singular objectives and well-defined, simple experimental workflows, such as chemical synthesis and catalysis. We present an AI-native autonomous laboratory, targeting highly complex scientific experiments for applications like autonomous biomolecular engineering. This system autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. Founded on a co-design philosophy of models, experiments, and instruments, the platform supports the co-evolution of AI models and the automation system. This establishes an end-to-end, multi-user autonomous laboratory that handles complex, multi-objective experiments across diverse instrumentation. Our autonomous laboratory supports fundamental nucleic acid functions-including synthesis, transcription, amplification, and sequencing. It also enables applications in fields such as disease diagnostics, drug development, and information storage. Without human intervention, it autonomously optimizes experimental performance to match state-of-the-art results achieved by human scientists. In multi-user scenarios, the platform significantly improves instrument utilization and experimental efficiency. This platform paves the way for advanced biomaterials research to overcome dependencies on experts and resource barriers, establishing a blueprint for science-as-a-service at scale.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI",
      "q-bio.BM"
    ],
    "url": "http://arxiv.org/abs/2507.02379v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02371v1",
    "title": "Time- and Polarization-Resolved Extreme Ultraviolet Momentum Microscopy",
    "authors": [
      "Sotirios Fragkos",
      "Quentin Courtade",
      "Olena Tkach",
      "Jérôme Gaudin",
      "Dominique Descamps",
      "Guillaume Barrette",
      "Stéphane Petit",
      "Gerd Schönhense",
      "Yann Mairesse",
      "Samuel Beaulieu"
    ],
    "abstract": "We report the development of an instrument combining an ultrafast, high-repetition-rate, polarization-tunable monochromatic extreme ultraviolet (XUV, 21.6 eV) beamline and a next-generation momentum microscope endstation. This setup enables time- and angle-resolved photoemission spectroscopy of quantum materials, offering multimodal photoemission dichroism capabilities. The momentum microscope simultaneously detects the full surface Brillouin zone over an extended binding energy range. It is equipped with advanced electron optics, including a new type of front lens that supports multiple operational modes. Enhanced spatial resolution is achieved by combining the small XUV beam footprint (33 $\\mu$m by 45 $\\mu$m) with the selection of small regions of interest using apertures positioned in the Gaussian plane of the momentum microscope. This instrument achieves an energy resolution of 44 meV and a temporal resolution of 144 fs. We demonstrate the capability to perform linear, Fourier, and circular dichroism in photoelectron angular distributions from photoexcited 2D materials. This functionality paves the way for time-, energy-, and momentum-resolved investigations of orbital and quantum geometrical properties underlying electronic structures of quantum materials driven out of equilibrium.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "url": "http://arxiv.org/abs/2507.02371v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02365v1",
    "title": "Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations",
    "authors": [
      "Muhammad Usama",
      "Dong Eui Chang"
    ],
    "abstract": "Equalizer parameter optimization for signal integrity in high-speed Dynamic Random Access Memory systems is crucial but often computationally demanding or model-reliant. This paper introduces a data-driven framework employing learned latent signal representations for efficient signal integrity evaluation, coupled with a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization. The latent representation captures vital signal integrity features, offering a fast alternative to direct eye diagram analysis during optimization, while the reinforcement learning agent derives optimal equalizer settings without explicit system models. Applied to industry-standard Dynamic Random Access Memory waveforms, the method achieved significant eye-opening window area improvements: 42.7\\% for cascaded Continuous-Time Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\\% for Decision Feedback Equalizer-only configurations. These results demonstrate superior performance, computational efficiency, and robust generalization across diverse Dynamic Random Access Memory units compared to existing techniques. Core contributions include an efficient latent signal integrity metric for optimization, a robust model-free reinforcement learning strategy, and validated superior performance for complex equalizer architectures.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02365v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02364v1",
    "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers",
    "authors": [
      "Pilsung Kang"
    ],
    "abstract": "Parameterized quantum circuits (PQCs) have recently emerged as promising components for enhancing the expressibility of neural architectures. In this work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the feedforward network (FFN) modules of a compact BERT variant are replaced by PQC-based layers. This design is motivated by the dominant parameter contribution of FFNs, which account for approximately two-thirds of the parameters within standard Transformer encoder blocks. While prior studies have primarily integrated PQCs into self-attention modules, our work focuses on the FFN and systematically investigates the trade-offs between PQC depth, expressibility, and trainability. Our final PQC architecture incorporates a residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating entanglement strategy to ensure stable training and high expressibility. Our experiments, conducted on a classical simulator, on the SST-2 and DBpedia benchmarks demonstrate two key findings. First, a carefully configured QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its classical counterpart in a full-data setting while reducing FFN-specific parameters by over 99%. Second, our model exhibits a consistent and competitive edge in few-shot learning scenarios, confirming its potential for superior data efficiency. These results, supported by an ablation study on a non-optimized PQC that failed to learn, confirm that PQCs can serve as powerful and parameter-efficient alternatives to classical FFNs when co-designed with foundational deep learning principles.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02364v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02356v1",
    "title": "Offline Reinforcement Learning with Penalized Action Noise Injection",
    "authors": [
      "JunHyeok Oh",
      "Byung-Jun Lee"
    ],
    "abstract": "Offline reinforcement learning (RL) optimizes a policy using only a fixed dataset, making it a practical approach in scenarios where interaction with the environment is costly. Due to this limitation, generalization ability is key to improving the performance of offline RL algorithms, as demonstrated by recent successes of offline RL with diffusion models. However, it remains questionable whether such diffusion models are necessary for highly performing offline RL algorithms, given their significant computational requirements during inference. In this paper, we propose Penalized Action Noise Injection (PANI), a method that simply enhances offline learning by utilizing noise-injected actions to cover the entire action space, while penalizing according to the amount of noise injected. This approach is inspired by how diffusion models have worked in offline RL algorithms. We provide a theoretical foundation for this method, showing that offline RL algorithms with such noise-injected actions solve a modified Markov Decision Process (MDP), which we call the noisy action MDP. PANI is compatible with a wide range of existing off-policy and offline RL algorithms, and despite its simplicity, it demonstrates significant performance improvements across various benchmarks.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02356v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02354v1",
    "title": "Lightweight Shrimp Disease Detection Research Based on YOLOv8n",
    "authors": [
      "Fei Yuhuan",
      "Wang Gengchen",
      "Liu Fenghao",
      "Zang Ran",
      "Sun Xufei",
      "Chang Hao"
    ],
    "abstract": "Shrimp diseases are one of the primary causes of economic losses in shrimp aquaculture. To prevent disease transmission and enhance intelligent detection efficiency in shrimp farming, this paper proposes a lightweight network architecture based on YOLOv8n. First, by designing the RLDD detection head and C2f-EMCM module, the model reduces computational complexity while maintaining detection accuracy, improving computational efficiency. Subsequently, an improved SegNext_Attention self-attention mechanism is introduced to further enhance the model's feature extraction capability, enabling more precise identification of disease characteristics. Extensive experiments, including ablation studies and comparative evaluations, are conducted on a self-constructed shrimp disease dataset, with generalization tests extended to the URPC2020 dataset. Results demonstrate that the proposed model achieves a 32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5 of 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms other lightweight YOLO-series models in mAP@0.5, parameter count, and model size. Generalization experiments on the URPC2020 dataset further validate the model's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The proposed method achieves an optimal balance between accuracy and efficiency, providing reliable technical support for intelligent disease detection in shrimp aquaculture.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV"
    ],
    "url": "http://arxiv.org/abs/2507.02354v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02353v1",
    "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent",
    "authors": [
      "Bowen Chen",
      "Zhao Wang",
      "Shingo Takamatsu"
    ],
    "abstract": "Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02353v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02345v1",
    "title": "HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3",
    "authors": [
      "Jie Gao",
      "Jing Hu",
      "Shanzhuo Zhang",
      "Kunrui Zhu",
      "Sheng Qian",
      "Yueyang Huang",
      "Xiaonan Zhang",
      "Xiaomin Fang"
    ],
    "abstract": "Antibody engineering is essential for developing therapeutics and advancing biomedical research. Traditional discovery methods often rely on time-consuming and resource-intensive experimental screening. To enhance and streamline this process, we introduce a production-grade, high-throughput platform built on HelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure prediction model, HelixFold3. The platform facilitates the large-scale generation of antibody candidate sequences and evaluates their interaction with antigens. Integrated high-performance computing (HPC) support enables high-throughput screening, addressing challenges such as fragmented toolchains and high computational demands. Validation on multiple antigens showcases the platform's ability to generate diverse and high-quality antibodies, confirming a scaling law where exploring larger sequence spaces increases the likelihood of identifying optimal binders. This platform provides a seamless, accessible solution for large-scale antibody design and is available via the antibody design page of PaddleHelix platform.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "q-bio.BM",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02345v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02342v1",
    "title": "DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values",
    "authors": [
      "Changhun Kim",
      "Yechan Mun",
      "Sangchul Hahn",
      "Eunho Yang"
    ],
    "abstract": "This study proposes DeltaSHAP, a novel explainable artificial intelligence (XAI) algorithm specifically designed for online patient monitoring systems. In clinical environments, discovering the causes driving patient risk evolution is critical for timely intervention, yet existing XAI methods fail to address the unique requirements of clinical time series explanation tasks. To this end, DeltaSHAP addresses three key clinical needs: explaining the changes in the consecutive predictions rather than isolated prediction scores, providing both magnitude and direction of feature attributions, and delivering these insights in real time. By adapting Shapley values to temporal settings, our approach accurately captures feature coalition effects. It further attributes prediction changes using only the actually observed feature combinations, making it efficient and practical for time-sensitive clinical applications. We also introduce new evaluation metrics to evaluate the faithfulness of the attributions for online time series, and demonstrate through experiments on online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI methods in both explanation quality as 62% and computational efficiency as 33% time reduction on the MIMIC-III decompensation benchmark. We release our code at https://github.com/AITRICS/DeltaSHAP.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02342v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02325v1",
    "title": "Grid-Connected, Data-Driven Inverter Control, Theory to Hardware",
    "authors": [
      "Sebastian Graf",
      "Keith Moffat",
      "Anurag Mohapatra",
      "Alessandro Chiuso",
      "Florian Dörfler"
    ],
    "abstract": "Grid-connected inverter control is challenging to implement due to the difficulty of obtaining and maintaining an accurate grid model. Direct Data-Driven Predictive Control provides a model-free alternative to traditional model-based control methods. This paper describes how the recently-proposed Transient Predictive Control (TPC) can be used for real-world, plug-and-play inverter control. The following hypotheses were tested: 1) The TPC algorithm can be run online using standard hardware, and 2) TPC, which is derived using Linear Time-Invariant assumptions, is effective for grid-connected inverter control, which is a nonlinear and time-varying system. Experiments conducted on a two-converter benchtop setup and at the CoSES Laboratory on a 25 kVA converter connected to the Munich grid support these hypotheses.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "eess.SY",
      "cs.SY"
    ],
    "url": "http://arxiv.org/abs/2507.02325v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02312v1",
    "title": "Enhancement of quantum coherence in solid-state qubits via interface engineering",
    "authors": [
      "Wing Ki Lo",
      "Yaowen Zhang",
      "Ho Yin Chow",
      "Jiahao Wu",
      "Man Yin Leung",
      "Kin On Ho",
      "Xuliang Du",
      "Yifan Chen",
      "Yang Shen",
      "Ding Pan",
      "Sen Yang"
    ],
    "abstract": "Shallow nitrogen-vacancy (NV) centers in diamond are promising quantum sensors but suffer from noise-induced short coherence times due to bulk and surface impurities. We present interfacial engineering via oxygen termination and graphene patching, extending shallow NV coherence to over 1 ms, approaching the T1 limit. Raman spectroscopy and density-functional theory reveal surface termination-driven graphene charge transfer reduces spin noise by pairing surface electrons, supported by double electron-electron resonance spectroscopy showing fewer unpaired spins. Enhanced sensitivity enables detection of single weakly coupled 13C nuclear spins and external 11B spins from a hexagonal boron nitride (h-BN) layer, achieving nanoscale nuclear magnetic resonance. A protective h-BN top layer stabilizes the platform, ensuring robustness against harsh treatments and compatibility with target materials. This integrated approach advances practical quantum sensing by combining extended coherence, improved sensitivity, and device durability.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02312v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02303v1",
    "title": "Measurements and Modeling of Air-Ground Integrated Channel in Forest Environment Based on OFDM Signals",
    "authors": [
      "Zhe Xiao",
      "Shu Sun",
      "Na Liu",
      "Lianming Xu",
      "Li Wang"
    ],
    "abstract": "Forests are frequently impacted by climate conditions, vegetation density, and intricate terrain and geology, which contribute to natural disasters. Personnel engaged in or supporting rescue operations in such environments rely on robust communication systems to ensure their safety, highlighting the criticality of channel measurements in forest environments. However, according to current research, there is limited research on channel detection and modeling in forest areas in the existing literature. This paper describes the channel measurements campaign of air and ground in the Arxan National Forest Park of Inner Mongolia. It presents measurement results and propagation models for ground-to-ground (G2G) and air-to-ground (A2G) scenarios. The measurement campaign uses orthogonal frequency division multiplexing signals centered at 1.4 GHz for channel sounding. In the G2G measurement, in addition to using omnidirectional antennas to record data, we also use directional antennas to record the arrival angle information of the signal at the receiver. In the A2G measurement, we pre-plan the flight trajectory of the unmanned aerial vehicle so that it can fly at a fixed angle relative to the ground. We present path loss models suitable for G2G and A2G in forest environments based on the analysis of measurement results. The results indicate that the proposed model reduces error margins compared with other path loss models. Furthermore, we derive the multipath model expression specific to forest environments and conduct statistical analysis on key channel parameters e.g., shadow fading factor, root mean square delay spread, and Rician K factor. Our findings reveal that signal propagation obstruction due to tree crowns in A2G communication is more pronounced than tree trunk obstructions in G2G communication. Adjusting the elevation angle between air and ground can enhance communication quality.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.IT",
      "math.IT"
    ],
    "url": "http://arxiv.org/abs/2507.02303v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02300v1",
    "title": "Human-Centered Explainability in Interactive Information Systems: A Survey",
    "authors": [
      "Yuhao Zhang",
      "Jiaxin An",
      "Ben Wang",
      "Yan Zhang",
      "Jiqun Liu"
    ],
    "abstract": "Human-centered explainability has become a critical foundation for the responsible development of interactive information systems, where users must be able to understand, interpret, and scrutinize AI-driven outputs to make informed decisions. This systematic survey of literature aims to characterize recent progress in user studies on explainability in interactive information systems by reviewing how explainability has been conceptualized, designed, and evaluated in practice. Following PRISMA guidelines, eight academic databases were searched, and 100 relevant articles were identified. A structural encoding approach was then utilized to extract and synthesize insights from these articles. The main contributions include 1) five dimensions that researchers have used to conceptualize explainability; 2) a classification scheme of explanation designs; 3) a categorization of explainability measurements into six user-centered dimensions. The review concludes by reflecting on ongoing challenges and providing recommendations for future exploration of related issues. The findings shed light on the theoretical foundations of human-centered explainability, informing the design of interactive information systems that better align with diverse user needs and promoting the development of systems that are transparent, trustworthy, and accountable.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2507.02300v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02295v1",
    "title": "Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources",
    "authors": [
      "Roopkatha Banerjee",
      "Prince Modi",
      "Jinal Vyas",
      "Chunduru Sri Abhijit",
      "Tejus Chandrashekar",
      "Harsha Varun Marisetty",
      "Manik Gupta",
      "Yogesh Simmhan"
    ],
    "abstract": "With the recent improvements in mobile and edge computing and rising concerns of data privacy, Federated Learning(FL) has rapidly gained popularity as a privacy-preserving, distributed machine learning methodology. Several FL frameworks have been built for testing novel FL strategies. However, most focus on validating the learning aspects of FL through pseudo-distributed simulation but not for deploying on real edge hardware in a distributed manner to meaningfully evaluate the federated aspects from a systems perspective. Current frameworks are also inherently not designed to support asynchronous aggregation, which is gaining popularity, and have limited resilience to client and server failures. We introduce Flotilla, a scalable and lightweight FL framework. It adopts a ``user-first'' modular design to help rapidly compose various synchronous and asynchronous FL strategies while being agnostic to the DNN architecture. It uses stateless clients and a server design that separates out the session state, which are periodically or incrementally checkpointed. We demonstrate the modularity of Flotilla by evaluating five different FL strategies for training five DNN models. We also evaluate the client and server-side fault tolerance on 200+ clients, and showcase its ability to rapidly failover within seconds. Finally, we show that Flotilla's resource usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It also scales significantly better compared to Flower for 1000+ clients. This positions Flotilla as a competitive candidate to build novel FL strategies on, compare them uniformly, rapidly deploy them, and perform systems research and optimizations.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.DC"
    ],
    "url": "http://arxiv.org/abs/2507.02295v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02287v1",
    "title": "Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents",
    "authors": [
      "Lapo Santarlasci",
      "Armando Rungi",
      "Antonio Zinilli"
    ],
    "abstract": "This paper introduces Natural Language Processing for identifying ``true'' green patents from official supporting documents. We start our training on about 12.4 million patents that had been classified as green from previous literature. Thus, we train a simple neural network to enlarge a baseline dictionary through vector representations of expressions related to environmental technologies. After testing, we find that ``true'' green patents represent about 20\\% of the total of patents classified as green from previous literature. We show heterogeneity by technological classes, and then check that `true' green patents are about 1\\% less cited by following inventions. In the second part of the paper, we test the relationship between patenting and a dashboard of firm-level financial accounts in the European Union. After controlling for reverse causality, we show that holding at least one ``true'' green patent raises sales, market shares, and productivity. If we restrict the analysis to high-novelty ``true'' green patents, we find that they also yield higher profits. Our findings underscore the importance of using text analyses to gauge finer-grained patent classifications that are useful for policymaking in different domains.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "econ.GN",
      "cs.CL",
      "q-fin.EC"
    ],
    "url": "http://arxiv.org/abs/2507.02287v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02286v1",
    "title": "Experimental demonstration of the clock asynchrony model in space-borne gravitational wave detection",
    "authors": [
      "Ming-Yang Xua",
      "Yu-Jie Tan",
      "Ning Ma",
      "Ao-Ting Fang",
      "Yi-Jun Xia",
      "Cheng-Gang Shao"
    ],
    "abstract": "Space-borne gravitational wave detection will open the observation window in the 0.1 mHz$-$1 Hz bandwidth, playing a crucial role in the development of cosmology and physics. Precise clock synchronization among satellites is essential for the accurate detection of gravitational wave signals. However, the independent clock counting mechanisms of each satellite pose a significant challenge. This work reports the mathematical model of clock asynchrony, which is mainly dominated by the constant term factor and the linear term factor. Moreover, it experimentally verifies the clock asynchronization technique based on a dual-phasemeter system. Through experimentation, the impacts of these two aspects of clock asynchrony were confirmed, and post-processing techniques were employed to reduce these impacts to as low as $\\rm 2\\pi \\times 10^{-6} rad/Hz^{1/2}@ 3mHz$. Specifically, the constant term factor is measured by Time-delay Interferometry Ranging (TDIR), while the linear term factor can be gauged by clock transmission link. This study provides a reference for understanding the clock asynchrony mechanism and processing clock synchronization issues. Additionally, a low additional noise clock synchronization test system is introduced to support such measurements.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "gr-qc",
      "astro-ph.IM",
      "physics.app-ph",
      "physics.ins-det"
    ],
    "url": "http://arxiv.org/abs/2507.02286v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02283v1",
    "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness",
    "authors": [
      "Tim Rogers",
      "Ben Teehankee"
    ],
    "abstract": "This paper examines a critical yet unexplored dimension of the AI alignment problem: the potential for Large Language Models (LLMs) to inherit and amplify existing misalignments between human espoused theories and theories-in-use. Drawing on action science research, we argue that LLMs trained on human-generated text likely absorb and reproduce Model 1 theories-in-use - a defensive reasoning pattern that both inhibits learning and creates ongoing anti-learning dynamics at the dyad, group, and organisational levels. Through a detailed case study of an LLM acting as an HR consultant, we show how its advice, while superficially professional, systematically reinforces unproductive problem-solving approaches and blocks pathways to deeper organisational learning. This represents a specific instance of the alignment problem where the AI system successfully mirrors human behaviour but inherits our cognitive blind spots. This poses particular risks if LLMs are integrated into organisational decision-making processes, potentially entrenching anti-learning practices while lending authority to them. The paper concludes by exploring the possibility of developing LLMs capable of facilitating Model 2 learning - a more productive theory-in-use - and suggests this effort could advance both AI alignment research and action science practice. This analysis reveals an unexpected symmetry in the alignment challenge: the process of developing AI systems properly aligned with human values could yield tools that help humans themselves better embody those same values.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "I.2.6; H.1.2"
    ],
    "url": "http://arxiv.org/abs/2507.02283v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02276v1",
    "title": "VLTI observations of the Orion Belt stars: I. eps Orionis",
    "authors": [
      "Alžběta Oplištilová",
      "Miroslav Brož",
      "Christian A. Hummel",
      "Petr Harmanec",
      "Brad Barlow"
    ],
    "abstract": "Massive stars play a decisive role in the evolution of the Universe. In order to constrain their current state and structure, we need sufficiently complex models, constrained by astrometric, interferometric, and spectroscopic observations. However, they are not available for distant stars. Instead, we focused on the nearest massive stars in the Orion Belt. We obtained VLTI interferometric observations of Orion Belt stars and calibrated visibility data from the GRAVITY and PIONIER instruments. Additionally, we obtained spectroscopic data from the CFHT and CTIO observatories. For modelling, we used a modified version of PHOEBE2, extended with new interferometric and spectroscopic modules. To describe non-spherical, rotating, or Roche-like stars, integrals over triangular meshes have to be computed, using extensive grids of synthetic spectra. For fitting, we used the simplex algorithm and chi2 mapping of the parameter space. In this paper, we present single-star models of the B0Ia supergiant eps Ori. Interferometric visibilities indicate that the star is not spherical but rotating close to its critical velocity. The preferred distance, d=(384+-8)pc, corresponds to the median of distances for the Orion OB1b association. We obtained the following parameters: m=(28.4+-2.0)Msol, R=(27.6+-1.5)Rsol, Teff=25000 K, i=45deg, longitude of the ascending node, Omega=300deg, and Prot=4.3+1.0d. This compromise model provides a reasonable fit to wind-free Balmer line profiles, but there is still some tension between interferometric and spectroscopic datasets, corresponding to a faster- vs. slower-rotating star. Our fast-rotating model implies that circumstellar matter should be naturally present, in the form of wind or disk, and contribute to continuum radiation. The fast rotation of eps Ori is compatible with a merger, formed from a multiple system of comparable mass, like del, zet or sig Ori.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.SR",
      "astro-ph.GA"
    ],
    "url": "http://arxiv.org/abs/2507.02276v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02255v1",
    "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation",
    "authors": [
      "Zihao Li",
      "Chao Yang",
      "Tong Zhang",
      "Yakun Chen",
      "Xianzhi Wang",
      "Guandong Xu",
      "Daoyi Dong"
    ],
    "abstract": "Preference alignment has achieved greater success on Large Language Models (LLMs) and drawn broad interest in recommendation research. Existing preference alignment methods for recommendation either require explicit reward modeling or only support pairwise preference comparison. The former directly increases substantial computational costs, while the latter hinders training efficiency on negative samples. Moreover, no existing effort has explored preference alignment solutions for tail-item recommendation. To bridge the above gaps, we propose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison to listwise comparison, to improve the efficiency of model training. Specifically, we derive a closed form optimal policy to enable more efficient and effective training without explicit reward modeling. We also present an adaptive negative sampling and reweighting strategy to prioritize tail items during optimization and enhance performance in tail-item recommendations. Besides, we theoretically prove that optimizing the listwise preference optimization (LPO) loss is equivalent to maximizing the upper bound of the optimal reward. Our experiments on three public datasets show that our method outperforms 10 baselines by a large margin, achieving up to 50% performance improvement while reducing 17.9% GPU memory usage when compared with direct preference optimization (DPO) in tail-item recommendation. Our code is available at https://github.com/Yuhanleeee/LPO4Rec.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.IR",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02255v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02252v1",
    "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement",
    "authors": [
      "Zeyu Lei",
      "Hongyuan Yu",
      "Jinlin Wu",
      "Zhen Chen"
    ],
    "abstract": "Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02252v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02248v1",
    "title": "Transfer Learning for Matrix Completion",
    "authors": [
      "Dali Liu",
      "Haolei Weng"
    ],
    "abstract": "In this paper, we explore the knowledge transfer under the setting of matrix completion, which aims to enhance the estimation of a low-rank target matrix with auxiliary data available. We propose a transfer learning procedure given prior information on which source datasets are favorable. We study its convergence rates and prove its minimax optimality. Our analysis reveals that with the source matrices close enough to the target matrix, out method outperforms the traditional method using the single target data. In particular, we leverage the advanced sharp concentration inequalities introduced in \\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the convergence rate, which is crucial for proving the minimax optimality. When the relevance of source datasets is unknown, we develop an efficient detection procedure to identify informative sources and establish its selection consistency. Simulations and real data analysis are conducted to support the validity of our methodology.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.ML",
      "cs.LG",
      "15A83",
      "I.2.6; G.3"
    ],
    "url": "http://arxiv.org/abs/2507.02248v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02245v1",
    "title": "CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather",
    "authors": [
      "Minghao Ning",
      "Yufeng Yang",
      "Keqi Shu",
      "Shucheng Huang",
      "Jiaming Zhong",
      "Maryam Salehi",
      "Mahdi Rahmani",
      "Yukun Lu",
      "Chen Sun",
      "Aladdin Saleh",
      "Ehsan Hashemi",
      "Amir Khajepour"
    ],
    "abstract": "We present CoInfra, a large-scale cooperative infrastructure perception system and dataset designed to advance robust multi-agent perception under real-world and adverse weather conditions. The CoInfra system includes 14 fully synchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR, deployed across a shared region and operating continuously to capture all traffic participants in real-time. A robust, delay-aware synchronization protocol and a scalable system architecture that supports real-time data fusion, OTA management, and remote monitoring are provided in this paper. On the other hand, the dataset was collected in different weather scenarios, including sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR frames and 390k camera images from 8 infrastructure nodes that are globally time-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding box annotations for five object classes (i.e., car, bus, truck, person, and bicycle) are provided in both global and individual node frames, along with high-definition maps for contextual understanding. Baseline experiments demonstrate the trade-offs between early and late fusion strategies, the significant benefits of HD map integration are discussed. By openly releasing our dataset, codebase, and system documentation at https://github.com/NingMingHao/CoInfra, we aim to enable reproducible research and drive progress in infrastructure-supported autonomous driving, particularly in challenging, real-world settings.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.RO"
    ],
    "url": "http://arxiv.org/abs/2507.02245v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02244v1",
    "title": "Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies",
    "authors": [
      "Fangzhou Shi",
      "Xiaopeng Ke",
      "Xinye Xiong",
      "Kexin Meng",
      "Chang Men",
      "Zhengdan Zhu"
    ],
    "abstract": "The proliferation of ride-hailing aggregator platforms presents significant growth opportunities for ride-service providers by increasing order volume and gross merchandise value (GMV). On most ride-hailing aggregator platforms, service providers that offer lower fares are ranked higher in listings and, consequently, are more likely to be selected by passengers. This competitive ranking mechanism creates a strong incentive for service providers to adopt coupon strategies that lower prices to secure a greater number of orders, as order volume directly influences their long-term viability and sustainability. Thus, designing an effective coupon strategy that can dynamically adapt to market fluctuations while optimizing order acquisition under budget constraints is a critical research challenge. However, existing studies in this area remain scarce.   To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based subsidy strategy framework designed to rapidly adapt to competitors' pricing adjustments. Our approach integrates two key techniques: Fast Competition Adaptation (FCA), which enables swift responses to dynamic price changes, and Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget constraints while optimizing coupon decisions on new price landscape. Furthermore, we introduce RideGym, the first dedicated simulation environment tailored for ride-hailing aggregators, facilitating comprehensive evaluation and benchmarking of different pricing strategies without compromising real-world operational efficiency. Experimental results demonstrate that our proposed method consistently outperforms baseline approaches across diverse market conditions, highlighting its effectiveness in subsidy optimization for ride-hailing service providers.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG",
      "cs.AI"
    ],
    "url": "http://arxiv.org/abs/2507.02244v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02242v1",
    "title": "Predicting Flow-Induced Vibration in Isolated and Tandem Cylinders Using Hypergraph Neural Networks",
    "authors": [
      "Shayan Heydari",
      "Rui Gao",
      "Rajeev K Jaiman"
    ],
    "abstract": "We present a finite element-inspired hypergraph neural network framework for predicting flow-induced vibrations in freely oscillating cylinders. The surrogate architecture transforms unstructured computational meshes into node-element hypergraphs that encode higher-order spatial relationships through element-based connectivity, preserving the geometric and topological structure of the underlying finite-element discretization. The temporal evolution of the fluid-structure interaction is modeled via a modular partitioned architecture: a complex-valued, proper orthogonal decomposition-based sub-network predicts mesh deformation using a low-rank representation of Arbitrary Lagrangian-Eulerian (ALE) grid displacements, while a hypergraph-based message-passing network predicts the unsteady flow field using geometry-aware node, element, and hybrid edge features. High-fidelity ALE-based simulations provide training and evaluation data across a range of Reynolds numbers and reduced velocities for isolated and tandem cylinder configurations. The framework demonstrates stable roll-outs and accurately captures the nonlinear variation of oscillation amplitudes with respect to reduced velocity, a key challenge in surrogate modeling of flow-induced vibrations. In the tandem configuration, the model successfully resolves complex wake-body interactions and multi-scale coupling effects, enabling accurate prediction of pressure and velocity fields under strong wake interference conditions. Our results show high fidelity in reproducing force statistics, dominant frequencies, and flow-field dynamics, supporting the framework's potential as a robust surrogate model for digital twin applications.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.flu-dyn",
      "physics.comp-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02242v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02241v1",
    "title": "VERBA: Verbalizing Model Differences Using Large Language Models",
    "authors": [
      "Shravan Doda",
      "Shashidhar Reddy Javaji",
      "Zining Zhu"
    ],
    "abstract": "In the current machine learning landscape, we face a \"model lake\" phenomenon: Given a task, there is a proliferation of trained models with similar performances despite different behavior. For model users attempting to navigate and select from the models, documentation comparing model pairs is helpful. However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a number prohibitive for the model developers to manually perform pairwise comparisons and prepare documentations. To facilitate fine-grained pairwise comparisons among models, we introduced $\\textbf{VERBA}$. Our approach leverages a large language model (LLM) to generate verbalizations of model differences by sampling from the two models. We established a protocol that evaluates the informativeness of the verbalizations via simulation. We also assembled a suite with a diverse set of commonly used machine learning models as a benchmark. For a pair of decision tree models with up to 5% performance difference but 20-25% behavioral differences, $\\textbf{VERBA}$ effectively verbalizes their variations with up to 80% overall accuracy. When we included the models' structural information, the verbalization's accuracy further improved to 90%. $\\textbf{VERBA}$ opens up new research avenues for improving the transparency and comparability of machine learning models in a post-hoc manner.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02241v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02215v1",
    "title": "Hybrid least squares for learning functions from highly noisy data",
    "authors": [
      "Ben Adcock",
      "Bernhard Hientzsch",
      "Akil Narayan",
      "Yiming Xu"
    ],
    "abstract": "Motivated by the need for efficient estimation of conditional expectations, we consider a least-squares function approximation problem with heavily polluted data. Existing methods that are powerful in the small noise regime are suboptimal when large noise is present. We propose a hybrid approach that combines Christoffel sampling with certain types of optimal experimental design to address this issue. We show that the proposed algorithm enjoys appropriate optimality properties for both sample point generation and noise mollification, leading to improved computational efficiency and sample complexity compared to existing methods. We also extend the algorithm to convex-constrained settings with similar theoretical guarantees. When the target function is defined as the expectation of a random field, we extend our approach to leverage adaptive random subspaces and establish results on the approximation capacity of the adaptive procedure. Our theoretical findings are supported by numerical studies on both synthetic data and on a more challenging stochastic simulation problem in computational finance.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "stat.ML",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "url": "http://arxiv.org/abs/2507.02215v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02212v1",
    "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers",
    "authors": [
      "Takuro Kawada",
      "Shunsuke Kitada",
      "Sota Nemoto",
      "Hitoshi Iyatomi"
    ],
    "abstract": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key findings of scientific papers. While recent research has increasingly incorporated visual materials such as Figure 1 as de facto GAs, their potential to enhance scientific communication remains largely unexplored. Moreover, designing effective GAs requires advanced visualization skills, creating a barrier to their widespread adoption. To tackle these challenges, we introduce SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific papers and 1.14 million figures, explicitly designed for supporting GA selection and recommendation as well as facilitating research in automated GA generation. As a preliminary step toward GA design support, we define two tasks: 1) Intra-GA recommendation, which identifies figures within a given paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation, which retrieves GAs from other papers to inspire the creation of new GAs. We provide reasonable baseline models for these tasks. Furthermore, we propose Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation metric that offers a fine-grained analysis of model behavior. CAR addresses limitations in traditional ranking-based metrics by considering cases where multiple figures within a paper, beyond the explicitly labeled GA, may also serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a foundation for advancing visual scientific communication while contributing to the development of AI for Science.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02212v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02207v1",
    "title": "Public perspectives on the design of fusion energy facilities",
    "authors": [
      "Nathan Kawamoto",
      "Daniel Hoover",
      "Jonathan Xie",
      "Jacob Walters",
      "Katie Snyder",
      "Aditi Verma"
    ],
    "abstract": "As fusion energy technologies approach demonstration and commercial deployment, understanding public perspectives on future fusion facilities will be critical for achieving social license, especially because fusion energy facilities, unlike large fission reactors, may be sited in closer proximity to people and communities, due to distinct regulatory frameworks. In a departure from the 'decide-announce-defend' approach typically used to site energy infrastructure, we develop a participatory design methodology for collaboratively designing fusion energy facilities with prospective host communities. We present here our findings from a participatory design workshop that brought together 22 community participants and 34 engineering students. Our analysis of the textual and visual data from this workshop shows a range of design values and decision-making criteria with 'integrity' and 'respect' ranking highest among values and 'economic benefits' and 'environmental protection/safety' ranking highest among decision-making criteria. Salient design themes that emerge across facility concepts include connecting the history and legacy of the community to the design of the facility, care for workers, transparency and access to the facility, and health and safety of the host community. Participants reported predominantly positive sentiments, expressing joy and surprise as the workshop progressed from learning about fusion to designing the hypothetical facility. Our findings suggest that carrying out participatory design in the early stages of technology development can invite and make concrete public hopes and concerns, improve understanding of, and curiosity about, an emerging technology, build toward social license, and inform context-specific development of fusion energy facilities.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "physics.soc-ph",
      "cs.HC",
      "physics.ed-ph",
      "physics.plasm-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02207v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02202v1",
    "title": "Quark-Gluon Plasma as a Quantum Channel: Entanglement, Decoherence, and Hadronization",
    "authors": [
      "Fidele J. Twagirayezu"
    ],
    "abstract": "We propose a quantum information framework to model the quark-gluon plasma (QGP) as a composite quantum channel acting on a multi-qubit or multi-qutrit color-entangled system. The QGP's effects are represented by amplitude damping (jet quenching), $SU(3)$ depolarizing noise (decoherence), and a thermal hadronization channel projecting onto color-singlet states. This construction captures energy loss, decoherence, and confinement dynamics in a unified open quantum system framework. We analyze the evolution of entanglement entropy and purity under this composite channel. Amplitude damping reduces entropy by driving subsystems toward pure states, while decoherence increases mixedness. Hadronization further modifies correlations via thermal projections weighted by hadron masses and freeze-out temperature ($T \\sim 156 \\,\\text{MeV}$). Numerical simulations show monotonic entropy and purity loss, consistent with entanglement degradation and confinement. Our results support interpreting the QGP as a noisy quantum channel that progressively erases color entanglement. This framework bridges quantum information theory and QCD, offering new insights into hadronization and non-perturbative dynamics in heavy-ion collisions.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "nucl-th"
    ],
    "url": "http://arxiv.org/abs/2507.02202v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02199v1",
    "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer",
    "authors": [
      "Wenquan Lu",
      "Yuechuan Yang",
      "Kyle Lee",
      "Yanshu Li",
      "Enqi Liu"
    ],
    "abstract": "Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02199v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02186v1",
    "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge",
    "authors": [
      "Zahra Ashktorab",
      "Elizabeth M. Daly",
      "Erik Miehling",
      "Werner Geyer",
      "Martin Santillan Cooper",
      "Tejaswini Pedapati",
      "Michael Desmond",
      "Qian Pan",
      "Hyo Jin Do"
    ],
    "abstract": "With the broad availability of large language models and their ability to generate vast outputs using varied prompts and configurations, determining the best output for a given task requires an intensive evaluation process, one where machine learning practitioners must decide how to assess the outputs and then carefully carry out the evaluation. This process is both time-consuming and costly. As practitioners work with an increasing number of models, they must now evaluate outputs to determine which model and prompt performs best for a given task. LLMs are increasingly used as evaluators to filter training data, evaluate model performance, assess harms and risks, or assist human evaluators with detailed assessments. We present EvalAssist, a framework that simplifies the LLM-as-a-judge workflow. The system provides an online criteria development environment, where users can interactively build, test, and share custom evaluation criteria in a structured and portable format. We support a set of LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a prompt-chaining approach we developed and contributed to the UNITXT open-source library. Additionally, our system also includes specially trained evaluators to detect harms and risks in LLM outputs. We have deployed the system internally in our organization with several hundreds of users.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC"
    ],
    "url": "http://arxiv.org/abs/2507.02186v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02183v1",
    "title": "Computer Science Education in the Age of Generative AI",
    "authors": [
      "Russell Beale"
    ],
    "abstract": "Generative AI tools - most notably large language models (LLMs) like ChatGPT and Codex - are rapidly revolutionizing computer science education. These tools can generate, debug, and explain code, thereby transforming the landscape of programming instruction. This paper examines the profound opportunities that AI offers for enhancing computer science education in general, from coding assistance to fostering innovative pedagogical practices and streamlining assessments. At the same time, it highlights challenges including academic integrity concerns, the risk of over-reliance on AI, and difficulties in verifying originality. We discuss what computer science educators should teach in the AI era, how to best integrate these technologies into curricula, and the best practices for assessing student learning in an environment where AI can generate code, prototypes and user feedback. Finally, we propose a set of policy recommendations designed to harness the potential of generative AI while preserving the integrity and rigour of computer science education. Empirical data and emerging studies are used throughout to support our arguments.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CY",
      "cs.HC",
      "H.5.0; K.3.1; K.3.2"
    ],
    "url": "http://arxiv.org/abs/2507.02183v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02180v1",
    "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future",
    "authors": [
      "Russell Beale"
    ],
    "abstract": "Large language Models have only been widely available since 2022 and yet in less than three years have had a significant impact on approaches to education and educational technology. Here we review the domains in which they have been used, and discuss a variety of use cases, their successes and failures. We then progress to discussing how this is changing the dynamic for learners and educators, consider the main design challenges facing LLMs if they are to become truly helpful and effective as educational systems, and reflect on the learning paradigms they support. We make clear that the new interaction paradigms they bring are significant and argue that this approach will become so ubiquitous it will become the default way in which we interact with technologies, and revolutionise what people expect from computer systems in general. This leads us to present some specific and significant considerations for the design of educational technology in the future that are likely to be needed to ensure acceptance by the changing expectations of learners and users.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.HC",
      "cs.CY",
      "H.5.0; K.3.1; K.3.2"
    ],
    "url": "http://arxiv.org/abs/2507.02180v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02177v1",
    "title": "ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space",
    "authors": [
      "Yan Long",
      "Jiancong Cui",
      "Yuqing Yang",
      "Tobias Alam",
      "Zhiqiang Lin",
      "Kevin Fu"
    ],
    "abstract": "This work investigates how to monitor access to Android zero-permission sensors which could cause privacy leakage to users. Moreover, monitoring such sensitive access allows security researchers to characterize potential sensor abuse patterns. Zero-permission sensors such as accelerometers have become an indispensable part of Android devices. The critical information they provide has attracted extensive research investigating how data collectors could capture more sensor data to enable both benign and exploitative applications. In contrast, little work has explored how to enable data providers, such as end users, to understand sensor usage. While existing methods such as static analysis and hooking-based dynamic analysis face challenges of requiring complicated development chains, rooting privilege, and app-specific reverse engineering analysis, our work aims to bridge this gap by developing ARMOUR for user-space runtime monitoring, leveraging the intrinsic sampling rate variation and convergence behaviors of Android. ARMOUR enables privacy-aware users to easily monitor how third-party apps use sensor data and support security researchers to perform rapid app-agnostic sensor access analysis. Our evaluation with 1,448 commercial applications shows the effectiveness of ARMOUR in detecting sensor usage in obfuscated code and other conditions, and observes salient sensor abuse patterns such as 50% of apps from seemingly sensor-independent categories accessing data of multiple zero-permission sensors. We analyze the impact of Android's recent policy changes on zero-permission sensors and remaining technical and regulatory problems.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.CR",
      "K.6.5; D.4.6"
    ],
    "url": "http://arxiv.org/abs/2507.02177v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02174v1",
    "title": "Investigating Magnetic Field Fluctuations in Jovian Auroral Electron Beams",
    "authors": [
      "A. Salveter",
      "J. Saur",
      "G. Clark",
      "A. Sulaiman",
      "B. H. Mauk",
      "J. E. P. Connerney",
      "B. Bonfond"
    ],
    "abstract": "The Juno spacecraft provides a unique opportunity to explore the mechanisms generating Jupiter's aurorae. Past analyses of Juno data immensely advanced our understanding of its auroral acceleration processes, however, few studies utilized multiple instruments on Juno in a joint systematic analysis. This study uses measurements from the Juno Ultraviolet Spectrograph (UVS), the Jupiter Energetic particle Detector Instrument (JEDI), and the Juno Magnetometer (MAG) from the first 20 perijoves. On magnetic field lines associated with the diffuse aurora, we consistently find small-scale magnetic field fluctuations with amplitudes of up to 100 nT on time scales of seconds to 1 minute. On magnetic field lines directly linked to the main emission, the electron distribution is field-aligned, mostly broad-band in energy, and accompanied by large-scale magnetic field perturbations of several 100 nT on time scales of tens of min (except one case). These large-scale perturbations are generally associated with quasistatic field-aligned electric currents. Small-scale magnetic fields are not resolved over the main emission zone closer than radial distances 4 Jovian radii due to the digitization limit of the magnetometer. However, in all cases where Juno crosses the main auroral field lines beyond 4RJ, the digitization limit is significantly reduced and we detect small-scale magnetic field fluctuations of 2 nT to 10 nT consistent with a turbulent spectrum. Associated energy fluxes projected to Jupiter can exceed 1000 mW/m2. The general broad-band nature of the electron distributions and the consistent presence of small-scale magnetic field fluctuations over the main emission support that wave-particle interaction can dominantely contribute to power Jupiter's auroral processes.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "astro-ph.EP",
      "physics.space-ph"
    ],
    "url": "http://arxiv.org/abs/2507.02174v1"
  },
  {
    "id": "http://arxiv.org/abs/2507.02169v1",
    "title": "Statistical Inference for Responsiveness Verification",
    "authors": [
      "Seung Hyun Cheon",
      "Meredith Stewart",
      "Bogdan Kulynych",
      "Tsui-Wei Weng",
      "Berk Ustun"
    ],
    "abstract": "Many safety failures in machine learning arise when models are used to assign predictions to people (often in settings like lending, hiring, or content moderation) without accounting for how individuals can change their inputs. In this work, we introduce a formal validation procedure for the responsiveness of predictions with respect to interventions on their features. Our procedure frames responsiveness as a type of sensitivity analysis in which practitioners control a set of changes by specifying constraints over interventions and distributions over downstream effects. We describe how to estimate responsiveness for the predictions of any model and any dataset using only black-box access, and how to use these estimates to support tasks such as falsification and failure probability estimation. We develop algorithms that construct these estimates by generating a uniform sample of reachable points, and demonstrate how they can promote safety in real-world applications such as recidivism prediction, organ transplant prioritization, and content moderation.",
    "year": 2025,
    "venue": "arXiv",
    "keywords": [
      "cs.LG"
    ],
    "url": "http://arxiv.org/abs/2507.02169v1"
  }
]